[
  {
    "answer_id": 170,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "brain_teaser",
    "question_text": "You're debugging Argus's PPG sensor where the glucose readings suddenly spike whenever the user walks upstairs. The device uses 6-channel PPG with Red/IR/Green LEDs and has accelerometer data. The readings are perfect when stationary but show a 3x increase in glucose variance during stair climbing. Given that the motion artifact removal algorithm uses accelerometer magnitude thresholds and adaptive filtering, what could be causing this specific stair-related issue rather than general walking motion?",
    "expected_answer": "Stair climbing creates periodic acceleration patterns (1-2 Hz) that coincide with heart rate harmonics, causing the adaptive filter to mistake cardiac signals for motion artifacts. The algorithm likely has a notch filter that's removing the actual PPG signal components while preserving the motion-induced baseline shifts. Solution involves adjusting the frequency domain analysis to distinguish between gait harmonics and actual pulse signals.",
    "candidate_answer": "An accelorometer detects acceleration by taking into account 1g of gravity plus the acceleration. So the magnitude when calculated, for walking just adds a small acceleration component to the horizontal motion but on vertical motion, the acceleration gets added to 1g which shoots up the magnitude. When walking upstairs there are higher z axis oscillations as we push harder off the trailing leg to climb which leads to sharper more vertical impacts. Another thing to consider is when walking upstairs, the gravity vector projection changes since we lean a bit forward when walking upstairs so the acceleration component changes even more. Adaptive filtering will also remove relevant signals instead of just motion artifacts. Stair climbing might create a harmonic pattern that might overlap with the heart rate which might not work well for the adaptive filter. Heart rate increases on climbing stairs and the vertical acceleration increases producing harmonics at multiples just like heart rate. This results into HR not being separated well from PPG through adaptive filtering",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate correctly identified that stair climbing creates specific acceleration patterns that interfere with the PPG signal processing. They explained the physics of vertical acceleration during stair climbing and how this affects the accelerometer readings. They also recognized that adaptive filtering can mistakenly remove actual cardiac signals. However, they didn't explicitly mention the critical 1-2 Hz frequency overlap with heart rate harmonics, which is the key insight for this specific stair-related issue. The explanation about gravity vector changes and vertical impacts is relevant but somewhat tangential to the core frequency-domain problem."
  },
  {
    "answer_id": 171,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "brain_teaser",
    "question_text": "During FDA testing, Argus shows perfect glucose correlation in morning readings but afternoon readings have 15% error. The device compensates for skin tone using Fitzpatrick scale detection. Morning tests were done on participants 1-3, afternoon on 4-6. Given that melanin affects optical path length and the device uses fixed LED intensity for all skin types, explain why this time-of-day pattern emerges despite identical environmental conditions.",
    "expected_answer": "Higher Fitzpatrick skin types (4-6) have increased melanin which absorbs more green light (used for glucose estimation) but reflects more IR. The fixed LED intensity causes non-linear optical path variations. Morning participants (1-3) have lower melanin allowing deeper tissue penetration, while afternoon participants (4-6) with higher melanin get shallower readings affecting glucose estimation accuracy. The device needs dynamic LED intensity adjustment based on detected skin tone.",
    "candidate_answer": "Time of the day matters because in the morning melanin absorption is lower as compared to afternoon. In the afternoon the skin hydration changes and optical path becomes shorter and more variable so glucose models predict incorrectly. Fixed LED intensity does not work for all melanin levels. Darker skin absorbs far more light so less light reaches the photodiode and signals might saturate or disappear. ",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate correctly identifies that melanin absorption varies by skin type and that fixed LED intensity is problematic. However, they incorrectly attribute the time-of-day pattern to hydration changes rather than the participant grouping by skin tone. The key insight - that participants 1-3 have lower Fitzpatrick scores (less melanin) while 4-6 have higher scores - is missing. While hydration effects exist, they are explicitly ruled out by 'identical environmental conditions'. The answer also introduces saturation as a potential issue without clear justification."
  },
  {
    "answer_id": 172,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "brain_teaser",
    "question_text": "Your BLE transmission shows 47ms latency spikes every 180 seconds when transmitting glucose data packets. The system uses 23-byte packets every 5 seconds normally. Argus has a 50ms hard real-time constraint for glucose alerts. Given that glucose alerts require 3 consecutive readings to trigger, explain why these specific latency spikes occur and how they might violate FDA requirements for hypoglycemic alerts.",
    "expected_answer": "The 180-second interval suggests buffer flushing or encryption key rotation in the BLE stack. During spikes, 3 packets might be queued, causing 47ms \u00d7 3 = 141ms total delay. This violates the 50ms constraint for critical alerts. Solution requires prioritizing alert packets or implementing emergency transmission bypass for glucose <70mg/dL readings.",
    "candidate_answer": "The latency spikes occur and are seen until 141ms as 47ms*3ms which is beyond the 50ms constraint for glucose alerts. Low power BLE devices sleep every few minutes to recalibrate their oscillator. There might be security processes and other housekeeping tasks that might be running every 180 seconds which results into these latency spikes. There is also frequency hopping that happens across channels that might lead to this. FDA violation happens because 141ms>50ms threshold. The 47ms latency blocks all packets simultaneously and not one after another.",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate correctly identified the 141ms total delay (47ms \u00d7 3 packets) and recognized this violates the 50ms FDA constraint. They also correctly identified potential causes like security processes and housekeeping tasks. However, they missed the critical insight about buffer flushing/encryption key rotation that the 180-second interval specifically suggests. The explanation about frequency hopping is less relevant to periodic spikes. While they understood the FDA violation, their explanation of packet blocking was unclear - the 47ms spike likely affects all queued packets simultaneously rather than sequentially. The response lacks the specific medical device context (hypoglycemic alerts <70mg/dL) and doesn't propose concrete solutions like packet prioritization or emergency bypass."
  },
  {
    "answer_id": 173,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "brain_teaser",
    "question_text": "In temperature-compensated glucose readings using PPG waveform morphology, you notice perfect correlation at 22\u00b0C but 8% drift at 32\u00b0C ambient temperature. The temperature sensor shows stable skin contact. Given that vasodilation changes blood volume and PPG amplitude, and your algorithm uses fixed baseline correction, explain why the drift is non-linear with temperature and how this affects FDA 510(k) submission.",
    "expected_answer": "Temperature-induced vasodilation non-linearly affects blood volume (Q10 effect), changing the optical path length exponentially. Fixed baseline correction assumes linear temperature response, but actual physiological response follows Arrhenius kinetics. This requires temperature-dependent calibration curves for FDA submission, not single-point correction, as the relationship is exponential rather than linear.",
    "candidate_answer": "It could be that photodiode sensitivity changes with temperature but more importantly the blood vessel diameter changes exponentially with temperature not linearly. FDA requires to have high accuracy across the full temperature range with proper calibration curves. Using the Arrhenius kinetic equations, the sensor output vs temperature may drift because the optical absorption changes with temperature.",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate correctly identifies the exponential relationship between temperature and blood vessel diameter/volume, which is the core reason for non-linear drift. They also mention the need for calibration curves for FDA submission. However, the explanation is somewhat fragmented - mixing photodiode sensitivity (a hardware issue) with the physiological response without clearly prioritizing the dominant physiological mechanism. The mention of Arrhenius kinetics is good but could be better integrated into explaining the exponential relationship. The response lacks the specific detail about optical path length changes and doesn't explicitly address why fixed baseline correction fails for this non-linear response."
  },
  {
    "answer_id": 178,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "code_review",
    "question_text": "Review this PPG signal processing code for motion artifact detection. The algorithm is supposed to detect motion artifacts in multi-wavelength PPG data using accelerometer data, but it's producing false positives during normal cardiac cycles.",
    "expected_answer": "Multiple issues: 1) Heart rate assumption is fixed at 60 BPM instead of being calculated from PPG data, 2) No frequency-domain analysis to distinguish motion artifacts (0.5-7Hz) from cardiac signals (0.8-2Hz), 3) No consideration of phase relationship between PPG and accelerometer, 4) Should use adaptive threshold based on signal quality, 5) Missing band-pass filtering for both PPG and accelerometer data before analysis",
    "candidate_answer": "The issues are that in the initial code there is normalization of access_mag with hr_freq but this does not have any physical justification in the sense that access_mag does not scale with hr_freq; also squaring the ppg signal could lead to exponential increase in both signal and the noise so a threshold based filtering will result into miss of relevant signals. In this fix, first off we perform window based analysis so that noise is smoothened out. Using spectral purity index, checked whether the energy is around the heart_rate frequency, if it is not it means it is spread out and signal is distorted or motion is present.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate identified some valid issues with the original code (fixed heart rate assumption, lack of frequency analysis) and attempted to implement a frequency-domain approach using spectral purity index (SPI). However, the implementation has significant flaws: 1) The code has syntax errors (incorrect import statement, invalid numpy indexing with && instead of &), 2) The SPI calculation is incomplete (total_energy is undefined), 3) The window-based approach is good but the implementation is buggy, 4) The candidate didn't address the fundamental problem of distinguishing motion artifacts from cardiac signals in the frequency domain, 5) Missing proper filtering before analysis, 6) The calculate_hr function has incorrect parameters (fs*50 instead of distance parameter), 7) No consideration of phase relationship between PPG and accelerometer data. While the conceptual direction is partially correct, the execution is poor."
  },
  {
    "answer_id": 179,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "code_review",
    "question_text": "This embedded C code is designed to calculate SpO2 from dual-wavelength PPG (Red and IR) on an ARM Cortex-M4, but it's causing stack overflow crashes during continuous monitoring. The device has 64KB RAM and needs to process 100Hz sampling.",
    "expected_answer": "Memory allocation issues: 1) Allocating 320KB for buffers on stack (64KB device), 2) Should use static/global buffers or circular buffers, 3) Missing bounds checking, 4) Processing entire 40s window instead of sliding window, 5) No memory optimization for embedded constraints, 6) Consider using fixed-point arithmetic instead of float",
    "candidate_answer": "Floats are of 4 bytes and there are arrays with 4000 elements which will be about 16KB of memory. Since we are declaring 9 arrays of this 16KB, the total size will be 144Kb which will all be on the stack memory. The fix is to either reduce use data types that take up less space or process data in chunks instead of storing all in memory. Another is store these large files in SD card or flash and then read small chunks into the RAM. Some arrays are unused so do not store them at all if not needed like spo2_values, filtered_red, temp buffer. The intermediate arrays of ac_component and dc_component are not needed at all; the computation can directly be used for the ratios array hence eliminated those. Int should not be used as the data type as values could be float.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate correctly identified that the memory usage is excessive for the given constraints, but made several critical errors in their analysis and proposed solution. They underestimated the memory usage (144KB vs actual 288KB for floats), failed to address the stack allocation issue properly, and their code contains significant bugs including incorrect indexing, logic errors in the calculation, and improper loop structure. The suggestion to use external storage shows poor understanding of real-time embedded constraints. While the idea of eliminating unused arrays is good, the implementation is flawed."
  },
  {
    "answer_id": 180,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "code_review",
    "question_text": "This Python code implements a Kalman filter for glucose trend prediction from PPG features, but the predictions are oscillating wildly and diverging from actual glucose readings.",
    "expected_answer": "Issues: 1) Mixing PPG prediction and actual glucose updates incorrectly, 2) No validation of PPG-based predictions before using as measurements, 3) Process noise Q is too high causing instability, 4) Not using actual measurement noise for different sources, 5) Should separate prediction and update steps properly, 6) Missing constraint on glucose values (40-400 mg/dL range)",
    "candidate_answer": "Reduce the initial covariance matrix that is there and also there is mixing of PPG predictions and actual glucose measurements. The pricess noise is extremely high, this needs to be much lower. The predictions are separated from actual measurements and feedback is given to correct the predictions. The actual glucose values need to be stored as a separate array to compare at every possible step. ",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate correctly identified some high-level issues (high process noise, mixing predictions/updates) but provided an incomplete and incorrect code fix. The response lacks depth in understanding Kalman filter principles, fails to address all critical issues, and the provided code is syntactically invalid (mixing Python and JavaScript syntax, typos like 'linavg.inv', wrong method signature). There's no proper separation of prediction/update steps, no validation mechanism for PPG predictions, and no handling of edge cases. The solution doesn't demonstrate understanding of how to properly integrate PPG features into the Kalman framework."
  },
  {
    "answer_id": 181,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "code_review",
    "question_text": "This BLE communication code for transmitting encrypted glucose data is failing HIPAA compliance audits due to potential data leakage. The device needs to transmit readings every 5 minutes while maintaining security.",
    "expected_answer": "HIPAA compliance issues: 1) Hardcoded encryption key violates key rotation requirements, 2) ECB mode is insecure - use AES-GCM or CBC with IV, 3) Storing unencrypted PII in local logs, 4) Missing device authentication, 5) No integrity checking (MAC/hash), 6) PHI transmitted without proper de-identification, 7) Missing secure key storage (should use Azure Key Vault pattern)",
    "candidate_answer": "The potential data leak is due to local logs for debugging being written in plaintext or unencrypted form. We also want that the encryption keys rotate frequently so that if one session is compromised not all other also are. Also in ECB encryption, you can perform pattern matching which means if the same text blocks are repeated frequently, the cipher also repeats. The plaintext block should be xored with previous cipher text block or add a set of random bytes per session to ensure randomness and prevent successful decryption. Keeping a hardcoded key will result in HIPAA violation. The key hardcoded will be the master key that can be public; iv is the key that will be agreed upon by the receiver to decrypt. There is one key that needs to be fixed otherwise decryption will be impossible. Another thing that can be added to remove HIPAA compliance is completely remove the user_id that is PII.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate identified some key HIPAA compliance issues but failed to address them comprehensively or provide a correct solution. While they recognized the plaintext logging and hardcoded key problems, their proposed fix introduces new issues and doesn't properly solve the original problems. The code still has critical security flaws and HIPAA violations."
  },
  {
    "answer_id": 164,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "coding",
    "question_text": "Implement a real-time motion artifact detection algorithm for PPG signals. Given 6-channel PPG data (Red, IR, Green wavelengths) sampled at 100Hz and synchronized accelerometer data, write a function that classifies each 5-second window as clean or corrupted by motion artifacts. Your solution must run on an ARM Cortex-M4 with 64KB RAM and complete within 50ms.",
    "expected_answer": "Implement a lightweight classifier using accelerometer-derived motion intensity (RMS of 3-axis acceleration) combined with PPG signal quality metrics (AC/DC ratio, skewness, zero-crossing rate). Use fixed-point arithmetic and circular buffers for memory efficiency. Return 0 for clean, 1 for corrupted.",
    "candidate_answer": "The solution uses spi method to see how much of the ppg signal energy is concentrated at heart rate frequency. If it is around 1, that means it is a clean periodic heartbeat vs if it is further away from 1 it shows it has energy spread across frequencies and the signal is distorted or contains motion artifacts.",
    "kimi2_score": 25.0,
    "kimi2_feedback": "This response demonstrates a fundamental misunderstanding of the problem requirements. The candidate attempts to use spectral peak identification (SPI) for motion detection, which is a valid concept, but the implementation is severely flawed. The code uses floating-point arithmetic throughout, imports scipy (which won't fit on Cortex-M4), uses numpy arrays (memory intensive), and has numerous syntax errors and missing variables. The approach doesn't address the 64KB RAM constraint, 50ms timing requirement, or ARM Cortex-M4 limitations. Critical variables like window_size, fs, acc_thresh, and spi_thresh are undefined. The code won't compile due to syntax errors (missing colons, undefined variables). There's no consideration of fixed-point arithmetic, circular buffers, or memory optimization. The solution completely ignores the multi-channel PPG data structure and synchronized accelerometer requirement."
  },
  {
    "answer_id": 182,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "signal_processing",
    "question_text": "You're processing 6-channel PPG data from the MAXM86146 sensor hub with sampling at 100Hz. Design a filter chain to remove motion artifacts from Red/IR/Green LED readings while preserving the cardiac signal components needed for glucose estimation. The system has strict <50ms latency constraints on Cortex-M4. How would you implement motion artifact removal given simultaneous accelerometer data?",
    "expected_answer": "Implement adaptive filtering using accelerometer data as reference noise. Use a combination of: 1) IIR notch filters (2-4Hz) to remove walking motion, 2) LMS adaptive filter with accelerometer as reference, 3) Downsample-then-filter approach to meet latency. Suggest fixed-point arithmetic for M4 implementation.",
    "candidate_answer": "I would use a bandpass filter with passband as 0.5-5Hz as it will capture the HR fundamentals and first few harmonics. This removes the high frequency noise that might arise. Use accelerometer magnitude as reference noise and then do adaptive filtering. Since there is a latency limit on the system, it is important to downsample the right amount of samples and follow Nyquist criteria. ",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate provides a basic approach but lacks critical technical depth and specificity required for this complex signal processing problem. While they correctly identify the need for filtering and adaptive techniques, they miss key implementation details that would make this solution practical on a Cortex-M4 with strict latency constraints. The response doesn't address the multi-channel nature of the PPG data, specific motion artifact frequencies, or the glucose estimation requirements. The bandpass filter suggestion is too broad and would likely remove important signal components needed for glucose estimation. There's no mention of IIR vs FIR tradeoffs, fixed-point arithmetic optimization, or the specific challenges of real-time adaptive filtering on constrained hardware."
  },
  {
    "answer_id": 183,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "signal_processing",
    "question_text": "For skin-tone adaptive glucose estimation, you have PPG data from 3 wavelengths (Red=660nm, IR=940nm, Green=525nm) and need to normalize signals across different Fitzpatrick skin types (I-VI). Given that melanin absorption varies exponentially with wavelength, design a normalization scheme that compensates for skin tone without affecting glucose-related optical properties.",
    "expected_answer": "Use wavelength-dependent melanin absorption coefficients to create skin-tone correction factors. Implement a ratio-based approach: (Red/IR)/Green ratios normalized by pre-calibrated skin-tone factors. Store correction tables in flash memory for each Fitzpatrick type.",
    "candidate_answer": "The way to normalize is taking into factor a lot of different parameters like skin tone, melanin absorption, time of day, ration between wavelengths. There is an exponential relationship between melanin absorption and wavelength so calculation of these correction factors from melanin absorption data will be needed. Also a normalization using the 3 wavelengths will be needed. ",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate recognizes the key challenge (melanin absorption varying exponentially with wavelength) and mentions using ratios between wavelengths, which aligns with the expected approach. However, the response is vague and lacks specific implementation details. They mention 'taking into factor a lot of different parameters' without explaining how to handle them, and fail to address the critical requirement of compensating for skin tone without affecting glucose-related optical properties. There's no mention of Fitzpatrick skin types, correction tables, or the ratio-based approach (Red/IR)/Green that was expected. The answer reads like a high-level restatement of the problem rather than a concrete solution."
  },
  {
    "answer_id": 184,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "signal_processing",
    "question_text": "Implement a real-time HRV analysis algorithm that extracts RR intervals from PPG peaks in the presence of arrhythmia artifacts. Your algorithm must run on ARM Cortex-M4 with 64kB RAM and produce valid HRV metrics (RMSSD, pNN50) every 30 seconds despite missing/double peaks.",
    "expected_answer": "Use sliding window peak detection with adaptive threshold based on 10-second moving average. Implement outlier rejection for arrhythmia using median filtering on RR intervals. For missing peaks: interpolate based on local rhythm. Cache last 30 peaks for rolling HRV calculation.",
    "candidate_answer": "void calculate_hrv(float*buffer, float*metrics){\nfloat fs=30;\nvector<int>peaks=detect_peaks(buffer,len,fs);\n}",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate provided an extremely incomplete solution that barely addresses any requirements. They only declared a sampling frequency and called a non-existent peak detection function. There's no implementation of the actual algorithm, no handling of arrhythmia artifacts, no missing peak interpolation, no HRV metric calculation, and no consideration of the ARM Cortex-M4 constraints. The code won't compile due to undefined variables and missing includes. This response shows a fundamental lack of understanding of the problem requirements."
  },
  {
    "answer_id": 185,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "signal_processing",
    "question_text": "You discover that your glucose estimation model shows 15% error variance between different skin tones, particularly underestimating for Types V-VI. The PPG signals show amplitude differences but similar waveform morphology. Design a signal processing approach to reduce this bias while maintaining real-time performance.",
    "expected_answer": "Implement wavelength-specific gain compensation using multi-wavelength ratio analysis. Apply Discrete Wavelet Transform (DWT) to separate pulse wave components from tissue scattering. Use energy-normalized wavelet coefficients as features, reducing skin tone dependency while preserving glucose correlation.",
    "candidate_answer": "Perform discrete wavelet transform to perform time frequency domain mapping and extract amplitude over a waveform. The wavelet like the waveform morphology should be used where one example is Daubechies wavelet. The normalization of wavelet coefficients can be done for both approximate and detail coefficients to account for amplitude differences over skin tones. ",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate correctly identified DWT as a key technique and mentioned normalization to address amplitude differences, which aligns with the expected approach. However, the response lacks crucial details about multi-wavelength analysis and doesn't explain how the compensation would specifically target skin tone bias. The answer is too generic and doesn't address the core challenge of maintaining glucose correlation while reducing skin tone dependency. The mention of Daubechies wavelet is appropriate but insufficient without explaining why it's suitable for PPG signals. The response also fails to address real-time performance requirements."
  },
  {
    "answer_id": 167,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "system_design",
    "question_text": "Design a real-time embedded system for the Argus CGM wearable that processes 6-channel PPG data (Red, IR, Green LEDs at 100Hz each) while maintaining <50ms end-to-end latency from sensor sampling to glucose estimation output. The system must handle motion artifacts from accelerometer data and adaptively adjust LED intensity based on skin tone (Fitzpatrick scale). How would you architect the data pipeline on an ARM Cortex-M4 with 256KB RAM and 1MB flash, considering power constraints (must run 7 days on 100mAh battery)?",
    "expected_answer": "Describe a multi-stage pipeline: 1) Sensor hub sampling with DMA buffer management, 2) Motion artifact removal using accelerometer fusion with Kalman filtering, 3) Skin-tone adaptive LED control loop, 4) Discrete Wavelet Transform for denoising, 5) Feature extraction for glucose estimation, 6) Edge ML inference using quantized models. Include power budgeting (LED duty cycling, sleep modes), memory allocation (circular buffers for PPG windows), and real-time scheduling (RTOS with priority queues). Mention BLE notification strategy for glucose alerts.",
    "candidate_answer": "For low latency and power constraints, the tradeoff is between processing capacity and battery life. Sensor sampling needs to be performed where the embedded system samples at a rate greater than Nyquist rate as that is what is needed to reconstruct a signal. Next part is removing noise and distortion by focusing on a particular range of frequencies that the ppg signal falls in. Accelerometer when integrated requires calibration and for best results should be paired with an IMU instead of using it on its own. Since the RAM and CPU on the embedded system, will be limited, it is important to ensure the data is being pushed to an external storage or database instead of on device. While this might lead to security reviews needed, the processing power of the embedded system can be pushed to maximum. Since the sampling rate is quite high, DMA buffers can be used to ensure it is written directly to memory without CPU intervention. To handle the battery constraint, choose a low power microcontroller especially ones with sleep modes and fast wake times. BLE or wireless communication will be the highest power consumer so send multiple frames in one packet whenever possible",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate demonstrates basic understanding of embedded constraints but fails to provide a coherent system architecture. While mentioning some relevant concepts (DMA, sleep modes, Nyquist rate), the response lacks the specific technical depth required for this complex real-time system. Key components like motion artifact removal algorithms, skin-tone adaptive LED control, glucose estimation pipeline, and memory management are either missing or inadequately addressed. The suggestion to push data to external storage contradicts the real-time latency requirement and shows poor understanding of embedded system design. The answer reads more like generic embedded system advice rather than a tailored solution for the CGM use case."
  },
  {
    "answer_id": 168,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "system_design",
    "question_text": "Architect a HIPAA-compliant cloud pipeline that ingests encrypted PPG waveforms, accelerometer data, and glucose estimates from 100K+ Argus devices. Design must handle 500MB/hour peak data ingestion with <30 second patient alert latency for critical glucose events. How do you ensure data lineage, audit trails, and key rotation while maintaining sub-second query performance for medical professionals?",
    "expected_answer": "Design layered architecture: 1) BLE gateway service with device authentication, 2) Streaming ingestion via Azure Event Hub, 3) Real-time processing with Azure Stream Analytics for glucose alerts, 4) Cold storage in Azure Data Explorer with KQL indexing, 5) Key management using Azure Key Vault with CMK rotation, 6) FHIR-compliant API layer. Address data partitioning strategies, encryption at rest/in-flight, and GDPR compliance requirements.",
    "candidate_answer": "The first step to ensure HIPAA compliance is ensuring there is no personally identifiable information anywhere in the pipeline which means removing this information at the setup. In this problem since no where are we adding customer data but instead just the health readings, confidentiality is guaranteed. For cloud pipelines and virtualization there can be cases where noisy neighbor issues happen or because data of multiple customers is on a virtual machine that is running on a single host, data from customers can be compromised. Key rotation should be handled in a way such that the onus of rotating keys should be in customer's control and that the encryption key is only accessible to the customer. The key rotation policies should be enforced by cloud provider based on a frequency the customer is comfortable with. This key should be stored in a safe cloud key vault and linked with stream analytics pipeline to ensure once the key is changed, the new key is used by the analytics pipeline for processing. For audit trails it is important to have telemetry of the system rather than logging the data itself. We should have logging and audits for what are the endpoints accessing each component of the data pipeline, if any system in the distributed setup has broken connection with others for example if we are performing analysis using Stream Analytics and visualizing them, the data source link should not be broken. The alert latency can be kept minimal by automating monitoring of edge cases and failures. This can be either setting up static thresholds or adaptive thresholds. When it comes to scaling, industry standard tools and technology that can handle large scale of data should be used like Prometheus and Grafana which internally performs batching.",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate's response demonstrates a fundamental misunderstanding of HIPAA compliance requirements and lacks the technical depth needed for this system design challenge. The answer focuses on high-level concepts without providing concrete architectural details or addressing the specific technical requirements. Key issues include: 1) Incorrect assumption that removing PII automatically ensures HIPAA compliance (HIPAA requires protecting all PHI, including health readings), 2) No mention of actual data ingestion architecture (Event Hub, Kinesis, etc.), 3) Missing specific latency guarantees (<30 seconds for alerts), 4) No discussion of data partitioning strategies for 500MB/hour throughput, 5) Vague mention of 'industry standard tools' without specific implementation details, 6) Failure to address sub-second query performance requirements, 7) No concrete FHIR compliance strategy, 8) Missing data lineage implementation details. The response reads more like general cloud security advice than a system design solution."
  },
  {
    "answer_id": 169,
    "candidate_id": 6,
    "candidate_name": "ADITI OAK",
    "category": "system_design",
    "question_text": "Design a fault-tolerant firmware update system for Argus devices that must maintain continuous glucose monitoring during OTA updates. The system should handle power failure during updates, verify firmware signatures for FDA compliance, and support rollback. How do you ensure zero-downtime glucose monitoring while updating 128KB firmware image over BLE with 20ms connection intervals?",
    "expected_answer": "Implement A/B partition scheme with: 1) Dual-bank firmware storage, 2) Incremental chunked downloads (4KB blocks), 3) CRC verification per chunk, 4) Power-loss recovery using checkpointing, 5) Signature verification using ECDSA, 6) Seamless rollback via bootloader. Include BLE flow control, battery monitoring during update, and failsafe glucose estimation fallback mode.",
    "candidate_answer": "Firmware updates should be first designed in a way to ensure it does not break workings of old versions. To ensure safe recovery if update takes place when power is lost, ensure to have a version control history where firmware sets back to last working version on power failure. Also a strategy should be added where a checkpoint is created for when a power failure happens so that when the power is restored the rest of the update can be picked up from that checkpoint onwards. A/B testing should be performed on the devices where results of the update made on the new set of devices should be compared with results of the devices with old versions. Updates should be performed first on test devices to ensure everything is working as expected. After that, a migration strategy should be used wherein, updates are pushed first to devices that do not have frequent usage so that if something is not caught through testing, in the worst case the risk tolerance of failing can be a light higher for devices that are not as frequently used. Ensure that all the updates needed will be non blocking upgrades. To ensure the glucose monitoring is still ongoing, dual partitioning might also be useful. ",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate demonstrates some understanding of firmware update safety but lacks technical depth and specificity for a medical device context. While they mention concepts like rollback and checkpoints, they don't address the core technical challenges: how to maintain continuous glucose monitoring during updates, the specific BLE constraints (20ms intervals, 128KB image), or FDA compliance requirements. The response mixes high-level deployment strategies (A/B testing as testing methodology vs. A/B partition scheme) with vague technical concepts without concrete implementation details. Critical gaps include: no mention of firmware signature verification, dual-bank partitioning implementation, BLE flow control specifics, or how to handle the 128KB/20ms constraints. The 'version control history' suggestion is impractical for embedded systems with limited storage. The answer needs significant technical depth to be considered adequate for a medical device firmware update system."
  },
  {
    "answer_id": 10,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "brain_teaser",
    "question_text": "You have a 1024-channel retinal implant with 1 MHz bandwidth per channel, needing to transmit spike-sorted neural data in real-time. Design a compression scheme that preserves spike timing accuracy while achieving 100\u00d7 bandwidth reduction. Consider that spikes are 1-2 ms wide and timing errors >0.5 ms significantly degrade vision quality.",
    "expected_answer": "Multi-stage approach: 1) Compressed sensing with spike-specific dictionary (exploiting sparsity), 2) Adaptive delta modulation for residual timing, 3) Hardware-friendly Huffman coding. Should mention FPGA implementation with <1 ms latency and power <1 mW.",
    "candidate_answer": "The question is somewhat vague, so I am going to interpret it as follows. A spike is just a voltage deflection on a channel exceeding some threshold. We are not interested in attributing spikes to neurons. Since we can tolerate up to 0.5ms error in spike times, I would design a system which flips a bit if the voltage on a channel ever exceeds some threshold, and then I would reset this bit every 0.5ms. This is similar to an event camera. Since spikes are 1-2ms wide, after the first detection of a spike, I would invalidate the recording on 3-4 bins that come after the first detected spike.\n\nThen, to further increase compression, I would transmit the time between events rather than absolute event times themselves. I would have some looping counter with a period far greater than the expected time between subsequent spiking events.\n\nI would do this for each channel independently, though we could do interesting things with nearest neighbor electrode compression given the same neural spike will be recorded at multiple adjacent electrodes. Since the raw data is 1MHz, just thresholding on 0.5ms time bins is already a 500x compression of the data. But the actual final compression will depend on the frequency of spiking events.\n\n",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate provided a creative approach using event-based compression with temporal binning, achieving significant compression through threshold detection and delta encoding. However, the solution misses several critical aspects required for retinal implants: it doesn't address spike sorting (mentioned in the question), lacks consideration for spike waveform preservation (important for distinguishing neurons), and doesn't provide the requested 100\u00d7 compression guarantee. The 500\u00d7 compression claim is misleading as it assumes perfect threshold detection without accounting for noise or overlapping spikes. The solution also lacks discussion of hardware constraints (FPGA implementation, power budget) and doesn't validate timing accuracy preservation. While the event camera analogy is clever, it oversimplifies the neural signal processing requirements."
  },
  {
    "answer_id": 11,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "brain_teaser",
    "question_text": "Your Hodgkin-Huxley parameter inference algorithm runs on IBM POWER9 with 4\u00d7SMT8 cores. Profile shows 80% time in Jacobian computation for 2000-neuron network. Design a vectorization strategy using POWER's VSX instructions that maintains numerical stability while exploiting the 128-bit vector width.",
    "expected_answer": "Vectorize across neurons using VSX intrinsics (vec_mul, vec_madd), pack 4 Hodgkin-Huxley variables into vector lanes, use FMA for Jacobian updates. Should reduce to <0.5ms with SIMD and cache blocking.",
    "candidate_answer": "All that the code here is doing is computing the outer product of 2 length-4 vectors for each neuron. The \"i\" corresponding to the specific neuron we are considering is not even used in the Jacobian calculation. ie All neurons will have the same Jacobian. I am assuming that this a bug and that actually different neurons will have different Jacobians. \n\nWhat we could do is run all neurons in parallel to one another on different threads. 4 SMT8 cores have 32 threads cumulatively and each thread could be dedicated to computing a separate neuron. So we could batch process the 2000 neurons in batches of 32. Now for each neuron we need to efficiently compute the outer product of 2 length 4 vectors.\n\nI don't know off the top of my mind what the exact name of the vector multiple instruction is. But we would use that to compute this outer product. If we are using 32 bit floats, we can store 4 of them in a register if the vector width is 128 bits. Then we can compute the product of the vector with the 4 scalars one at a time and then assemble them into the final 4x4 matrix corresponding to the Jacobian for that one neuron.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate correctly identified that the naive implementation is inefficient and recognized the outer product structure, but missed the key insight about vectorizing across neurons rather than within a single neuron's computation. The suggestion to use 32 threads for 2000 neurons (62.5 neurons per thread) is reasonable for parallelization, but fails to leverage the vectorization potential of VSX instructions. The candidate didn't demonstrate understanding of how to pack data for SIMD operations or use FMA instructions for numerical stability. The response lacks specific POWER9/VSX knowledge and doesn't address the cache hierarchy or numerical stability requirements. While the parallelization approach is valid, it doesn't achieve the expected <0.5ms performance target through vectorization."
  },
  {
    "answer_id": 12,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "brain_teaser",
    "question_text": "In a 50-neuron spike-for-spike reproduction experiment, you discover that electrode cross-talk creates 15% false positive spikes in adjacent channels. Given that true spikes are Poisson(\u03bb=30Hz) and false spikes are Poisson(\u03bb=4.5Hz), design an optimal filtering strategy that maximizes true positive rate while keeping false discovery rate <5%.",
    "expected_answer": "Bayesian approach: Model cross-talk as linear mixing matrix A, solve inverse problem y = Ax + n. Use variational inference with spike-and-slab priors. Should mention EM algorithm for blind source separation.",
    "candidate_answer": "This question is ill-posed. There is a distinction between spikes in neurons and spikes in channels. I'm going to ignore the first part of the question.\n\nLet's say I have real spikes arriving across the array at 30Hz and 15% of the time a spike is recorded on an adjacent channel. This would imply that whenever a false positive occurs, the false positive spike will be co-located With a true spike.\n\nAssuming the number of channels is much larger than the number of neighbors per channel and we have no additional information about a spike other than its time, the optimal matching algorithm is just to reject one of the spikes whenever a spike occurs on two neighboring channels. The likelihood that a spike with no neighbor is real will be close to 100% and the likelihood that 2 neighboring spikes will be real in this case will be very low (<<50%), so we should reject one of them. \n\nThere will exist some critical number of channels such that if we are below, the optimal strategy will just be to classify all spikes as being real. This is when the probability of neighboring spikes being real exceeds 15%. \n\nIf we had information about amplitude profile of cross talk spikes, we can incorporate this in some sort of Bayesian prior. And then use this prior to evaluate the probability of each spike being real. Then classify the spike as real or not real on a 50% threshold wrt this probability.\n\n\n\n",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate fundamentally misunderstood the problem setup and dismissed the actual question as 'ill-posed'. The core issue is modeling cross-talk as a Poisson process with \u03bb=4.5Hz (15% of 30Hz), not as a deterministic 15% co-occurrence rate. The candidate's approach assumes perfect temporal alignment of spikes, which contradicts the Poisson process description. The solution proposed is a simple neighbor-rejection heuristic that doesn't address the statistical nature of the problem or the false discovery rate constraint. While mentioning Bayesian priors at the end shows some awareness, it wasn't integrated into the actual solution. The response lacks mathematical rigor and doesn't use the provided parameters correctly."
  },
  {
    "answer_id": 13,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "brain_teaser",
    "question_text": "Your retinal implant's analog front-end sees 5 mV neural spikes riding on 100 mV stimulation artifacts that decay with \u03c4=2ms time constant. Design a mixed-signal cancellation circuit that can resolve 50 \u03bcV spikes during 200 \u03bcs stimulation pulses, with <1 \u03bcW power budget per channel.",
    "expected_answer": "Switched-capacitor adaptive filter: 1) Sample-and-hold the artifact template, 2) Continuous-time subtraction with gm-C filter (\u03c4=2ms), 3) Dynamic element matching for offset cancellation. Should include noise analysis and mention chopper stabilization.",
    "candidate_answer": "I would not implement an analog circuit for cancellation if the artifact is this simple. I am assuming the receiver chain already has clamping/blanking during stimulation to prevent amplifier saturation. A 100mV stimulation artifact would saturate recording if required precision is on the order of 50uV.\n\nAfter this, I would then apply a simple high pass digital filter to the resulting recording to remove as much of the exponential decay artifact. Then, I would implement partial template matching using the tool I developed in my research called PartialSort.",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate completely missed the core requirement of designing a mixed-signal cancellation circuit. Instead of providing an analog front-end solution, they dismissed the analog domain entirely and proposed a digital-only approach. This fails to address the fundamental challenge of preventing amplifier saturation during stimulation. The response shows poor understanding of mixed-signal design principles and the constraints of neural recording systems. While acknowledging the saturation issue is good, the proposed digital filtering approach cannot recover information lost during amplifier saturation. The mention of 'PartialSort' appears to be self-promotion rather than a relevant technical solution. The power budget constraint (<1 \u03bcW per channel) makes digital processing impractical, and the candidate failed to address this critical constraint."
  },
  {
    "answer_id": 1,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "code_review",
    "question_text": "Review the following code and identify all bugs. Explain what's wrong and how to fix each issue.",
    "expected_answer": "1) calculate_average: Division by zero if empty list. 2) find_duplicates: Compares element with itself (j should start at i+1), adds duplicates multiple times. 3) reverse_string: Index out of range (should be len(s)-1), uses reserved keyword 'reversed'.",
    "candidate_answer": "calculate_averages should work if an iterable object of numbers is passed to it. However, it will fail if something else is passed or if the utterable is empty. I added in code to check for these cases and simplified the function. See above\n\nfind_duplicates will always double count every duplicate at the minimum since it iterates through every element in the nested for loop. It also runs in O(n^2) with the size of the list. I implemented it much faster with dictionaries, which should also eliminate the double count issue. See above\n\nreverse_string starts the for loop with len(s). However s[len(s)] is not a valid element, so this will throw an access error. The last element is s[length(s) - 1]. I fixed the bounds of the for loop to address this. The terminating condition/element should be -1. See above",
    "kimi2_score": 85.0,
    "kimi2_feedback": "The candidate correctly identified the main bugs in all three functions and provided working fixes. They caught the division by zero in calculate_average, the double-counting and inefficiency in find_duplicates, and the index out of bounds error in reverse_string. The solutions are efficient and correct. However, the candidate missed mentioning the reserved keyword 'reversed' issue in reverse_string, and their calculate_average fix has redundant checks (both 'if not numbers' and 'if len(numbers) == 0' serve the same purpose). The explanation was somewhat vague ('see above' references without actual code shown initially), though the provided solutions demonstrate good understanding."
  },
  {
    "answer_id": 2,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "code_review",
    "question_text": "This code has performance issues. Identify them and suggest improvements.",
    "expected_answer": "1) find_common_elements: O(n*m) due to 'in' on list. Use sets for O(n+m). 2) count_words: Manual dictionary lookup is O(n) per word. Use dict.get() or collections.Counter.",
    "candidate_answer": "find_common_elements runs in O(n * m) where n and m are the number of elements in each list. We can make this run in O(n) by using sets since they implement hash tables for large lists. We can check if an element exists in a set in O(1) time. I converted both lists to sets which can be done in O(n) first and then searched for duplicates.\n\ncount_words is unnecessarily looping through the word_count dictionary to check if the word is already there, which takes O(n). You can check if the word is already there in O(1) time since it's a dictionary.\n\n",
    "kimi2_score": 85.0,
    "kimi2_feedback": "The candidate correctly identified the performance bottlenecks in both functions and provided optimized solutions. They accurately explained the time complexity issues (O(n*m) for list lookup and O(n) for manual dictionary iteration) and proposed appropriate fixes using sets and dictionary operations. The provided code solutions are correct and efficient. However, the explanation could be more precise - they mentioned O(n) for the set intersection but didn't clarify it's actually O(min(n,m)) for the intersection operation itself. The count_words solution is good but missed the opportunity to mention collections.Counter as an even more Pythonic approach."
  },
  {
    "answer_id": 3,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "code_review",
    "question_text": "Identify the security vulnerabilities in this code and explain how to fix them.",
    "expected_answer": "1) SQL Injection: Use parameterized queries. 2) Command Injection: Use subprocess with shell=False or escape input. 3) Path Traversal: Validate filename doesn't contain '..' or absolute paths.",
    "candidate_answer": "get_user is vulnerable to SQL injection. get_user uses an f string for the query, so an attacker could provide a malicious SQL syntax input into the query to do all sorts of attacks. I modified the code to use a parametrized query so that the username can only be treated as a string literal to search for in the database instead of executed as SQL code.\n\nThe run_command function has a similar issue. It is intended to just print some provided string to stdout, but with a malicious input, you can make the system execute any command. for examples: user_input=\"hi; rm -rf /\" will print hello and then delete the entire file system. Just put a semicolon and then you can execute any command on this system. I fixed this by just swapping out the os system call with a print. All echo does is write to stdout, which can be done with print.\n\nread_file will allow an attacker to read any file in the file system, not just files in data as is intended. for example. you could have filename = \"../secure_info.txt\" and read from another directory. I just added a check that the filename requested is actually in the data directory to circumvent this issue.\n",
    "kimi2_score": 85.0,
    "kimi2_feedback": "The candidate correctly identified all three major security vulnerabilities (SQL injection, command injection, and path traversal) and provided working fixes for each. The explanations were clear and demonstrated understanding of how these attacks work. However, the implementation for path traversal validation has a significant flaw - using os.listdir() only checks against existing files, which could miss valid files or allow access to subdirectories. A more robust approach would be to use os.path.abspath() with path normalization and ensure the resolved path starts with '/data/'. Additionally, the candidate could have mentioned input sanitization as a defense-in-depth strategy and discussed the principle of least privilege."
  },
  {
    "answer_id": 7,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "coding",
    "question_text": "Design and implement a real-time spike sorting system for multi-electrode array recordings that must handle stimulus artifacts without blanking or interpolation. The system should process 1024 channels at 30 kHz with <5ms latency while simultaneously estimating neural response thresholds. Use PyTorch for the ML components and optimize for IBM POWER9 architecture with vector-scalar instructions.",
    "expected_answer": "Solution should include: 1) CUDA/PyTorch tensor operations optimized for POWER9 VSX, 2) Online spike detection using rolling window PCA with artifact-aware templates, 3) Differentiable clustering in Jax for real-time parameter updates, 4) SIMD-optimized preprocessing in NumPy, 5) Hardware-specific optimizations using PowerISA intrinsics via Numba. Must demonstrate <5ms end-to-end latency in benchmarks.",
    "candidate_answer": "Solution choices are explained in code comments. This pseudocode estimates a neural threshold with Bayesian optimal experimental design.",
    "kimi2_score": 25.0,
    "kimi2_feedback": "This response fundamentally misunderstands the requirements. The question asks for a real-time spike sorting system to process 1024 channels at 30 kHz with <5ms latency, handling stimulus artifacts without blanking/interpolation. Instead, the candidate provided pseudocode for a single-cell threshold estimation experiment using Bayesian methods. There's no implementation of spike sorting, no handling of 1024 channels, no 30 kHz processing, no latency optimization, and no POWER9-specific optimizations. The code is high-level pseudocode for a completely different problem - estimating neural response thresholds via stimulation rather than sorting spikes from multi-electrode recordings. The solution doesn't address any of the core technical challenges: real-time processing, artifact handling, parallel processing of 1024 channels, or hardware-specific optimizations."
  },
  {
    "answer_id": 9,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "coding",
    "question_text": "Implement a closed-loop retinal prosthesis controller that: 1) Performs real-time spike sorting on 256-electrode array data, 2) Predicts neural responses using inferred HH models, 3) Optimizes stimulation patterns for target spike trains via Bayesian experimental design. Must run on embedded ARM+FPGA system with <2ms latency per electrode.",
    "expected_answer": "Complete implementation with: 1) FPGA-based spike detection using VHDL threshold crossing with adaptive filters, 2) CPU-based Bayesian optimization using Jax+Scipy, 3) PyTorch model server for HH predictions, 4) Optimal experimental design using spectral clustering of electrode responses, 5) Linux real-time scheduling with PREEMPT_RT. Include latency profiling and FPGA resource usage (LUTs, DSPs).",
    "candidate_answer": "Comments explain rationale. But basically we use a differentiable simulator for Hodgkin Huxley models forward hh_model_forward which allows us to estimate parameters With gradient descent. We would run this algorithm off system in the cloud. We only allocate stimuli on the ARM+FPGA system.",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate completely missed the core requirements of the question. They focused only on HH model parameter fitting using gradient descent, ignoring the critical real-time constraints, FPGA implementation, spike sorting, and closed-loop control aspects. The answer describes an offline parameter estimation approach that runs 'in the cloud' rather than the required embedded system implementation. There's no mention of the 2ms latency requirement, FPGA hardware-software partitioning, real-time spike sorting, or Bayesian experimental design for stimulation optimization. The code provided is just a basic gradient descent loop for model fitting, not the complete retinal prosthesis controller requested. The candidate's claim that they 'already described spike sorting and Bayesian experimental design in Question 1' is insufficient - this question requires specific implementation details for the embedded system."
  },
  {
    "answer_id": 14,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "signal_processing",
    "question_text": "You are designing a real-time closed-loop retinal prosthesis system running on an FPGA+ARM SoC. The DSP pipeline must: (1) spike-sort 256-channel high-density MEA data sampled at 30 kHz, (2) estimate instantaneous firing rates, (3) compute optimal stimulation waveforms via gradient descent on a differentiable Hodgkin-Huxley model in under 2 ms, and (4) drive a 16-electrode current-steering DAC array. How do you architect the memory hierarchy (on-chip BRAM vs DDR vs cache) and choose fixed-point vs floating-point for each stage to meet power (<50 mW) and latency (<3 ms) budgets?",
    "expected_answer": "Use BRAM ping-pong buffers for raw MEA data (int16) and spike-sorting features (int8). Perform spike sorting with fixed-point FFT + spectral clustering in FPGA fabric; use float16 for HH gradients in ARM NEON. Cache HH parameters (float32) in L2. Double-buffer stimulation waveform tables in BRAM. Estimate 6\u20138 MB/s sustained DDR BW.",
    "candidate_answer": "Since we are on FPGA, I will use fixed-point computation for all steps that do not require a large dynamic range. And I will use the minimum number of bits wherever possible. All computation on the ARM SoC I would elect to do in floating point for simplicity.\n\nThe general principle for memory hierarchy on FPGA is to keep data that will be repeatedly accessed in BRAM and to keep data that will be accessed less frequently off chip in DRAM. First I will simulate my application dataflow to calculate how much data is in high reuse over short time scales. I will size my allocation of FPGA BRAM appropriately to meet this minimum demand. Then I will write my software to put this highly used data in BRAM swapping out through cache as the application evolves and data needs change.",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate provides only high-level principles without concrete architectural decisions for this specific prosthesis system. They fail to address the critical design constraints: 256 channels at 30 kHz, 2 ms processing deadline, 50 mW power budget, and the differentiable Hodgkin-Huxley model requirements. The answer lacks quantitative analysis of memory bandwidth, bit-width choices, or latency calculations. The vague statement about 'minimum number of bits' and 'floating point for simplicity' on ARM shows insufficient understanding of the power/performance trade-offs in neural prosthesis systems. No mention of ping-pong buffering, cache hierarchy optimization, or the gradient descent computation requirements. The response reads like generic FPGA advice rather than a solution tailored to this real-time neural interface."
  },
  {
    "answer_id": 15,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "signal_processing",
    "question_text": "Review the following PyTorch/Jax code intended to fit a 3-compartment Hodgkin-Huxley model to 128-electrode voltage maps. Identify numerical instabilities and concurrency hazards that would crash a 1000-neuron batch optimization on TPU.",
    "expected_answer": "Fixes: (1) inplace update 'v = v.at...' causes copy\u2014use lax.scan. (2) g unconstrained \u2192 NaNs; apply softplus. (3) dt=0.025 violates CFL for HH; reduce to 0.005 ms. (4) No TPU parallel mapping; add pmap axis. (5) float32 accumulation overflows; cast to float64 for residuals.",
    "candidate_answer": "This code is using a forward Euler solver and also it seems that it is not even computing axial currents. So it's not even a \"3-compartment HH model\". Its effectively 3 single-compartment models.\n\nForward Euler is known to be numerically unstable for multi compartment Hodgkin Huxley simulations. The timestep dt would need to be very small here depending on the value of g. We can implement a backward Euler solver or an exponential Euler solver which would achieve equivalent numerical stability with larger time steps. \n\nI do not see any concurrency hazards in the code. The for loop cannot be optimized away, this is just part of solving HH models.",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate correctly identified that the code doesn't represent a true 3-compartment Hodgkin-Huxley model and pointed out the numerical instability of forward Euler. However, they completely missed the critical JAX-specific issues that the question explicitly asked about: numerical instabilities from inplace updates, unconstrained parameters leading to NaNs, TPU parallel mapping requirements, and float32 overflow hazards. The claim that 'there are no concurrency hazards' is particularly concerning given the explicit use of JAX/jit and TPU context. The response focuses on physics/modeling issues rather than the computational/parallel computing problems that would cause crashes on TPU."
  },
  {
    "answer_id": 16,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "signal_processing",
    "question_text": "You must implement a non-uniform FFT (NUFFT) to reconstruct 3D retinal electric fields from irregular MEA electrode positions in <1 ms on IBM POWER10. Given 256 arbitrary (x,y,z) coordinates and a 128\u00b3 target grid, design the kernel using VSX intrinsics. How do you choose gridding convolution kernel width, oversampling ratio, and cache-blocking strategy to hide DDR latency?",
    "expected_answer": "Use Kaiser-Bessel kernel with width=3, oversample=2, pre-compute on-chip LUT in L3 cache. Block 8\u00d78\u00d78 voxels per VSX register tile. Overlap gridding with PCIe transfers using POWER10\u2019s 16-way SMT prefetch threads. Achieve ~0.7 ms for single precision.",
    "candidate_answer": "I would choose the minimum gridding convolution kernel width and oversampling ratio that ensures sufficient correctness for the computation. The number of FLOPs scales with the cube of both of these parameters, so we want to minimize those. \n\nTo hide DDR latency, we want to pipeline the computation and prefetch data for the next block in the pipeline while the current block is executing. \n\nI think the optimal scheme would split the computation of the intermediate oversampled grid from the final fft on the actual grid. So in one stage of the pipeline, I would sequence the intermediary oversampled grid into slabs and then compute those values by convolving the gridding kernel with the relevant nonuniform points. While that is happening, I would begin loading the data needed for the next slab in parallel. Then when this slab is complete, I would compute these points contribution to the final FFT result, storing the final accumulated result in a dedicated section of DRAM. I would do this computation in parallel with the computation of the intermediate points of the next slab.\n\nI would pick the size of the slab so that both the DRAM bandwidth is fully utilized and all cores are active. I would use the appropriate VSX SIMD instructions for computing the convolutions, which would be vector multiply accumulates.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate demonstrates understanding of basic NUFFT concepts and latency hiding through pipelining, but fails to provide specific technical details required for the POWER10 architecture. The response lacks concrete values for kernel width/oversampling ratio, doesn't address the cache hierarchy (L3 vs DDR), and doesn't leverage POWER10-specific features like VSX intrinsics or 16-way SMT. The slab-based approach is conceptually correct but needs quantification for the 128\u00b3 grid with 256 electrodes. Missing critical details: specific kernel choice (Kaiser-Bessel), cache-blocking strategy (8\u00d78\u00d78 tiles), and the 0.7ms performance target. The answer reads as generic parallel computing advice rather than a POWER10-specific optimization."
  },
  {
    "answer_id": 17,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "signal_processing",
    "question_text": "Design a Kalman-filter-based closed-loop controller for a retinal implant that adapts stimulation amplitude in real time to maintain target firing rate despite slow electrode impedance drift (\u03c4\u224860 s). Your filter must run at 1 kHz but converge within 2 s after sudden 50 % impedance drop. How do you tune process-noise covariance Q and measurement-noise R when neural SNR is only 3 dB and spike sorting has 5 % false-positive rate?",
    "expected_answer": "Model impedance as random-walk with Q=1e-5. Set R via EM algorithm using false-positive statistics: R_diag = \u03c3\u00b2_spike_sort + (0.05*\u03bb)\u00b2. Use square-root UKF for numerical stability and vectorized PyTorch ops on GPU. Reset covariance trace every 500 ms to accelerate convergence.",
    "candidate_answer": "Since we know the unreliability of our measurements, (5% false positive error rate), we can directly set the R parameter in our Kalman filter according to this error.\n\nHow we tune Q should be based on the physical reality of how firing rate drifts over time. But since we want convergence after 2 seconds this is only possible if 3 Kalman time constants is on the order of 2 seconds. The Kalman time constant depends on the ratio of Q and R, so we set Q to be the largest possible value that still ensures convergence in 2 seconds. This may not be the physical reality however in which case our estimation of the true firing rate will be incorrect.",
    "kimi2_score": 50.0,
    "kimi2_feedback": "Unable to evaluate automatically. Manual review required."
  },
  {
    "answer_id": 4,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "system_design",
    "question_text": "Design a closed-loop retinal stimulation system for 10,000-electrode arrays that must achieve real-time spike-for-spike pattern reproduction at 1 kHz update rates. The system must handle electrical artifact suppression, biophysical parameter inference, and optimal experimental design simultaneously. How would you architect the hardware-software co-design to achieve <1ms latency from neural recording to stimulation?",
    "expected_answer": "Candidate should propose a layered architecture: 1) Custom ASIC for analog front-end with integrated artifact suppression circuits, 2) FPGA-based real-time DSP pipeline for spike detection using learned templates from Hodgkin-Huxley models, 3) GPU cluster for biophysical parameter inference using differentiable Jax simulations, 4) Edge AI accelerator for optimal electrode selection using spectral clustering. Must address memory bandwidth challenges (100 GB/s for 10k channels), power constraints (<50 mW implant power budget), and thermal management. Should mention RF telemetry for uplink/downlink and VHDL implementation of critical paths.",
    "candidate_answer": "<1ms latency from neural recording to stimulation is not the metric I would optimize for. Firstly, I would perform the entirety of spike sorting/threshold estimation with optimal experimental design and biophysical parameter inference entire off the implanted device entirely on high performance servers.\n\nMy system would be as follows: 1. an implanted MEA chip with addressable stimulation and recording channels. I would have minimal compute on this device for event based voltage recording compression and transmission of raw voltage recording data to an intermediary node outside of the body. Then I would stream this data with no modification to an external cloud server where all spike sorting and biophysical estimation of the tissue is performed. For optimal experimental design based calibration of stimulus responses, I would set a batch size to be a multiples of the latency to transfer data to the server and process it. So for example, if the algorithm takes ~10 seconds to stream raw data to the server and then compute the next set of optimal stimuli to calibrate the device, then I would send ~20 seconds worth of measurement stimuli at a time. I would size this to ensure that the implanted device is always being 100% utilized to measure properties of the system.",
    "kimi2_score": 15.0,
    "kimi2_feedback": "This response fundamentally misunderstands the core requirements of the system. The candidate proposes a cloud-based architecture that cannot possibly meet the <1ms real-time latency requirement for spike-for-spike pattern reproduction. The design completely misses the critical constraint of real-time closed-loop operation at 1kHz, instead opting for batch processing that would introduce multi-second delays. The solution fails to address any of the key technical challenges: custom ASIC design for artifact suppression, FPGA-based real-time processing, GPU acceleration for biophysical modeling, or edge AI for electrode selection. The power budget and thermal constraints are ignored, and there's no mention of hardware-software co-design. The candidate's approach would result in an open-loop system rather than the required closed-loop system."
  },
  {
    "answer_id": 5,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "system_design",
    "question_text": "Design a scalable spike sorting system for high-density multi-electrode arrays (10,000+ channels) that must continuously adapt to electrode drift and maintain <5% classification error. How would you architect the system to handle streaming data at 30 kHz sampling while performing online learning of neural templates?",
    "expected_answer": "Should describe distributed processing architecture: edge devices for initial denoising and compression, cluster for template learning with PyTorch, and hierarchical storage system. Must address: 1) Incremental EM algorithm for template updates, 2) GPU-accelerated template matching with CUDA kernels, 3) Metadata system for electrode drift tracking using Pandas, 4) Load balancing across compute nodes, 5) Real-time quality metrics using spectral clustering. Should mention use of NumPy for efficient template storage and SciPy for drift compensation.",
    "candidate_answer": "The standard approach to online learning of neural templates is to 1. bandpass filter raw data, 2. detect spikes with threshold crossing, 3 align spikes and extract their features, 4. incorporate the spike in the clustering scheme (ie create a new unit or match the template to an existing unit). I would learn neural templates online by taking an exponential moving average of all previously recorded spikes assigned to the unit and the new spike. The time constant of the moving average will be selected with respect to noise level in new spikes and expected rate of the drift. This is essentially using a Kalman filter.\n\nTo make this system scalable to large electrode arrays and avoid operations that scale superlinearly with the number of electrodes, I would simply break up the electrode array into fixed size overlapping chunks. Neural spikes are only recorded on electrodes spatially proximal to the cell. So we can chunk the array along clusters of cells and run the online spike sorting in parallel on each chunk. Assuming additional compute resources, the time to perform online spike sorting will not scale with array size.\n\n",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate provides a basic approach to spike sorting but fails to address the core scalability and architectural challenges of a 10,000+ channel system. While the exponential moving average approach is valid for template updates, it lacks the sophistication needed for <5% classification error under electrode drift. The 'chunking' strategy is overly simplistic and doesn't address distributed processing, GPU acceleration, or the streaming data requirements at 30 kHz. The response completely misses key technical requirements like CUDA kernels for template matching, hierarchical storage systems, load balancing, and real-time quality metrics. There's no mention of PyTorch, distributed computing, or the metadata system for drift tracking. The solution appears to be a single-machine approach that wouldn't scale to the specified requirements."
  },
  {
    "answer_id": 6,
    "candidate_id": 1,
    "candidate_name": "Amrith Lotlikar",
    "category": "system_design",
    "question_text": "Design a wireless communication system for transmitting 10 Gbps of compressed neural data from an implanted device while maintaining <5 mW power consumption. The system must operate through biological tissue at 2.4 GHz with adaptive modulation. How would you optimize the RF link budget and implement error correction?",
    "expected_answer": "Should propose MIMO antenna design for tissue penetration, adaptive QAM modulation based on link quality, LDPC codes for error correction, and duty cycling. Must address: 1) Tissue channel modeling using measured S21 parameters, 2) Power-efficient modulation switching (BPSK/QPSK/16-QAM), 3) Compressed sensing for neural data reduction (10:1 ratio), 4) Packet structure optimized for neural data bursts, 5) Hardware implementation on custom RF ASIC with integrated power management. Should mention use of IBM POWER architecture for baseband processing and VHDL for RF front-end control.",
    "candidate_answer": "10Gbps uplink for an implanted device on a 5mW power budget is orders of magnitude more than any modern system. The requirement is absurd. From my understanding, modern WiFi 6 systems which operate at 2.4GHz consume on the order of Watts of power to achieve 1Gbps speed. The wireless link my lab is designing for our implanted retinal device achieves ~1Mbps on a 10mW budget. I will present some general strategies for optimizing RF link budget in the context of implanted medical devices.\n\nFirstly, I would not use 2.4GHz. Lower frequencies have better penetration depth. 400MHz is often chosen for implanted medical devices.\n\nGiven we are designing a transmitter in an extreme low power domain (<5mW), we most likely do not have the compute budget required to perform channel estimation for adaptive modulation. Also, we can fix the external receiver's location so channel estimation is unnecessary. I would then err on the side of hard coding a modulation scheme and offload as much work to the decoder, which is outside the body and shouldn't be as power limited. \n\nI would start by implementing a QPSK modulation scheme with a single carrier using the full allotted bandwidth. Then, I would use a Hamming code or bit repetition to introduce redundancy in encoding for error correction. Then on the receiver side I would implement the appropriate decoder.\n\nI would test this system empirically, and then boost the aggressiveness of the wireless modulation and Hamming code until I reach some unacceptable bit error rate. So if QPSK is too conservative, I would then try 16QAM and so on. Or I could scale the Hamming code to have fewer parity bits.\n\n\n\n\n",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate correctly identified that the 10 Gbps at 5 mW requirement is unrealistic, which shows good engineering judgment. However, the proposed solution is fundamentally inadequate for the problem. The candidate dismissed the 2.4 GHz requirement without justification and proposed a much simpler system (QPSK + Hamming codes) that would achieve perhaps 1-10 Mbps at best - orders of magnitude below the target. The response lacks the sophisticated system design expected for this senior-level question, failing to address adaptive modulation, MIMO, advanced error correction (LDPC), or the specific neural data optimization requirements. The approach is too conservative and doesn't demonstrate the creative engineering thinking needed to approach such an extreme specification."
  },
  {
    "answer_id": 424,
    "candidate_id": 13,
    "candidate_name": "Chloe Zhong",
    "category": "brain_teaser",
    "question_text": "You're debugging Argus's multi-wavelength PPG system where Red/IR/Green LEDs show perfect glucose correlation in lab phantoms but completely fail on human trials. The failure pattern is curious: subjects with Fitzpatrick skin types I-II work fine, but types IV-VI show inverse glucose trends. The team suspects your motion artifact removal algorithm is skin-tone dependent. Given that melanin absorption peaks at 335nm but affects all visible wavelengths, and your LEDs operate at 525nm (Green), 660nm (Red), and 940nm (IR), how would you mathematically prove whether this is a melanin interference issue vs. a fundamental flaw in your Beer-Lambert law application for glucose estimation? Assume your algorithm uses a linear combination of wavelength ratios.",
    "expected_answer": "Calculate melanin's absorption coefficient at each LED wavelength using the empirical formula \u03bc_a(\u03bb) = \u03bc_0*(\u03bb/335nm)^(-3.2). For Fitzpatrick scale, model melanin concentration as C_m = [0.5, 1.0, 1.5, 2.5, 4.0, 6.0] mg/ml for types I-VI. Show that the ratio (\u03bc_a_green - \u03bc_a_IR)/(\u03bc_a_red - \u03bc_a_IR) becomes non-linear at higher melanin concentrations, causing your linear glucose estimation to flip sign. Demonstrate this creates a systematic error where glucose appears to decrease when it should increase for types IV-VI.",
    "candidate_answer": "u_a green = green = 0.221u_0\nu_a red = red = 0.115u_0\nu_a IR = IR = 0.044u_0\n\n(0.221-0.044)/(0.115-0.044) = 2.5\n\n(green - IR)/(red - IR) = [(0.221-0.044)u_0]/[(0.115-0.044)u_0] = 2.5 = constant and does not depend on melanin concentration. This remains constant across all Fitzpatrick types because it is a constant number.\nBeer-Lambert applies differently to each wavelength. Since u_a terms are multiplied by C_m in Beer-Lambert, this implies that the system now depends on melanin concentration, meaning it's a melanin interference issue.",
    "kimi2_score": 30.0,
    "kimi2_feedback": "The candidate correctly identified that melanin absorption coefficients are wavelength-dependent and that Beer-Lambert law involves concentration, but made a critical mathematical error by claiming the ratio remains constant. The ratio (\u03bc_a_green - \u03bc_a_IR)/(\u03bc_a_red - \u03bc_a_IR) is indeed constant, but this is NOT the ratio used in glucose estimation algorithms. The actual glucose estimation uses ratios of optical density changes (\u0394OD), which depend on both melanin concentration and glucose-induced changes. The candidate failed to show how melanin concentration affects the differential path length factors and how this creates non-linear errors in glucose estimation. They also didn't demonstrate the sign flip mechanism for higher melanin concentrations. The response lacks the mathematical rigor needed to prove the systematic error."
  },
  {
    "answer_id": 435,
    "candidate_id": 13,
    "candidate_name": "Chloe Zhong",
    "category": "system_design",
    "question_text": "Argus needs to stream continuous PPG data while maintaining 3-day battery life. Design a power management strategy that dynamically scales LED intensity (Red/IR/Green) based on signal quality, implements adaptive sampling rates, and manages BLE transmission duty cycles. How would you handle edge cases like poor perfusion or high motion environments?",
    "expected_answer": "Expect discussion of: Dynamic LED PWM control based on perfusion index feedback, variable sampling rates (25-100Hz) tied to motion detection via accelerometer fusion, aggressive duty cycling with predictive buffering, and graceful degradation modes. Should include specific numbers for current draw targets (sub-mA average) and strategies for handling signal dropout scenarios.",
    "candidate_answer": "- to manage power with the LEDs, create a closed-loop system to use only as much power as needed (for example, if the SNR is high enough, lower the power, if it's too low, increase the power)\n- to implement the adaptive sampling rate, use adaptive logic that samples at a lower rate when the signal is stable but sample more when it isn't. we can determine signal stability not only based off of the raw signals, but also based off of the accelerometer and temperature sensor\n- for BLE transmission duty cycles, if the signal is stable, transmit less because it is less likely to change. If it is unstable, transmit more because it will likely not be the same between samples\n- if perfusion is poor and motion is also high, ",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate demonstrates a basic understanding of the core concepts but lacks the depth and specificity required for a senior-level system design question. The response is too high-level and doesn't provide concrete implementation details, specific power numbers, or comprehensive edge case handling. The answer cuts off mid-sentence when discussing poor perfusion/high motion scenarios, which is a critical omission. There's no mention of specific current draw targets, PWM control mechanisms, buffering strategies, or graceful degradation modes. The candidate needs to provide more technical depth including specific algorithms, thresholds, and quantitative power budgets."
  },
  {
    "answer_id": 244,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "brain_teaser",
    "question_text": "You have a PPG sensor collecting 6-channel data at 100Hz. Each channel uses 16-bit resolution. Due to motion artifacts, 15% of samples are corrupted in each channel, but the corruption pattern is different across channels. If you implement a voting system where a sample is considered valid only if at least 4 out of 6 channels agree within \u00b12 LSBs, what is the probability that a given timestamp will have a valid sample? Assume corruption causes random 16-bit values and agreement is defined as matching within the tolerance.",
    "expected_answer": "Calculate using combinatorics: probability = \u03a3 (from k=4 to 6) [C(6,k) * (0.85)^k * (0.15)^(6-k)] \u2248 0.953, or about 95.3% valid samples",
    "candidate_answer": "probability of corrupted= 0.15 \nprobability of non corrupted = 0.85 \n\nP(valid)= (6 4)(0.85)^4(0.15)^2 + (6 5)(0.85)^5(0.15) +(0.85)^6= 0.953\n probability that given timestamp. has valid samples = 95.3%",
    "kimi2_score": 85.0,
    "kimi2_feedback": "The candidate correctly identified the binomial probability model and computed the exact probability of having at least 4 good channels out of 6. They used the correct formula \u03a3 C(6,k) (0.85)^k (0.15)^(6-k) for k=4,5,6 and arrived at the expected 95.3%. However, they did not explicitly address the \u00b12 LSB tolerance requirement or the fact that the corruption pattern is different across channels, which implies that corrupted samples are uniformly random and thus extremely unlikely to accidentally agree within 2 LSBs. While the numerical answer is correct, the explanation lacks clarity on why the tolerance condition is effectively satisfied whenever 4+ channels are uncorrupted (since corrupted samples are random and won't match). The presentation is terse and would benefit from clearer notation and a brief justification of the assumptions."
  },
  {
    "answer_id": 245,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "brain_teaser",
    "question_text": "A glucose monitoring device must detect hypo/hyperglycemic events within 30 seconds of occurrence. The device processes PPG data through a pipeline: (1) Motion artifact removal (8ms), (2) Skin-tone compensation (5ms), (3) Feature extraction (12ms), (4) ML inference (25ms). You can parallelize steps 1&2, and steps 3&4, but step 4 depends on step 3's output. Steps 1&2 can run simultaneously. What's the minimum latency you can achieve, and how would you restructure the pipeline if the FDA requires <20ms detection?",
    "expected_answer": "Current minimum latency = max(8+5, 12) + 25 = 37ms. To meet <20ms: Run steps 1&2 on previous window while step 3 runs on current window, creating a sliding window approach with overlapping processing. This reduces effective latency to max(8+5, 12) = 13ms for features ready when needed.",
    "candidate_answer": " step 1 &2 parallel max(8 AND 5 ms)= 8ms\nStep 3 must complete before step 4 = 12ms + 25 ms =37 ms \ncritical path = 37ms + 8ms=45 ms \n45 ms meets the 20s requirement but fails <20ms FDA requirement \n- move motion + skin tone compensation into a single fused pass (6 to 7ms)\n- Replace full feature extraction with early / incremental features (<5ms)\n- use lightweight or exit ML model (decision tree/ linear model, 5 ms)\n- pipeline across windows so interference runs on partially accumulated features\nthis restructuring will brings the critical path to (15 to 18 ms)\n",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate correctly identified the parallelizable steps and understood the dependency constraints. However, they made a critical error in calculating the current minimum latency by incorrectly adding the parallel steps to the critical path (should be max(8,5) + max(12,25) = 37ms, not 45ms). The restructuring suggestions are creative and practical - fusing steps, using incremental features, and lightweight models are valid approaches. The final latency estimate of 15-18ms is reasonable for the proposed changes. The communication could be clearer with better formatting and more explicit explanation of the sliding window concept."
  },
  {
    "answer_id": 246,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "brain_teaser",
    "question_text": "Your BLE device transmits encrypted glucose readings every 5 minutes. Each packet is 20 bytes (16 bytes data + 4 bytes CRC). Due to RF interference, packets have 30% loss rate. If you implement a scheme where you send redundant copies after 1 minute delays, how many total bytes are transmitted per successful reading (including retransmissions), and what's the expected power consumption increase?",
    "expected_answer": "Expected transmissions = 1/(1-0.3) \u2248 1.43 per reading \u2192 28.6 bytes average. Power increase \u2248 43% for RF transmission, but sleep current dominates total budget. Optimize by bundling readings or adaptive intervals.",
    "candidate_answer": "Packet size= 20 bytes. \nPacket loss= 30%\np=1- 0.3=0.7 \nE[N]= 1/p= 1.43 transmission \n1.43 transmission X 20byte = 28.6 bytes per transmission. \n%increase in number of byte = 28.6/20= 143%\nexpected power increase = 143-100=43 %\nthe 1 minute delay doesn't increase the power consumed.  ",
    "kimi2_score": 85.0,
    "kimi2_feedback": "The candidate correctly identified the key calculation: expected transmissions = 1/(1-0.3) \u2248 1.43, leading to 28.6 bytes transmitted per successful reading. They also correctly calculated the 43% power increase for RF transmission. The insight that the 1-minute delay doesn't directly increase power consumption is accurate, as the device is in sleep mode during delays. However, the response lacks discussion of optimization strategies (bundling readings, adaptive intervals) and doesn't mention that sleep current typically dominates total power budget, which would provide important context for the power increase. The explanation is concise but could be more comprehensive."
  },
  {
    "answer_id": 247,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "brain_teaser",
    "question_text": "You have 3 temperature sensors (skin, ambient, device) with different thermal time constants: 2s, 10s, and 60s respectively. When the user removes the wearable, all sensors start cooling exponentially. After 30 seconds, the skin sensor reads 2\u00b0C above ambient. Given that glucose estimation requires skin temperature within \u00b10.5\u00b0C of steady-state, how long must the device wait before providing valid readings after re-application?",
    "expected_answer": "Skin sensor reaches ~13.5% of final value after 30s (2\u00b0C drop implies 14.8\u00b0C total possible drop). To reach \u00b10.5\u00b0C tolerance (3.4% of total change), need ~4.5 time constants = 9 seconds. However, since glucose algorithm uses temperature correction, wait until skin sensor stabilizes within 0.5\u00b0C of reading - approximately 15-20 seconds total.",
    "candidate_answer": "0.5 = e^(-t/2)  \nt= 2ln(4) = 2.77s \n\nyou need 3 seconds",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate correctly identified the exponential decay formula but made several critical errors. First, they used the wrong target temperature difference - the 0.5\u00b0C tolerance is relative to steady-state, not absolute temperature. Second, they didn't account for the 30-second cooling period that already occurred. Third, they ignored the glucose estimation context - the device needs to wait for thermal equilibrium after re-application, not just reach 0.5\u00b0C from some arbitrary point. The calculation assumes starting from perfect equilibrium, which isn't valid after 30 seconds of cooling. The actual wait time should account for the new warming phase after re-application, which would take approximately 9-15 seconds based on the skin sensor's 2s time constant."
  },
  {
    "answer_id": 241,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "code_review",
    "question_text": "An SpO2 routine uses both Red and IR PPG channels but yields saturation values outside the valid 0-100 % range on dark skin tones. Identify the bug and adapt the ratio-of-ratios calculation to account for Fitzpatrick-based attenuation correction.",
    "expected_answer": "Missing scaling for skin-tone-dependent DC offsets; dark skin reduces IR DC more than Red, inflating r. Add Fitzpatrick-corrected gain factors: red_dc *= skin_gain[FITZ], ir_dc *= skin_gain[FITZ]. Clamp output between 0-100.",
    "candidate_answer": "DC is normalized. red_dc and ir_dc are summed but never averaged, making the ration dependent on window length and LED power. \nmelanin bias not removed. Dark Skin increases DC absorption differently for RED vs IR, biasing \nno bounds checking the linear model can return invalid saturation values. ",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate correctly identified several issues (DC normalization, melanin bias, missing bounds checking) but failed to implement the core fix. They did not properly address the Fitzpatrick-based attenuation correction as requested. The code is incomplete - it introduces a variable 'fitz_corr' without showing how it's calculated or applied, and the final calculation is commented out. The DC normalization fix is good but doesn't solve the skin tone bias problem. The response lacks the critical insight that different wavelengths (Red vs IR) are attenuated differently by melanin, requiring separate correction factors for each channel."
  },
  {
    "answer_id": 242,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "code_review",
    "question_text": "The BLE notification handler below streams 6-channel PPG plus temperature every 20 ms, but packets are dropped under heavy motion. Find the root cause in the ring-buffer logic.",
    "expected_answer": "Buffer indices wrap-around not handled (uint8_t overflow), sizeof(Sample_t) > BLE_PAYLOAD_LEN causing MTU truncation, and no critical section around wr/rd access. Fix: mask indices with 0x1F, pack data to 20 bytes, and disable interrupts during update.",
    "candidate_answer": "1- broken ring buffer logic. under heavy motion , the producer advances wr faster, wrap occurs, and the buffer appears empty and the samples dropped.\n2- no modulo on read index.. rd is never wrapped so it eventually indexes outside the buffer. \n3- no overflow protection Writer can overwrite unread data when BLE stalls. \n4- ISR doing too much. Inside ISR increases latency and drop probability. ",
    "kimi2_score": 55.0,
    "kimi2_feedback": "The candidate correctly identified the primary issue (ring buffer wrap-around) and provided a partial fix by masking the read index, but failed to address the critical MTU size mismatch and race-condition problems. The solution is incomplete and introduces new issues."
  },
  {
    "answer_id": 243,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "code_review",
    "question_text": "This motion-artifact removal function uses accelerometer data to gate PPG windows, yet oscilloscope shows ~180 ms latency spikes. Pinpoint the timing flaw.",
    "expected_answer": "32-sample loop at 64 Hz \u2192 500 ms window, exceeding 50 ms budget. Reduce WIN to 4 samples (62.5 ms) and use integer approx sqrt or lookup table instead of 32-bit multiply in tight loop. Cache magnitude in ISR.",
    "candidate_answer": "1- full window accumulation is synchronous and blocking: win=32 samples, this loop runs every time the function is called. iff accel is sampled at 200hz this introduces window length latency. you are waiting 32 sample period before releasing ppg = 150ms. \n2- memset() un the real time Path. clear an entire PPG window synchronously. blocks the caller and flushes valid samples. Causes pipelines bubbles and cache stalls on cortex M. Amplifies jitters when motion toggles rapidly.\n3- Magnitude computation is unnecessary expensive. 3 multiplies x 32 samples= 96 multiplies per call. Should be incremental not recomputed per window.  ",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate correctly identified the primary timing flaw (32-sample window causing ~150-180 ms latency) and provided good insights about blocking behavior and computational cost. However, the proposed solution has significant issues: incomplete/incorrect code (syntax errors, undefined variables), the sliding-window approach still uses WIN=32 (not addressing the core latency issue), and the memset removal is incomplete (still zeros entire window elsewhere). The explanation about sampling rate assumptions was slightly off (200 Hz vs typical 64-128 Hz), but the core latency diagnosis was accurate. The incremental computation idea is good but poorly implemented."
  },
  {
    "answer_id": 249,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "coding",
    "question_text": "Write a Python function that implements a multi-wavelength PPG glucose estimation algorithm. Given synchronized Red/IR/Green PPG data, temperature, and accelerometer data, estimate non-invasive glucose levels using a trained ML model. The function must handle missing data gracefully and apply FDA-compliant data validation.",
    "expected_answer": "Extract HRV features (RMSSD, pNN50), PPG amplitude ratios, and temperature compensation. Apply FDA validation (glucose 50-400 mg/dL range). Use ensemble model combining optical and thermal features. Return None for >20% missing data or invalid physiological ranges.",
    "candidate_answer": "code is commeted ",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate demonstrates basic understanding of input validation and feature extraction, but the implementation has significant gaps. The code is incomplete (missing imports, syntax errors, incomplete FDA validation), lacks HRV feature extraction as requested, and doesn't properly handle the multi-wavelength PPG data or missing data scenarios. The perfusion index calculation is overly simplistic and doesn't leverage the multi-wavelength capabilities. The motion artifact detection is rudimentary and the temperature range check is too narrow for FDA compliance. The feature vector construction is minimal and doesn't include critical HRV features like RMSSD or pNN50. The response shows conceptual understanding but fails to deliver a complete, functional solution."
  },
  {
    "answer_id": 229,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "signal_processing",
    "question_text": "You're processing 6-channel PPG data from a MAXM86146 sensor hub where Red (660nm), IR (940nm), and Green (525nm) wavelengths are being used for glucose estimation. The signal contains motion artifacts from a person jogging (2-4 Hz motion) and heartbeat signals (1-2 Hz). Design a filter bank to separate these components while preserving the PPG morphology needed for glucose correlation. How would you handle the spectral overlap between motion artifacts and cardiac signals?",
    "expected_answer": "Implement a multi-stage filter bank: 1) High-pass (>0.5 Hz) to remove DC offset, 2) Band-pass (0.7-2.5 Hz) for cardiac PPG, 3) Adaptive notch filter for motion at 2-4 Hz using accelerometer reference, 4) Use wavelet denoising for morphology preservation. Address spectral overlap with adaptive filtering using accelerometer data as reference noise input.",
    "candidate_answer": "use a parallel filter bank rather than a single bandpass because jogging and heart beat frequencies are overlapping. First remove DC/baseline with HPF (0.3 to 0.5 Hz) then run three branches. Heart band (0.7 to 6 Hz) (Linear phase FIR). Cardiac dominant band (0.7 to 2.5Hz) to estimate artifact. Running band (2 to 5 Hz). Handle overlap by adaptive multichannel cancellation: estimate the motion component using IMU (LMS/RLS) or multi wavelength regression/PCA (motion is highly correlated across wavelengths), then subtract from each wavelength in the morphology band. Use Red/IR to lock beat timing and apply SNR Green drive detection. Finally do beat synchronous segmentation/ ensemble averaging to recover morphology for glucose features.   ",
    "kimi2_score": 85.0,
    "kimi2_feedback": "The candidate demonstrates strong understanding of the spectral overlap challenge and proposes a sophisticated multi-branch architecture. The use of parallel processing, adaptive cancellation with IMU data, and wavelength correlation techniques shows practical PPG signal processing knowledge. However, the explanation could be clearer about the specific filter design (e.g., FIR order/transition bands), and the distinction between 'heart band' and 'cardiac dominant band' is confusing. The approach to morphology preservation through beat-synchronous averaging is good, but lacks detail on how this specifically preserves features needed for glucose correlation. The mention of PCA/regression for motion estimation is valuable but needs more implementation detail."
  },
  {
    "answer_id": 230,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "signal_processing",
    "question_text": "Given 512-point FFTs of PPG signals sampled at 100 Hz across Red, IR, and Green wavelengths, you notice the Green channel shows 20 dB lower SNR compared to Red/IR due to skin tone (Fitzpatrick V-VI). How would you normalize the spectral magnitude across wavelengths to enable accurate SpO2 calculation while accounting for skin melanin absorption?",
    "expected_answer": "Apply skin-tone adaptive normalization: 1) Use DC component ratios to estimate melanin absorption coefficient per wavelength, 2) Apply wavelength-specific gain correction factors based on Fitzpatrick scale, 3) Implement dynamic range compression to equalize SNR across channels while preserving AC/DC ratios needed for SpO2 calculation.",
    "candidate_answer": "use AC/DC normalization per wavelength (so melanin geometry mostly divide out). Do ratio of ratio for ratio for SpO2. Use green only as melanin/quality proxy, not as direct SpO2 wavelength. \n-Compute R and apply Fitzpatrick conditioned correction  \n-extract pulsative AC from the FFT at the heart rate tone. \n- get DC per wavelength from the time signal.\n- Normalize each wavelength: perfusion index (cancel melanin \"static\" absorption)\n- Compute SpO2, ratio to ratio using Red +IR. \n- compute a melanin index from DC ratio (stable)\n- Correct for the ratio before converting to SpO2.\n- Stop green channel from injecting noise (use green only for M from DC and only if DC is not saturated)\n",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate correctly identifies the core principle of AC/DC normalization to compensate for melanin absorption and provides a practical approach using Red/IR for SpO2 while leveraging Green as a quality/melanin proxy. However, the response lacks the systematic spectral normalization approach requested, doesn't explicitly address the 20 dB SNR disparity in the FFT domain, and could be clearer in explaining the Fitzpatrick-conditioned correction mechanism."
  },
  {
    "answer_id": 231,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "signal_processing",
    "question_text": "For heart rate variability (HRV) analysis from PPG, you need to extract RR intervals with <10ms accuracy on an ARM Cortex-M4 with limited RAM. The PPG has baseline wander (0.05-0.5 Hz) and motion artifacts. Implement a memory-efficient peak detection algorithm that operates in real-time.",
    "expected_answer": "Use circular buffer with adaptive thresholding: track running mean/STD for adaptive threshold, implement derivative-based peak detection with refractory period, use linear interpolation for sub-sample accuracy. Include baseline drift removal via 0.5 Hz high-pass filter.",
    "candidate_answer": "// Memory-constrained HRV detection\n#define PPG_BUFFER_SIZE64 // Circular buffer\nInt16_t ppg_buffer[PPG_BUFFER_SIZE];\nuint8_t write_idx = 0;\nuint32_t last peak_time = 0;\nVoid process_ppg_sample(int16_t sample) {\nstatic int32_t dc= 0; \n// baseline tracker \nstatic int16_t prev=0;\nstatic int32_t noise = 0, signal = 0, Thresh= 0;\nstatic uint32_t sample_idx = 0;\n//store sample \nppg_buffer[write_idx] =. sample;\nwrite_idx = (write_idx +1) % PPG_BUFFER_SIZE; \nsample_idx++;\n//1sr order IIR high pass filter @0,5 Hz\n#define HPF_A_Q15 31769 //a = 0.9695 for Fs= 100Hz, fc 0.5Hz\nstatic int32_t yhp = 0;\nstatic int32_t x_prev = 0;\nint32 tmp= yhp + x -x_prev;\nyhp = (HPF_A_Q15 * tmp) >> 15 ;\nx_prev = x;\nint16_t xf = (int16_t)yhp; \n// store filters sample into its own circular buffer\nxf_buffer[write_idx] = xf; \n// slope energy (motion robust)\nint16_t dx = x - prev;\nprev = x;\nif (dx<0) dx= -dx; \n// adaptive threshold\nnoise+= (dx-noise) >> 5;\nThresh = noise +((signal - noise)>>1);\n// peak detection with refractory \nif (dx > thresh && (sample_idx - last_peak_time) > 30) {\nsignal  += (dx - signal) >> 4;\nlast_peak_tim + sample_idx;",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate demonstrates understanding of the core requirements (memory efficiency, baseline removal, peak detection) but the implementation has significant technical issues. The code has syntax errors (missing semicolons, incorrect variable names), incomplete logic (missing interpolation for <10ms accuracy), and questionable design choices. The high-pass filter implementation is on the right track but the slope-based peak detection is overly simplistic for motion artifact rejection. The adaptive threshold approach is conceptually correct but poorly executed. The refractory period check is present but the peak validation logic is incomplete. The code wouldn't compile or function correctly as written."
  },
  {
    "answer_id": 232,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "signal_processing",
    "question_text": "Your glucose estimation algorithm requires phase synchronization between Red/IR/Green PPG channels with <5ms group delay. However, each wavelength has different path delays due to tissue penetration depth. Design a delay compensation system using cross-correlation that works within the 50ms real-time constraint.",
    "expected_answer": "Implement efficient cross-correlation using FFT: 1) Pre-compute wavelength-specific delay constants based on optical properties, 2) Use 32-sample sliding window correlation at 100 Hz sampling, 3) Apply fractional delay filters (Farrow structure) for sub-sample precision, 4) Optimize with ARM CMSIS DSP library functions.",
    "candidate_answer": "Perform windowed band lifted cross correlation between channels using red as the timing reference. First bandpass all PPG channels in the cardiac band (0.7-4Hz) to suppress noise and motion. Then over a short sliding window (1-2s) compute normalized cross correlation between Red -Ir and RED-Green, restricting the lag search to a small range (10ms) based on physiological limits. The peak of the correlation gives the relative delay, which can be refined using parabolic interpolation for sub sample accuracy. Delays are applied via fractional delay interpolation or FIFO offsets and updated slowly (low pass filter) to ensure stability. This should be enough to save phase alignment limit of 5ms while executing the calculations within acceptable real time range. \n",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate demonstrates a solid understanding of the problem and proposes a reasonable approach. They correctly identify the need for band-pass filtering to isolate the cardiac signal, use normalized cross-correlation to find delays, and mention sub-sample precision. However, they miss several key technical details needed for a complete solution: they don't specify the FFT-based approach for efficiency, fail to mention pre-computed delay constants based on optical properties, and don't address the 50ms real-time constraint explicitly. The 1-2s sliding window is too large for the 50ms constraint, and they don't mention optimization with ARM CMSIS DSP library. While the core concept is sound, the implementation details are insufficient for a production system."
  },
  {
    "answer_id": 233,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "system_design",
    "question_text": "Design a real-time data pipeline for the Argus CGM device that must process 6-channel PPG data (Red/IR/Green LEDs) at 100Hz while maintaining <50ms latency for glucose trend updates. The system uses ARM Cortex-M4 with 256KB RAM and must simultaneously handle motion artifact removal, skin-tone adaptive algorithms, and BLE transmission. How would you architect the memory allocation and task prioritization scheme?",
    "expected_answer": "Describe a circular buffer scheme for PPG data with double-buffering for DMA, fixed-point arithmetic to reduce RAM usage, RTOS task priorities (PPG sampling > artifact removal > glucose estimation > BLE transmission), and memory pools for different data types. Include discussion of interrupt-driven sampling vs polled processing trade-offs.",
    "candidate_answer": "RTOS based pipeline with strict separation between real time sensing and non critical tasks. a high priority acquisition task (ISR- driven at 100Hz) handles LED timing, ADc capture and DMA transfer into small pre channel ring buffers sized to a few hundred milliseconds, ensuring constant memory usage. a second priority DSP task performs motion artifact removal, skin tone adaptive normalization, and feature extraction within fixed 10 rio 20 ms budget, using in place processing and preallocated buffers to avoid fragmentation.  glucose trend estimation as medium priority task on decimated features (1 to 5 hz), keeping compute low while meeting the <50ms end to end latency. BLE transmission, encryption and logging are in low priority tasks with queue based back pressure so they Never block sensing.  Memory is statically allocated in the startup (no heap at runtime ) with double buffering between stages and explicit CPU budgeting per task, ensuring predictable timing and safe operation within 256KB RAM. ",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate demonstrates a solid understanding of real-time embedded constraints and provides a coherent architecture. They correctly identify the need for ISR-driven sampling, static memory allocation, and task prioritization. However, the response lacks specific technical details that would demonstrate deep expertise - missing concrete buffer sizes, fixed-point discussion, exact task priorities, and quantitative memory budgeting. The answer is directionally correct but reads more like a high-level plan than a detailed design."
  },
  {
    "answer_id": 234,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "system_design",
    "question_text": "For HIPAA-compliant data transmission from Argus device to cloud, design a system that encrypts PPG and glucose data on-device before BLE transmission, handles connection loss gracefully, and ensures no PHI leakage during OTA firmware updates. How do you balance security overhead with the 50ms real-time constraint?",
    "expected_answer": "Outline AES-128 encryption on Cortex-M4 using hardware crypto accelerator, implement key rotation via ECDH handshake, use sequence numbers for replay protection, design store-and-forward buffer for offline mode, discuss firmware signing with ECDSA and encrypted firmware images. Address timing budgets for encryption operations.",
    "candidate_answer": "Encrypt PPG and glucose data on device using hardware accelerated AES-CCM-128 with hardware acceleration available on hardware M4 MCU. in practice AES-128 requires 150 to 250 CPU cycles per 16 bytes block in hardware. (vs>2000 cycles in SW), so encrypting typical 20 to 40 byte BLE payload completes in <5uS at 80 to 100 MHz, well below timing budget. CCM operates in 2 modes they are both executed by the crypto peripheral using DMA so the CPU remains free for signal processing. Data is accumulated in small frames (10 to 20ms worth of sample) before encryption to amortize the overhead, the =n transmitted with sequence counter to handle reconnection safely. OTA updates are verified using ECDSA signature checks only in bottloader, outside of the real time loop, with encrypted PHI regions locked during updates.  By batching emcryption, using HW  AES rotations, and decoupling  sensing from transport via buffering, cryptographic operations add <1%  CPU load and do not interfere with the 50ms limit of the sneer. ",
    "kimi2_score": 85.0,
    "kimi2_feedback": "The candidate demonstrates strong technical depth in embedded security and correctly identifies the key constraints. They provide specific performance metrics for AES-CCM-128 on Cortex-M4 hardware acceleration (150-250 cycles vs >2000 software), showing practical understanding of timing budgets. The approach of batching encryption (10-20ms frames) and using DMA to keep CPU free for signal processing is well-reasoned. However, the response lacks clarity on key rotation (only mentions 'HW AES rotations' without explaining ECDH handshake), doesn't explicitly address store-and-forward buffer design for offline mode, and the OTA update explanation is incomplete - mentions ECDSA signature checks but doesn't explain encrypted firmware images or PHI isolation during updates. The answer also contains typos ('bottloader', 'emcryption', 'sneer') that slightly detract from professionalism."
  },
  {
    "answer_id": 235,
    "candidate_id": 8,
    "candidate_name": "Khaled Ben Jamaa",
    "category": "system_design",
    "question_text": "Argus must maintain continuous glucose monitoring during charging via wireless power transfer (Qi standard). Design the system architecture to handle EMI from wireless charging coils that could corrupt PPG signals, while meeting FDA requirements for continuous monitoring during charging. Include fail-safe mechanisms for critical glucose alerts.",
    "expected_answer": "Design involves EMI shielding strategies (Faraday cage around PPG sensors), adaptive filtering to cancel charging coil harmonics (likely 110-205kHz), watchdog timers for sensor validation, backup battery for critical alerts, and graceful degradation modes. Include discussion of EMC testing per IEC 60601-1-2 and FCC Part 15.",
    "candidate_answer": "for this question there is different ways or approaches to solve:\ndesigning the matching frequency of the receiving coil to be out of the sensitive band of the sensor. transmitting coil and power electronics are isolated from the optical sensor using orthogonal coil orientation, ferrite shielding, and dedicated analogue ground island with common mode chokes on the ppg line to suppress conducted and radiated EMI. During  periods of degraded signal the system switch to a confidence aware estimation mode, holding the last validated value and widening alert threshold rather than suppressing alarms. If EMI exceeds safe limits, the device flags reduced accuracy while continuing monitoring satisfying FDA  requirement for continuous operation with explicit fail safe behavior rather than silent data dropout. For this part the device should be submitted to IEC 60601-1-2 radiated emission test radiated immunity. The test must be passed with a rational explaining fail safe mechanism and definition of normal functioning of the device.  ",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate correctly identifies key EMI mitigation techniques (orthogonal coils, ferrite shielding, common-mode chokes) and the need for regulatory compliance (IEC 60601-1-2). They also propose a graceful degradation strategy that keeps monitoring active while widening thresholds, which aligns with FDA continuous-monitoring requirements. However, the answer lacks depth on fail-safe mechanisms: no mention of watchdog timers, backup battery for critical alerts, or explicit fail-safe circuitry. The response is brief and omits the Faraday-cage concept, adaptive filtering, and detailed EMC test plan. While the orthogonal-coil idea is creative, it needs more rigor\u2014e.g., how orientation is maintained, quantitative shielding effectiveness, or how the 'confidence-aware estimation mode' is validated. Overall, the answer shows good direction but needs fuller system-level detail and explicit fail-safe architecture to be complete."
  },
  {
    "answer_id": 207,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "brain_teaser",
    "question_text": "You're designing Argus's motion artifact detection system. The PPG sensor samples at 100Hz across 6 channels (3 wavelengths \u00d7 2 LEDs). You notice that during vigorous exercise, motion artifacts appear as high-frequency spikes above 15Hz, but genuine cardiac signals rarely exceed 10Hz. However, you discover that some users experience arrhythmias that create harmonics up to 12Hz. How can you distinguish between motion-induced spikes and pathological cardiac rhythms using only the accelerometer data (50Hz sampling) and PPG data?",
    "expected_answer": "Use cross-correlation between accelerometer magnitude and PPG signal power in 10-15Hz band. True motion artifacts will show strong correlation with accelerometer peaks, while pathological rhythms won't. Implement a sliding window analysis with phase coherence checking between channels.",
    "candidate_answer": "First, given the thresholds, we can determine that a spike is motion-induced if it is above 15Hz (arrhythmias rarely goes above 12Hz).\nAssuming the accelerometer is a device that detects physical activity (vigorous exercise induces high acceleration). We can further distinguish between harmonics that are up to 12Hz by weighting the interaction between the acceleration with PPG data (high acceleration with high PPG at 12Hz would also mean motion spikes). Keep in mind that we do need to upsample the accelerometer so that they are in the same frequency domain.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate correctly identifies that motion artifacts appear above 15Hz while arrhythmias can reach 12Hz, but their approach has significant flaws. The suggestion to use acceleration magnitude as a weighting factor is vague and doesn't properly leverage the accelerometer data. The response lacks the key insight of using correlation analysis between accelerometer and PPG signals. The mention of upsampling is good but insufficient. Most importantly, the candidate fails to address the core challenge: distinguishing between 10-12Hz arrhythmia harmonics and motion artifacts, since both can appear in overlapping frequency ranges. The solution doesn't explain how to differentiate when motion artifacts might appear at 12Hz or when arrhythmia harmonics might be present during exercise."
  },
  {
    "answer_id": 208,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "brain_teaser",
    "question_text": "During testing, you observe that Argus's glucose readings become unreliable when users rapidly transition between different Fitzpatrick skin types (e.g., from pale Type I to tanned Type III). The device has stored calibration data for each skin type. Given that PPG amplitude ratios (Red/IR/Green) shift predictably with melanin changes, but glucose estimation relies on subtle optical scattering differences, how can you detect when recalibration is needed without ground truth glucose measurements?",
    "expected_answer": "Monitor the rate of change in PPG amplitude ratios between wavelengths. If ratios change faster than physiological glucose variations (>2%/min for ratios vs <0.5%/min for glucose), trigger recalibration. Use skin tone classification confidence scores as secondary validation.",
    "candidate_answer": "Since PPG amplitude is predictable with melanin changes, we can use PPG amplitude as a proxy for complexion (skin type). For example, a sufficient condition for needing glucose recalibration is if PPG amplitude shifts over a threshold, which means optical scattering is different. This is assuming that changes in PPG amplitude implies melanin change i.e. PPG is a function of only melanin. \nIn general, we can learn an encoder-decoder model incorporating the absolute values and the rates of changes of melanin and PPG so that we can predict the necessity of glucose recalibration with more confidence.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate identifies the core relationship between PPG amplitude and melanin changes, but misses several critical aspects of the problem. The approach is overly simplistic - it doesn't address how to distinguish between melanin-driven changes and actual glucose changes, which is the central challenge. The suggestion to use PPG amplitude as a proxy for complexion is problematic because it assumes PPG amplitude is solely a function of melanin, ignoring the glucose signal entirely. The encoder-decoder model suggestion is vague and doesn't explain how it would solve the fundamental problem of detecting when recalibration is needed without ground truth. The response lacks the specific threshold-based approach mentioned in the expected answer and doesn't leverage the predictable nature of melanin-driven ratio shifts. The candidate also fails to address how to handle the transition period when skin type is changing."
  },
  {
    "answer_id": 209,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "brain_teaser",
    "question_text": "Argus uses a 3-wavelength PPG system where each wavelength has different penetration depths. You discover that during dehydration, the Green signal (shallowest penetration) shows stronger correlation with glucose than the IR signal (deepest penetration). However, this breaks your multi-wavelength fusion model trained on hydrated subjects. How can you adapt the algorithm to maintain accuracy across hydration states?",
    "expected_answer": "Implement hydration state detection using the ratio of Green/IR signal strength as a proxy for blood volume changes. Dynamically weight wavelength contributions based on detected hydration state, giving more weight to deeper penetrating wavelengths during dehydration.",
    "candidate_answer": "I can train a single model with subjects in both dehydrated and hydrated states. To do that, we don't modify the model but only augment the dataset and the preprocessing pipeline. For example, we can consider warping the Green and IR signals in both dehydrated and hydrated states into the same domain and use normalization to make sure that the model interprets them as homogeneous data. We can also:\n1. train a model with dynamic weighting and unbalanced loss function (if the data availability is unbalanced)\n2. train separate models on the two different states and compile them into a single model with a pre-classifier that decides which sub-model to use.\n3. incorporate physical modelling of the different depth penetration and use contrastive learning ideas in the subject domain.",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate provided several valid approaches to address the hydration state issue, including data augmentation, domain adaptation, and ensemble methods. However, they missed the core insight about using the Green/IR ratio as a direct hydration indicator, which is the most elegant and physically-motivated solution. The approaches suggested are more complex and may not leverage the physiological understanding as effectively as the expected answer. The mention of physical modeling and contrastive learning shows good technical depth, but the response lacks the specific hydration detection mechanism that would be most practical for this problem."
  },
  {
    "answer_id": 210,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "brain_teaser",
    "question_text": "Your embedded system has 48KB RAM and must run continuous glucose estimation alongside heart rate detection. You find that HR detection requires 2KB per 10-second window while glucose estimation needs 8KB. Users report occasional dropouts during exercise when both algorithms compete for memory. If motion artifacts increase buffer requirements by 50%, how can you guarantee real-time performance (<50ms latency) without dropping samples?",
    "expected_answer": "Implement dynamic memory allocation based on motion state: during high motion, temporarily reduce glucose estimation buffer to 4KB and increase HR buffer to 3KB. Use accelerometer variance to predict motion state 100ms ahead and pre-allocate buffers.",
    "candidate_answer": "Assuming we don't modify the hardware, we can make process order modifications on both algorithms to improve data concurrency. For example, we can fuse the two algorithms so that data is only transferred onto chip only once by improving localization i.e. use the same batch of data on both algorithms simultaneously. We can also add dynamic memory allocation strategies and even pre-allocate the memories based on physiological heuristics. We can also include other parallel computing techniques/frameworks like multithreading to hide memory latency, utilizing parallel data algorithms with Thrust, or even translating the code to ISPC or even CUDA (if GPU is option)",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate's response misses the core constraint of the problem - the 48KB RAM limit and the specific memory requirements. While they mention dynamic memory allocation and pre-allocation, they don't provide a concrete solution for the 50% motion artifact buffer increase scenario. The suggestions about fusing algorithms, multithreading, and GPU acceleration are either too vague or inappropriate for an embedded system with severe memory constraints. The response lacks specific numbers or a clear strategy to guarantee <50ms latency when both algorithms compete for memory."
  },
  {
    "answer_id": 258,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "brain_teaser",
    "question_text": "You have 6 PPG channels from a MAXM86146 sensor (Red, IR, Green at 2 depths each). One channel suddenly starts showing inverted peaks. Your debugger shows the ADC is reading correctly, and no bits are flipped in memory. The device is strapped to a subject cycling on a stationary bike. After 30 seconds, the inversion spontaneously disappears. Explain the most likely physical phenomenon causing this inversion and why it self-corrected.",
    "expected_answer": "Motion artifact causing phase cancellation between direct and reflected light paths. As the cyclist's arm position changed, the optical path length varied, creating destructive interference that made the normally positive peaks appear inverted. The self-correction occurred when the arm returned to the original position where constructive interference resumed.",
    "candidate_answer": "the glucose level is changing when someone starts with physical work",
    "kimi2_score": 5.0,
    "kimi2_feedback": "The candidate's answer is completely incorrect. They suggested glucose level changes as the cause, which is physiologically impossible - glucose levels do not change rapidly enough (30 seconds) to cause PPG signal inversion, nor do they affect light phase relationships. The candidate failed to identify the actual physical phenomenon (optical interference/motion artifact) and provided no explanation for why the inversion would self-correct. This shows a fundamental misunderstanding of PPG signal physics and motion artifacts."
  },
  {
    "answer_id": 259,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "brain_teaser",
    "question_text": "A KOS AI engineer is testing glucose prediction accuracy. She notices that prediction error doubles every 20 minutes, but only during specific times: 2-4 PM and 8-10 PM. The error returns to baseline at other times. The device uses temperature-compensated algorithms. What's the most likely environmental factor causing this periodic accuracy degradation?",
    "expected_answer": "Ambient light interference during periods of changing natural light (sunset 2-4 PM, artificial light onset 8-10 PM) causing photodiode saturation or altered skin optical properties. The periodic nature suggests external lighting conditions affecting the PPG signal quality.",
    "candidate_answer": "Its after dinner. maybe the patient is eating to much sweets",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate's answer completely misses the technical nature of the question. Instead of identifying the environmental factor affecting the device's sensors, they provide a medical/behavioral explanation about patient diet. The question specifically asks about environmental factors causing periodic accuracy degradation in a glucose prediction device, not about patient behavior. The response shows no understanding of how glucose monitoring devices work or what environmental factors could affect their accuracy."
  },
  {
    "answer_id": 260,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "brain_teaser",
    "question_text": "Three Argus devices are worn simultaneously on the same patient. Device A shows glucose dropping 5 mg/dL/min, Device B shows stable glucose, and Device C shows glucose rising 3 mg/dL/min. All devices use identical algorithms and sensor hardware. The patient is motionless. After 10 minutes, all devices converge to the same reading. What could explain the initial divergence and subsequent convergence?",
    "expected_answer": "Different sensor-skin contact pressures creating varying perfusion levels. Light pressure (Device A) underestimates glucose due to reduced blood volume, firm pressure (Device C) overestimates due to venous pooling, optimal pressure (Device B) shows stable readings. As the patient adjusted position naturally, all devices reached optimal contact pressure.",
    "candidate_answer": "the devices were used from other patients before that had different glucose levels",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate's answer completely misses the core technical insight required by this brain teaser. The question is testing understanding of sensor physics and physiological factors affecting continuous glucose monitoring accuracy, particularly how contact pressure affects perfusion and readings. The response about 'devices used from other patients' is scientifically implausible - CGM sensors are single-use, disposable components that cannot retain previous patient glucose readings. This shows a fundamental misunderstanding of both medical device operation and the actual phenomenon being tested (pressure-induced perfusion changes). The answer fails to address any of the key elements: identical hardware/algorithms, motionless patient, initial divergence, and convergence over time."
  },
  {
    "answer_id": 261,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "brain_teaser",
    "question_text": "Your PPG signal shows perfect 60 BPM heart rate for 5 minutes, then suddenly jumps to 180 BPM for 30 seconds before returning to 60 BPM. The accelerometer shows no motion. Temperature sensor is stable. SpO2 drops from 98% to 85% during the spike. No actual cardiac event occurred. What physiological phenomenon could cause this reading, and how would you algorithmically detect it's artifactual?",
    "expected_answer": "Vasospasm causing temporary arterial constriction, making the pulse wave appear triple frequency (harmonic artifact). Algorithmic detection: Check for sudden frequency multiplication without corresponding SpO2/perfusion changes, verify absence of motion artifacts, use HRV analysis (real spikes show irregular intervals, harmonic artifacts show perfect integer multiples).",
    "candidate_answer": "fear could trigger heart rate, maybe the patient had also a stroke",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate fundamentally misunderstood the question. They suggested fear as a cause, but this would be a real physiological response, not an artifact. The stroke suggestion is medically implausible and doesn't explain the sudden return to 60 BPM. Most importantly, they completely missed the core concept of artifactual readings and provided no algorithmic detection method. The answer shows poor understanding of sensor artifacts and signal processing."
  },
  {
    "answer_id": 211,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "coding",
    "question_text": "Implement a real-time motion artifact detection algorithm for PPG signals. Given a 6-channel PPG stream (Red, IR, Green from 2 LEDs each) and accelerometer data, write a function that detects motion artifacts using the correlation between accelerometer magnitude and PPG signal variance. The function should return a boolean array indicating clean vs corrupted segments for 100ms windows.",
    "expected_answer": "Implement 0.5-8Hz band-pass filtering for accelerometer magnitude, calculate rolling variance/std for PPG channels, use cross-correlation or coherence analysis to detect motion artifacts. Threshold should be adaptive based on skin tone (darker skin = higher motion sensitivity). Return boolean mask for 100ms windows.",
    "candidate_answer": "We first quantified the accelerometer into a single motion signal by extracting the magnitude;\nband-pass filter the accelerometer to motion relevant frequencies;\nsegmented the motion signal into short windows;\ncomputed the motion energy per window;\nperformed adaptive thresholding using robust statistics;\nclassified windows as clean or motion-corrupted based on the thresholds",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate provided a basic implementation but missed several key requirements. While they correctly identified the need to use accelerometer magnitude and band-pass filtering, they failed to incorporate the PPG data at all - which was a central requirement of the problem. The solution only uses accelerometer data to detect motion, without correlating it with PPG signal quality as requested. Additionally, the implementation lacks the adaptive thresholding based on skin tone mentioned in the expected answer, uses a fixed threshold multiplier (4x median) without justification, and doesn't handle edge cases like when N isn't divisible by 10. The filtering range (0.5-5Hz vs expected 0.5-8Hz) is slightly off, and there's a typo in the variable name (threshold vs thresh). Most importantly, the core requirement of correlating accelerometer motion with PPG signal variance was completely omitted."
  },
  {
    "answer_id": 212,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "coding",
    "question_text": "Write a C++ function for an ARM Cortex-M4 processor that calculates SpO2 from dual-wavelength PPG (Red and IR) with these constraints: use only integer math, process 100 samples in <5ms, and handle integer overflow. The function should return SpO2 percentage (70-100 range) or -1 if signal quality is poor.",
    "expected_answer": "Calculate AC/DC ratios for both wavelengths, compute R = (ACred/DCred)/(ACir/DCir), use precomputed lookup table for SpO2 = 110 - 25*R mapping. Check signal quality via perfusion index (AC/DC ratio > 0.5%). All operations must be integer-based with saturation arithmetic.",
    "candidate_answer": "The code computes DC as the mean of Red and IR PPG over a window, then extracts AC by subtracting DC and using RMS amplitude for robustness, then applies signal-quality gates to reject windows, and finally forms ratio of ratios and clamps to a safe range to access the lookup table.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate demonstrates understanding of the core SpO2 calculation concept (ratio-of-ratios method) and attempts to implement signal quality checks. However, the solution fails on multiple critical requirements: it uses floating-point arithmetic throughout (violating the integer-only constraint), has syntax errors (missing semicolon after MIN_PI check), uses incorrect AC calculation (sum instead of RMS in ratio), and doesn't handle integer overflow. The perfusion index calculation is conceptually correct but implemented with floats. The lookup table indexing approach is reasonable but the implementation is incomplete. The code would not compile or meet the ARM Cortex-M4 integer-only requirement."
  },
  {
    "answer_id": 213,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "coding",
    "question_text": "Design a HIPAA-compliant data pipeline that processes 24-hour PPG data (86M samples at 1000Hz) into 1-minute glucose trend features. Write a Python function that compresses raw PPG into encrypted features for cloud transmission while maintaining differential privacy (\u03b5 < 1.0).",
    "expected_answer": "Downsample to 100Hz, extract 1-min HRV features (RMSSD, pNN50), add Laplacian noise scaled to feature sensitivity, compress with delta encoding, AES-256 encrypt with session key, include metadata for reconstruction. Total size should be <2KB per 24 hours.",
    "candidate_answer": "We consume the PPG stream sample-by-sample and aggregate into fixed 1-minute windows, avoiding storage of the full 86M sample signal--we clipped features (assuming we have a feature extraction algorithm) to bounded ranges and added Laplacian noise to enforce differential privacy while preserving long-term trends. \nWe then compressed and packed each minute into a compact binary record, achieving massive data reduction.\nFor security, we encrypted all records with AES-256-GCM, binding them to a non-reversible patient tag for secure, tamper-resistant cloud transmission.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate demonstrates understanding of the core requirements: downsampling, differential privacy via Laplacian noise, AES encryption, and data reduction. However, the implementation has significant correctness issues and lacks completeness. The code fails to properly handle the 1000Hz to 100Hz downsampling (it processes 6000 samples per minute instead of 600), uses undefined functions (_minute_features, AESGCM), has a critical bug in _laplace function (np.random_number_generator doesn't exist), and doesn't include glucose trend features as requested. The approach shows good architectural thinking with streaming processing and bounded ranges, but the technical execution is flawed."
  },
  {
    "answer_id": 222,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "signal_processing",
    "question_text": "KOS AI's MAXM86146 sensor outputs 6-channel PPG at 200Hz. You notice IR channel shows 60Hz electrical noise and Red channel has baseline drift from skin pressure changes. Implement a real-time DSP pipeline that removes both artifacts while maintaining <5ms processing latency per sample on ARM Cortex-M4.",
    "expected_answer": "Use IIR comb filter for 60Hz (zeros at 60Hz harmonics), cascaded with DC blocker (1-z^-1)/(1-0.99z^-1). Fixed-point implementation with Q15 format. Cycle count ~800 for 6 channels. Avoid FFT due to latency constraints.",
    "candidate_answer": "for (int c = 0; c< 3, c++) {\n    int32_t x = (int_32_t)channels[c];\n    st[c].baseline = ema_update(st[c]baseline, x, BASELINE_K);  // a constant like 13\n    inbt32_t x_hp = x-st[c].baseline;\n    int32_t x_notch = biquad_q30(&st[c].notch, x_hp);\n    // preserving low amplitude, hence no fast DC removal\n    if (y > 32767) {\n        y = 32767;}\n    if (y < -32767) {\n        y = -32768}\n    channels[c] = (int_16_t)y\n}",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate demonstrates understanding of the need for both DC removal and notch filtering, but the implementation has significant issues. The code uses exponential moving average for baseline drift removal (good choice for low computational cost) and mentions biquad filtering for 60Hz removal, but fails to provide the actual notch filter coefficients or implementation. The code has syntax errors (missing underscores in type names, incorrect loop syntax), missing variable declarations, and doesn't address the specific 60Hz requirement. The approach is conceptually correct but incomplete - there's no evidence this meets the <5ms latency requirement or ARM Cortex-M4 constraints. The answer lacks the precision needed for embedded DSP work."
  },
  {
    "answer_id": 262,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "signal_processing",
    "question_text": "You're processing 6-channel PPG data from the MAXM86146 sensor hub with Red/IR/Green LEDs at 100Hz. Motion artifacts are causing baseline drift and high-frequency noise. Design a filter cascade that removes motion artifacts while preserving the cardiac components needed for glucose estimation. Consider that your ARM Cortex-M4 has limited processing power and you need <50ms latency.",
    "expected_answer": "Design a 3-stage filter: (1) 0.5-8Hz bandpass Butterworth IIR (4th order) for cardiac components, (2) 0.2Hz high-pass for baseline drift removal, (3) Adaptive LMS filter with accelerometer reference for motion artifact cancellation. Use fixed-point arithmetic and implement as cascaded biquads. Memory footprint <2KB.",
    "candidate_answer": " intensive motions should be filtered out sensors only take data in phases of less intense motions",
    "kimi2_score": 5.0,
    "kimi2_feedback": "The candidate's response completely misses the technical requirements of the question. Instead of designing the requested filter cascade, they provide a vague statement about filtering during less intense motion phases. There's no mention of filter types, implementation details, memory constraints, or latency requirements. The answer lacks any technical depth regarding signal processing techniques, ARM Cortex-M4 optimization, or the specific PPG data characteristics mentioned."
  },
  {
    "answer_id": 263,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "signal_processing",
    "question_text": "Given multi-wavelength PPG signals (Red, IR, Green) from a dark-skinned individual (Fitzpatrick VI), explain how skin tone affects signal quality and propose an adaptive algorithm to normalize the signals for accurate SpO2 calculation. Your solution must work on edge devices.",
    "expected_answer": "Darker skin reduces AC/DC ratio due to melanin absorption. Implement adaptive LED current adjustment based on DC component levels, use wavelength-specific normalization factors derived from calibration curves, and apply skin-tone specific AC/DC scaling factors. Pre-compute lookup tables for Fitzpatrick scale values.",
    "candidate_answer": "skin tone of course can affect the quality of intensity of the measurement, and the melanin types influence the reflection of the PPGs signals. the adaptive algorithm should ensure more iterative measurements for dark skinned individuals",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate correctly identifies that skin tone affects signal quality through melanin absorption, but provides only a vague, high-level suggestion ('iterative measurements') without any specific technical implementation. There's no mention of the critical AC/DC ratio impact, no discussion of LED current adjustment, no normalization approach, and no consideration of edge device constraints. The response lacks the technical depth required for a mid-level signal processing role."
  },
  {
    "answer_id": 264,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "signal_processing",
    "question_text": "You have 60 seconds of PPG data sampled at 100Hz from a diabetic patient. Design an algorithm to extract heart rate variability (HRV) features that correlate with glucose levels. How would you handle missing peaks due to poor perfusion?",
    "expected_answer": "Apply peak detection with adaptive thresholding, use cubic spline interpolation for missing peaks <30% of signal, extract SDNN, RMSSD, pNN50 features. Implement quality scoring based on peak-peak intervals variance. Use 5-minute sliding windows with 50% overlap for trend analysis.",
    "candidate_answer": "the glucose level outside the area of normal glucose levels produce increase heart levels, \nif there is poor perfusion take data in a higher frequency",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate's response is extremely brief and lacks technical depth. They mention that glucose levels affect heart rate (which is true but not directly relevant to HRV feature extraction) and suggest taking data at higher frequency for poor perfusion (which is impractical as the sampling rate is already given as 100Hz). There is no mention of actual HRV features, peak detection algorithms, interpolation methods, or quality assessment - all critical components of the expected solution. The response shows minimal understanding of signal processing techniques needed for this biomedical application."
  },
  {
    "answer_id": 265,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "signal_processing",
    "question_text": "Implement a real-time FFT-based glucose trend prediction algorithm that processes 512-sample PPG windows every 5 seconds on ARM Cortex-M4. The algorithm must predict glucose trends 15 minutes ahead using spectral features from multiple wavelengths.",
    "expected_answer": "Replace averaging with wavelength-specific processing, use CMSIS DSP optimized FFT, implement feature extraction via magnitude calculation with arm_cmplx_mag_f32, quantize neural network to int8 weights, process in parallel using DMA. Target <10ms per window.",
    "candidate_answer": "i dont know",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided no substantive response to the technical question. They simply stated 'i dont know' without attempting any part of the implementation or discussing potential approaches. This shows a complete lack of engagement with the problem, no demonstration of signal processing knowledge, and no attempt to address the performance optimization requirements. Even a partial or incorrect attempt would have been more valuable than this response."
  },
  {
    "answer_id": 330,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "signal_processing",
    "question_text": "You're processing 6-channel PPG data from MAXM86146 sensor hub for glucose estimation. The Red (660nm) and IR (940nm) channels show strong motion artifacts from wrist movement during exercise. Design a filter chain that removes motion noise while preserving glucose-correlated components in the 0.5-3Hz range. Consider the device must maintain <50ms processing latency on ARM Cortex-M4.",
    "expected_answer": "Implement cascaded filters: 1) Adaptive LMS filter using accelerometer data as reference for motion artifact removal, 2) 4th-order Butterworth bandpass (0.5-3Hz) for glucose signal isolation, 3) Downsample to 25Hz to meet latency constraints. Use fixed-point arithmetic to optimize M4 performance. Filter coefficients should be pre-calculated and stored in lookup tables.",
    "candidate_answer": "eretwegtwrgbfbdbfhethehtetwhthh",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided completely nonsensical text that appears to be random keyboard mashing. There is no meaningful technical content, no attempt to address the filtering requirements, no mention of the specific components needed (LMS filter, Butterworth filter, downsampling), and no consideration of the ARM Cortex-M4 constraints. The response fails to demonstrate any understanding of signal processing concepts or the specific problem domain."
  },
  {
    "answer_id": 270,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "system_design",
    "question_text": "Design a HIPAA-compliant data pipeline for Argus CGM that collects 6-channel PPG data at 100Hz, accelerometer data at 50Hz, and skin temperature readings, then transmits processed glucose estimates via BLE to a smartphone app. How would you handle the 50ms real-time latency requirement while ensuring encryption at rest and in transit?",
    "expected_answer": "Design should include: 1) Edge preprocessing on ARM Cortex-M4 to reduce data volume (downsampling, feature extraction), 2) AES-128 encryption before BLE transmission, 3) Buffer management for real-time constraints (circular buffers with DMA), 4) Certificate-based authentication for BLE pairing, 5) Data segmentation for HIPAA compliance (PHI vs non-PHI data streams), 6) Local storage with encryption keys stored in secure element, 7) Batch transmission to cloud with TLS 1.3 + certificate pinning",
    "candidate_answer": "i would handle the 50ms by ensuring that the backend works as a base for the transmission of the glucose estimates via BLE",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate's response is extremely vague and lacks any technical depth. They mention 'backend works as a base' without explaining what this means or how it addresses the 50ms latency requirement. There's no mention of edge preprocessing, encryption methods, buffer management, HIPAA compliance specifics, or any of the key technical components needed for this medical device pipeline. The answer shows no understanding of embedded systems constraints, real-time processing, or security requirements for medical data."
  },
  {
    "answer_id": 271,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "system_design",
    "question_text": "You need to deploy a lightweight glucose prediction model (LSTM-based) on Argus wearable that runs every 5 seconds using the last 30 seconds of PPG data. The ARM Cortex-M4 has 256KB RAM, 1MB flash, and must maintain <5mA current draw. How would you optimize the model architecture and system design?",
    "expected_answer": "Approach: 1) Use 8-bit quantization for LSTM weights, 2) Prune model to <100KB (knowledge distillation from larger teacher model), 3) Implement sliding window inference with ring buffer, 4) Use CMSIS-NN for optimized kernels, 5) Dynamic voltage scaling during inference, 6) Cache frequently accessed weights in RAM, 7) Consider dual-core architecture with sensor hub (MAXM86146) handling signal preprocessing while M4 runs inference, 8) Implement early stopping based on signal quality metrics to skip unnecessary predictions",
    "candidate_answer": "i would optimize the model architecture by increasing the ARM Cortex to more than 5mA",
    "kimi2_score": 5.0,
    "kimi2_feedback": "The candidate completely misunderstood the core constraint of the problem. Instead of optimizing the model to work within the 5mA current draw limit, they suggested increasing the current draw beyond 5mA, which violates the fundamental requirement. This shows a lack of understanding of embedded system constraints and power optimization. The response is also extremely brief and lacks any technical depth or specific optimization strategies."
  },
  {
    "answer_id": 272,
    "candidate_id": 7,
    "candidate_name": "MASON HU",
    "category": "system_design",
    "question_text": "Argus needs to handle motion artifacts when users are running (high accelerometer variance) vs resting (low variance). Design a multi-tier system that: 1) Detects motion state in real-time, 2) Switches between different denoising algorithms based on motion, 3) Maintains glucose estimation accuracy, 4) Handles edge cases where motion coincides with actual glucose spikes. Include how this affects battery life and memory usage.",
    "expected_answer": "System design: 1) Motion classifier using accelerometer variance (running FFT on accelerometer data every 500ms), 2) Three-tier processing: a) Resting state - use Kalman filter with PPG template matching, b) Light motion - adaptive filtering + accelerometer artifact subtraction, c) High motion - trigger predictive model using last known good reading + HRV trends, 3) State machine with hysteresis to prevent rapid switching, 4) Memory: store only last 5 minutes of raw data, 5) Battery: switch to lower power mode during high motion (reduce LED current, lower sampling rate), 6) Use motion state as additional feature in glucose model, 7) Implement watchdog to detect when motion artifacts corrupt readings beyond recovery",
    "candidate_answer": "i think battery life is strongly affected by detection of real-time motion and therefore multi-tier system is impossible to design",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate's response is fundamentally flawed and demonstrates a lack of understanding of basic system design principles. The claim that 'multi-tier system is impossible to design' due to battery concerns is incorrect - many real-world systems successfully implement adaptive processing based on context. The response provides no actual design, no technical details, and no problem-solving approach. It dismisses the entire premise of the question rather than attempting to solve it."
  },
  {
    "answer_id": 107,
    "candidate_id": 4,
    "candidate_name": "Manal",
    "category": "brain_teaser",
    "question_text": "A KOS AI device is collecting 6-channel PPG data at 200Hz. Due to motion artifacts, exactly 3 consecutive samples are corrupted in one channel every 5 seconds. If we apply a median filter of window size 5, how many samples in a 10-second window will be affected by these artifacts after filtering?",
    "expected_answer": "Calculate that 6 corruption events occur in 10 seconds (2 per 5-second interval), each affecting 3 samples. The median filter spreads corruption to adjacent samples within its window, affecting 5 samples per corruption event (center + 2 on each side). Total affected samples = 6 events \u00d7 5 samples = 30 samples, but accounting for overlapping windows reduces this to approximately 24-26 unique samples.",
    "candidate_answer": "6 samples in a 10 second window",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate's answer of '6 samples' is fundamentally incorrect and shows a lack of understanding of both the corruption pattern and median filter behavior. They appear to have simply counted the number of corruption events (6 events in 10 seconds) without considering that each event affects multiple samples through the median filter window. The expected answer is 24-26 unique samples after accounting for overlap, but the candidate provided no calculation or explanation for how they arrived at 6 samples. This suggests they either misunderstood the question entirely or failed to grasp how median filters propagate corruption effects."
  },
  {
    "answer_id": 134,
    "candidate_id": 4,
    "candidate_name": "Manal",
    "category": "brain_teaser",
    "question_text": "You have a PPG sensor collecting 6-channel data at 100 Hz (Red, IR, Green \u00d7 2 LEDs each). Each sample is 16-bit. Your Cortex-M4 MCU has 192 kB RAM total, but 64 kB is reserved for BLE stack and OS. How many seconds of raw PPG data can you buffer before memory overflow? After processing, you compress each channel to 8-bit delta-encoded values with 50% compression ratio. Now how many seconds can you store?",
    "expected_answer": "Raw: 6 channels \u00d7 100 Hz \u00d7 2 bytes = 1200 bytes/sec. Available RAM = 128 kB = 131072 bytes. Raw buffer: 131072/1200 \u2248 109 seconds. Compressed: 6 \u00d7 100 \u00d7 1 \u00d7 0.5 = 300 bytes/sec. New buffer: 131072/300 \u2248 436 seconds.",
    "candidate_answer": "so if we have 6 channel at 100 hz, that means we 16*100 bits per second. Our RAM is 192-64 =128 kb. So 128/16*100 is the answer. What do you think?\n",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate made several critical errors in their calculation. First, they incorrectly calculated the data rate - they used 16 bits total for all channels instead of 16 bits per channel, completely missing the multiplication by 6 channels. Second, they mixed up units (bits vs bytes) throughout the calculation. Third, they didn't account for the 2 bytes per 16-bit sample. Fourth, they completely ignored the compression part of the question. The final calculation (128/16*100) is mathematically incorrect and doesn't represent any meaningful quantity. The answer shows poor understanding of data rates, unit conversions, and failed to address the full scope of the question."
  },
  {
    "answer_id": 135,
    "candidate_id": 4,
    "candidate_name": "Manal",
    "category": "brain_teaser",
    "question_text": "During a glucose estimation session, your device detects a sudden 40% drop in perfusion index while heart rate remains stable. The accelerometer shows no motion. What are the three most likely causes, and how would you validate each hypothesis using available sensors (PPG, temp, accelerometer)?",
    "expected_answer": "1) Vasoconstriction from cold - validate via temperature sensor drop 2) Pressure on sensor from tight clothing - validate via gradual PI recovery after adjustment 3) Skin-tone misclassification causing LED overdrive - validate via Fitzpatrick scale re-detection and LED current logs",
    "candidate_answer": "It could be I walked outside the lighting situation changed. It could also be vasoconstriction from cold exposure that the temperature  changed. Presure and compression could be a another reason such that people are pressing on the sensor",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate identified two relevant causes (cold-induced vasoconstriction and pressure/compression) but missed the critical skin-tone misclassification cause. The lighting change hypothesis is incorrect as PPG sensors use internal LEDs and aren't affected by ambient lighting. The response lacks validation methods for each hypothesis and doesn't address how to use available sensors to test the causes. The communication is vague and incomplete."
  },
  {
    "answer_id": 136,
    "candidate_id": 4,
    "candidate_name": "Manal",
    "category": "brain_teaser",
    "question_text": "Your FDA submission requires demonstrating that motion artifacts don't affect glucose readings during daily activities. Design a test protocol using accelerometer + PPG data that proves robustness for: walking (2-4 Hz motion), typing (8-12 Hz tremor), and car vibrations (20-50 Hz). Each test must run for 5 minutes. What's the minimum RAM needed to store synchronized 3-axis accelerometer + 6-channel PPG data at 100 Hz?",
    "expected_answer": "Data rate = (3 + 6) \u00d7 100 \u00d7 2 bytes \u00d7 300 sec = 540 kB. Must implement circular buffer with real-time processing or use external flash. For FDA proof, need 540 kB per test \u00d7 3 tests = 1.62 MB total storage.",
    "candidate_answer": "it would be 59*6*100. It would be 1.62 mb since there are three test 3*530kB =1,590 kB ",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate shows some understanding that multiple tests are needed and that storage accumulates, but the fundamental calculation is incorrect and lacks essential details. The data rate calculation (59\u00d76\u00d7100) appears to be a random guess rather than based on the actual sensor configuration described (3-axis accelerometer + 6-channel PPG = 9 channels total). The candidate failed to account for data type size (2 bytes per sample), test duration (300 seconds), or proper unit conversion. While they recognized the need to multiply by 3 tests, their base calculation is flawed. There's no mention of critical FDA requirements like synchronized data, real-time processing constraints, or storage strategy (circular buffer vs external flash). The answer lacks the precision required for FDA documentation."
  },
  {
    "answer_id": 137,
    "candidate_id": 4,
    "candidate_name": "Manal",
    "category": "brain_teaser",
    "question_text": "A patient's PPG signal shows perfect 60 BPM heart rate but glucose readings fluctuate wildly \u00b130 mg/dL every 12 seconds. Your ML model uses 30-second windows. Given this pattern, what's the root cause and how would you fix it without retraining? (Hint: The fluctuations align perfectly with window boundaries)",
    "expected_answer": "Window boundary alignment causing discontinuity artifacts. Fix by: 1) Overlapping windows (50% overlap minimum) 2) Using median filtering across windows 3) Implementing temporal smoothing with exponential decay. This prevents edge effects without model changes.",
    "candidate_answer": "there could be one miss reading that cause the windows to change. The last few readings affect the start of next reading windows. There might be systematic windows boundary effects. We use use overlapping windows or median filtering to fix this. ",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate correctly identified that window boundary effects are causing the issue and suggested overlapping windows and median filtering as solutions. However, the explanation lacks precision - they mention 'one miss reading' which doesn't align with the \u00b130 mg/dL fluctuation every 12 seconds pattern described. They failed to explicitly connect the 12-second fluctuation period to the 30-second window size (which suggests the fluctuation happens exactly at window transitions). The response is also incomplete as it doesn't mention temporal smoothing or exponential decay approaches. While the core direction is right, the explanation and solution coverage are incomplete."
  },
  {
    "answer_id": 115,
    "candidate_id": 4,
    "candidate_name": "Manal",
    "category": "system_design",
    "question_text": "Argus devices generate 6-channel PPG data at 100Hz + motion + temp sensors, creating ~2.5MB/hour per device. With 10,000 active users, design a cost-effective AWS architecture that: 1) Stores raw data for FDA audit trails, 2) Enables real-time glucose alerts, 3) Supports model retraining, 4) Provides dashboards for clinicians. Balance between DynamoDB for alerts, S3 for raw data, and Snowflake for analytics while keeping costs <$0.10/device/month.",
    "expected_answer": "Should propose tiered storage: hot (Redis/DynamoDB) for alerts, warm (S3) for recent data, cold (Snowflake) for analytics. Include data lifecycle policies, compression strategies, and multi-region replication for 99.9% uptime. Address how to handle the 2.4TB daily ingestion efficiently.",
    "candidate_answer": "Raw timeseries data can initially be stored in s3 or a rdbs. The models should be stored in S3. If we are retraining the models enough, then having a feature store would be effective. Snowflake can be leveraged for analytics and views that get updated daily. Dashboards can be implemented off of snowflake. ",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate provided a very high-level response that touches on some components (S3, Snowflake, feature store) but fails to address the core system design challenges. There's no mention of the real-time alerting requirement, no cost analysis against the $0.10/device/month constraint, no discussion of the 2.4TB daily ingestion architecture, and no tiered storage strategy. The answer lacks technical depth and doesn't demonstrate understanding of the scale or specific requirements. The suggestion to use RDS for raw timeseries data is questionable given the volume and nature of the data."
  },
  {
    "answer_id": 116,
    "candidate_id": 4,
    "candidate_name": "Manal",
    "category": "system_design",
    "question_text": "For KOS AI's CGM system, design a microservices architecture that handles: 1) Real-time signal quality assessment at edge, 2) Model deployment to ARM Cortex-M4 devices, 3) A/B testing different glucose estimation algorithms, 4) Canary deployments for edge ML models. How would you implement this using Kubernetes on AWS while ensuring zero-downtime updates to devices in the field?",
    "expected_answer": "Design should include: edge container orchestration, OTA updates via BLE, model registry integration, traffic splitting for A/B tests, rollback mechanisms. Address resource constraints (ARM Cortex-M4, 256KB RAM) and BLE bandwidth limitations.",
    "candidate_answer": "For edge signal quality assessment, timestamp alignment between the multiple sensors can be implemented in the firmware. Once the data is streamed to the mobile app, we can have additional signal filtering algorithms run on the data before it gets persisted to any db. Models would need to be quantized before getting deployed to the devices. A/B testing would not be required for an FDA-approved system. ",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate completely missed the core requirements of the question. They focused on firmware-level timestamp alignment and basic filtering, ignoring the microservices architecture, Kubernetes deployment, and zero-downtime update requirements. The dismissal of A/B testing for FDA systems shows a lack of understanding of how medical device companies actually iterate on algorithms post-approval. There's no mention of container orchestration, OTA mechanisms, model registry, or any of the requested technical components. The answer is more of a high-level firmware discussion than a system design."
  },
  {
    "answer_id": 92,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "brain_teaser",
    "question_text": "You have a PPG sensor collecting 6-channel data (Red, IR, Green LEDs) at 100Hz. Due to motion artifacts, you occasionally lose 2-3 seconds of clean signal every minute. Your glucose estimation algorithm requires 30 seconds of continuous clean data to make an accurate prediction. Given that motion artifacts follow a Markov process where clean segments have 80% probability of staying clean and 20% probability of becoming corrupted, while corrupted segments have 70% probability of staying corrupted and 30% probability of becoming clean, what is the probability that you can collect a full 30-second clean window within any 60-second interval? Also, if you implement a buffer that stores 40 seconds of data and uses a sliding window approach, what is the expected delay before you can make your first glucose prediction after device startup?",
    "expected_answer": "Calculate using Markov chains: P(clean 30s) \u2248 0.47 for any 60s window. Expected delay with 40s buffer: ~42 seconds considering transition probabilities and buffer filling time.",
    "candidate_answer": "every minute 2-3 seconds lose\naccurate prediction: 30 seconds of continuous data \n80% clean 20% corrupted (70% corrupted, 30% clean)\n\na full 30-second clean in 1 minutes. \nprobability of 2-3 corrupted in 30 \n\n20% of \np(clean/clean)=0.8   prop of a clean second\np(corrupted/clean) = 0.3\np(clean/corrupted)= 0.2\np(corrupted/corupted)=0.7\n\np_c= 0.8*p_c +0.3*p_k,  \np_k= 0.2*p_c+0.7*p_k,  so we have p_k= 0.2*p_c / (1-0.7) =2/3 * p_c\n p_total= p_k+p_c =1  \nfrom second line and third line we have: 2/3 *p_c + p_c= 1 ---> p_c=3/5 =0.6 \n\n\n\nP of collecting a full 30-second clean window: P=p_c^30 --> P=0.6^30 =  ---> The number of seconds that covers 30 seconds  run time= 30\n\nExpected delay per each second: 40 + 30/(0.6^30) \n",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate shows some understanding of the Markov process but makes critical errors in both the setup and the solution. They correctly identified the transition probabilities but reversed the clean/corrupted transition probabilities in their equations. The calculation of steady-state probability (0.6) is mathematically correct given their incorrect setup, but this steady-state probability is not what the question asks for - we need the probability of 30 consecutive clean seconds within a 60-second window, not the steady-state probability of being in a clean state. The final calculation 0.6^30 is completely wrong as it assumes independence between seconds, which violates the Markov assumption. The expected delay calculation is nonsensical - adding 40 seconds to 30/(0.6^30) has no theoretical basis. The candidate failed to recognize that this is a complex Markov chain problem requiring analysis of runs/sequences, not just single-state probabilities."
  },
  {
    "answer_id": 93,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "brain_teaser",
    "question_text": "Your Argus device uses multi-wavelength PPG where each wavelength responds differently to glucose levels and skin tone. You discover that for Fitzpatrick skin types I-II, glucose correlation is highest in green wavelength, while types V-VI show better correlation in IR. However, you can only process 3 wavelengths in real-time due to ARM Cortex-M4 memory constraints. Given that skin tone detection itself requires 5 seconds of data and has 85% accuracy, how do you optimize wavelength selection to minimize both glucose prediction error and processing latency under 50ms constraint? Consider that switching wavelengths requires 10ms reconfiguration time.",
    "expected_answer": "Implement adaptive wavelength selection: pre-compute optimal wavelengths per skin type, use majority voting over 5-second windows for skin tone detection, cache wavelength configurations, and prioritize IR+Red for darker skin, Green+Red for lighter skin with fallback to IR+Green for unknown types.",
    "candidate_answer": "Strategy: mix integer linear programing\n\n1)  Defining variables, symbols, parameters:\n\nW: The set of all available Wavelength \ns: The type of skins {L, D } , s in {L,D}.  let L be lighter skin and D to be darker\nc: The classifier notation for skin types, c in {L,D} \ne_wc: the error corresponding to Wavelength w and skin type s\nvariable x_wc: An integer binary variables for Wavelength w and classifier c, x_wc in {0,1} , sum x_wc=3\nparameter T_cw: time for switching wavelengths  T_cw<10ms\nparameter T_max: The processing latency time: T_max <50ms\nparameter T_base : skin tone detection needs 5s\nvariable t_w: processing time per each Wavelength\nP(c|s): The probability of classifier selects the right skin type P(c|s)= 0.85 if c=s and if c!=s is 15%\np_s: The prior probability of true skin tone\n\nE_sc=sum (x_wc * e_ws) : The model prediction error \nE_total= sum p_s * sum P(c|s)*E1 = sum p_s * ( 0.85* E_ss  + 0.15* E_ss^)  where s^ c!=s  , This is total expected error\n\n2) optimization model: \n\nobjective function: min (E_total  +  t_w*x_wc )\nconstraints: \nT_base+ sum(T_cw* x_wc) + sum (t_w * x_wc) < T_max ---->5+  10 sum(x_wc) + sum(t_w*x_wc) <50\nsum x_wc =3\n\nNote : since we know the error for glucose correction is low in green and for type L and low in IR for type D, we use 2 more parameter for their corresponding errors. \n\nNote: the question was not clear if processing latency is 50ms how come skin tone detection requires 5 seconds? So the time constrains is unclear \n",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate attempts to formalize the problem using integer linear programming, which shows mathematical thinking, but fundamentally misunderstands the problem constraints and practical implementation. The formulation is overly complex for the given constraints, and the candidate fails to address the core challenge of optimizing wavelength selection under real-time constraints. The confusion about the 5-second vs 50ms timing indicates poor problem comprehension - the 5 seconds is for initial skin tone detection (offline), while 50ms is for subsequent glucose measurements (real-time). The solution doesn't provide actionable wavelength selection strategy, doesn't address the ARM Cortex-M4 memory constraints, and the mathematical formulation appears incomplete with undefined variables (E_ss^). The approach lacks practical engineering considerations needed for embedded systems."
  },
  {
    "answer_id": 94,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "brain_teaser",
    "question_text": "You need to transmit PPG data via BLE in HIPAA-compliant manner. Each 6-channel sample is 24 bytes (4 bytes per channel), but BLE packets can only carry 20 bytes payload. You have 3 options: (1) transmit every sample in separate encrypted packets, (2) buffer and transmit in batches every second, or (3) compress using differential encoding. However, compression adds 5ms latency and motion artifacts can cause compression errors. Given that glucose estimation requires 30 seconds of data and you need to maintain <50ms end-to-end latency, which transmission strategy minimizes battery usage while ensuring data integrity for FDA compliance? Assume packet loss rate is 2%.",
    "expected_answer": "Use adaptive batching: transmit 1-second batches (600 samples) with forward error correction, implement differential compression only for clean signal segments, maintain separate encrypted metadata stream for packet reconstruction. Expected battery savings: 75% vs individual packets.",
    "candidate_answer": "The first option is not the best choice since we want to minimize battery usage. The second option since we add 1s on top of 30s window (and given the fact that the end to end latency is 50ms) is not acceptable. The third choice encoding is the best strategy since it only adds 5ms and doesn't violate the end to end latency and minimize the battery.  ",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate correctly identified that option 1 is inefficient for battery usage, but failed to properly analyze the latency implications of option 2 and made an incomplete assessment of option 3. The response lacks critical analysis of the 30-second data requirement, doesn't address packet loss handling, and provides no concrete solution for ensuring data integrity. The conclusion that compression is 'best' is not well-supported given the motion artifact risks and lack of error handling strategy."
  },
  {
    "answer_id": 95,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "brain_teaser",
    "question_text": "Your PPG sensor occasionally detects arrhythmias that create false glucose spikes. These arrhythmias occur in 0.1% of samples but when they occur, they persist for exactly 1 second. Your ML model has 95% accuracy on clean data but drops to 60% during arrhythmias. You can either: (a) flag all arrhythmia periods as invalid, losing 1% of monitoring time, or (b) implement a 2-stage classifier that first detects arrhythmias (99.5% accuracy, 5ms latency) then applies different models. Given that glucose alerts must trigger within 60 seconds of actual hyper/hypoglycemia, and arrhythmias cause false alerts 30% of the time, how do you optimize the system to minimize both false alerts and missed critical events?",
    "expected_answer": "Implement 2-stage classification with temporal smoothing: use arrhythmia detector with 3-second majority voting, apply arrhythmia-specific model when detected, maintain dual-threshold system where flagged periods require confirmation from adjacent clean windows. Expected false alert reduction: 85% with <0.5% missed critical events.",
    "candidate_answer": "if we flag arrhythmia as invalid it might be an expensive choice, since we lose 1% of monitoring time, and we might lose actual hyper events that happen coincide. \nA better strategy is to make a two stage classifier, with the advance AI models data cleaning can be done in a very advanced ways. We can first build a classifier to detect the arrhythmia. If the arrhythmia was detected the model can switch to a different mode (we can built a multi model). Since arrhythmia happens only in 1% of samples and FP of detection is 0.5% and cause false alert 30% of time --->0.1*0.5*0.30 is a small number and more optimized than the first choice. ",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate correctly identifies that the 2-stage classifier is the better approach and provides some basic reasoning about why flagging all arrhythmias as invalid is suboptimal. However, the response lacks depth in technical analysis, contains significant calculation errors, and fails to address critical system requirements like the 60-second alert latency constraint. The mathematical calculation (0.1*0.5*0.30) is incorrect - it should be 0.001 * 0.005 * 0.30 = 0.0000015, which demonstrates poor attention to the actual problem parameters. There's no discussion of how to handle the latency requirements, temporal smoothing, or dual-threshold systems mentioned in the expected solution. The answer is vague about implementation details and doesn't provide concrete optimization metrics."
  },
  {
    "answer_id": 88,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "signal_processing",
    "question_text": "You're designing a multi-wavelength PPG system for continuous glucose monitoring using Red (660nm), IR (940nm), and Green (525nm) LEDs. Given that motion artifacts create frequency-domain interference patterns that differ by wavelength due to optical path length variations, design a cascaded filter architecture that: (1) preserves glucose-related signal components below 3Hz, (2) removes motion artifacts above 5Hz that exhibit wavelength-dependent phase shifts, and (3) maintains real-time processing with <10ms latency on ARM Cortex-M4. How would you optimize the filter coefficients across wavelengths while accounting for skin tone variations (Fitzpatrick I-VI)?",
    "expected_answer": "Propose a wavelength-adaptive cascaded IIR filter with: 1) Pre-processing stage using wavelength-specific motion artifact detection via accelerometer correlation, 2) Adaptive bandpass filters (0.5-3Hz) with dynamically adjusted Q-factors based on skin tone melanin absorption coefficients, 3) Phase-compensated delay equalization across wavelengths, 4) Fixed-point optimization using ARM CMSIS-DSP library with 16-bit coefficients. Include derivation of melanin-corrected attenuation factors for each Fitzpatrick scale level and prove stability bounds for the fixed-point implementation.",
    "candidate_answer": "An optimization model can be done, since. required tasks  can be modeled as constraints.  ",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate's response is extremely vague and lacks any technical depth. They mention 'optimization model' and 'constraints' without providing any specific implementation details, mathematical framework, or addressing any of the core technical challenges. There's no discussion of filter design, wavelength-specific processing, motion artifact removal, skin tone compensation, or ARM Cortex-M4 optimization. The response fails to demonstrate understanding of PPG signal processing, filter design principles, or embedded systems constraints."
  },
  {
    "answer_id": 89,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "signal_processing",
    "question_text": "For the Argus CGM device, you have 6-channel PPG data sampled at 200Hz. Motion artifacts create harmonics that alias into the 0.5-3Hz glucose-relevant band. Design an anti-aliasing strategy that: (1) detects motion artifact frequency signatures using the 3-axis accelerometer, (2) applies frequency-domain adaptive filtering, and (3) preserves HRV information needed for glucose prediction. How do you handle the tradeoff between aggressive motion artifact removal and preserving cardiac signal components?",
    "expected_answer": "Implement a two-stage approach: 1) Accelerometer-based motion frequency prediction (5-15Hz) with adaptive notch filtering, 2) Residual artifact removal using wavelet packet decomposition with entropy-based thresholding. Use Lomb-Scargle periodogram for irregular HRV extraction from corrupted segments. Demonstrate mathematically how to maintain >95% HRV accuracy while removing >90% motion artifacts.",
    "candidate_answer": "On the PPG data we can apply different metric then for detect motion artifact a CNN algorithm can be built. Then the second NN can be built takes the output of first+PPG for preserving HRV info the network can be trained on multi-objective loss. In NN tasks for handling different parameters we play with multi-objective loss and do optimization. This task can also be run with transfer models and multi-agent strategy.  ",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate completely missed the technical depth and specificity required for this senior-level signal processing problem. They proposed a generic neural network approach without addressing the core signal processing challenges: frequency-domain analysis, aliasing prevention, adaptive filtering, or mathematical validation of the 95% HRV accuracy requirement. The response lacks any discussion of sampling theory, frequency domain operations, or the specific 0.5-3Hz glucose band. The mention of CNN/NN without explaining how they would handle frequency aliasing, motion harmonics, or the mathematical relationship between artifact removal and HRV preservation shows poor understanding of the underlying signal processing principles. The answer reads like a generic ML solution rather than a domain-specific engineering approach."
  },
  {
    "answer_id": 90,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "signal_processing",
    "question_text": "Given temperature-induced baseline drift in PPG signals affects glucose estimation accuracy by up to 15mg/dL, design a drift compensation algorithm that: (1) estimates temperature from PPG waveform morphology changes, (2) corrects baseline shifts without distorting AC components, and (3) operates within 2KB RAM constraint on MAXM86146 sensor hub. Provide the mathematical derivation for temperature coefficient extraction from PPG pulse width variations.",
    "expected_answer": "Develop temperature compensation using: 1) PPG pulse width (PW50) temperature coefficient extraction via least-squares fitting of \u0394PW50/\u0394T = k\u00b7(dP/dt)_max, 2) Baseline drift correction using exponential moving average with adaptive forgetting factor based on temperature gradient, 3) Memory-efficient implementation using circular buffer (256 samples) with 8-bit fixed-point arithmetic. Show derivation of k from Beer-Lambert law temperature dependence.",
    "candidate_answer": "A regression model can be built on PPG to estimate the temp.  We can extract some features and then build the model. Small regression model can be run on 2KB ram. For correcting baseline, we can use a linear model. B(T) = B_0 +B(T-T_0) then we can estimate temp on device as x_corr= x_raw -B(T-T_0). For mathematical derivation for temp I need to see and analysis PPG data. ",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate provided a superficial response that lacks the technical depth required for this senior-level signal processing problem. While they correctly identified the need for regression and memory constraints, they failed to provide any specific mathematical derivation, algorithmic details, or implementation considerations. The response is vague about feature extraction, provides no derivation for temperature coefficient extraction from PPG morphology, and offers an oversimplified baseline correction approach that would likely distort the AC components. The candidate's admission that they 'need to see and analysis PPG data' for the mathematical derivation shows they haven't prepared the required theoretical framework."
  },
  {
    "answer_id": 91,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "signal_processing",
    "question_text": "Design a signal quality assessment algorithm for the 6-channel PPG system that must classify data quality in real-time (<5ms) to trigger LED power adjustments. Given that poor perfusion (perfusion index <1%) creates non-stationary noise, while motion artifacts create periodic interference, how would you implement a lightweight classifier that: (1) fuses multi-channel PPG signals, (2) uses the accelerometer to validate motion artifacts, and (3) provides confidence scores for glucose estimation? Optimize for edge deployment.",
    "expected_answer": "Create a hierarchical classifier: 1) Channel-wise quality via adaptive thresholding on signal-to-noise ratio (AC/DC ratio >0.5%), 2) Motion validation using accelerometer-PPG coherence analysis (cross-correlation lag <20ms), 3) Ensemble voting across channels with weighted confidence based on perfusion index. Use integer-only features: zero-crossing rate, skewness, kurtosis. Implement decision tree with 8-bit quantized thresholds for Cortex-M4 deployment.",
    "candidate_answer": "This problem is also requires a multi objective loss function.  The task can be done by different approaches. an Optimization model can be designed for the problem or an ML model.  ",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate's response is extremely vague and fails to address any specific technical requirements of the problem. They mention 'multi-objective loss function' and 'optimization model' without any concrete implementation details. The response lacks any discussion of the actual signal processing techniques needed (PPG signal analysis, motion artifact detection, real-time constraints), doesn't mention the accelerometer integration, provides no algorithmic approach for quality classification, and completely ignores the edge deployment constraints. This is essentially a non-answer that restates the problem as needing 'different approaches' without providing any actual solution."
  },
  {
    "answer_id": 96,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "system_design",
    "question_text": "Design a HIPAA-compliant data pipeline for KOS AI's Argus CGM wearable that processes 6-channel PPG data (Red/IR/Green LEDs) from 1000+ concurrent devices, each generating 256Hz sampled data. The system must provide real-time glucose predictions within 50ms latency while maintaining offline capability during BLE disconnections. How would you architect the data flow from edge device to cloud, considering encryption at rest/transit, audit logging, and FDA validation requirements?",
    "expected_answer": "Expected solution should describe: 1) Edge preprocessing on ARM Cortex-M4 with local encrypted storage during disconnections, 2) Differential privacy techniques for PII removal in PPG waveforms, 3) Azure IoT Hub with device provisioning service for secure BLE connections, 4) Delta Lake architecture on Databricks with time-series partitioning, 5) End-to-end encryption using AES-256 with key rotation, 6) Immutable audit trails using append-only Parquet files, 7) Canary deployment strategy for FDA 510(k) validation",
    "candidate_answer": "We can built an multi-agent system using transfer models. \n1- classifier agent:  We select a model that that has a high rank in classification tasks. Llama can be a good choice since its tokenization is one of the best choice when working with numbers. We have to make sure the device can handle the model, so we need to fine-tune parameters very well and keep it small as much as we can (quantization with 4 bit) and target important layers. \n2- security agent: check data with regulation \n3- monitoring agent to check every thing is ok in different part. \n",
    "kimi2_score": 15.0,
    "kimi2_feedback": "This response completely misses the mark on the system design requirements. The candidate proposes a multi-agent system with LLMs for a real-time medical device data pipeline, which is fundamentally inappropriate for this use case. The answer shows no understanding of HIPAA compliance, FDA validation, or the technical constraints of processing high-frequency PPG data from ARM Cortex-M4 devices. There's no mention of encryption, BLE protocols, Azure IoT Hub, Delta Lake, or any of the expected architectural components. The focus on LLMs and quantization demonstrates a misunderstanding of the problem domain - this is a real-time signal processing pipeline requiring deterministic latency guarantees, not a generative AI system."
  },
  {
    "answer_id": 97,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "system_design",
    "question_text": "Argus devices experience severe motion artifacts during exercise, causing 40% data loss. Design a system that maintains glucose prediction accuracy >85% while power consumption stays under 15mW. How would you architect a multi-modal fusion system combining PPG, accelerometer, and temperature sensors with adaptive sampling rates based on activity classification?",
    "expected_answer": "Propose hierarchical architecture: 1) Real-time motion classifier using RNN on accelerometer data to trigger adaptive PPG sampling (256Hz \u2192 64Hz during motion), 2) Edge-based signal quality scoring using SVM to discard corrupted PPG segments, 3) Kalman filter fusion with uncertainty quantification, 4) Dynamic model selection (CNN vs. LSTM) based on signal confidence, 5) Power-aware scheduling using integer linear programming to optimize sensor duty cycles, 6) Cloud-edge hybrid approach for model updates via federated learning preserving HIPAA privacy",
    "candidate_answer": "A multi agent sytem :\n\nFor each data set we design an agent using LLM models to classify different states in each data set such as temp, activity and ... we can test both transfer classifiers and traditional ML algorithm classifier such as NN, SVM, random forest and select the best model. \nThen we design an agent that connects all agents for final glucose prediction. ",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate's response is fundamentally flawed and demonstrates a lack of understanding of both the technical constraints and the specific requirements of the problem. The proposal to use LLMs for real-time glucose prediction on edge devices is completely unrealistic given the 15mW power budget - LLMs require orders of magnitude more power. The 'multi-agent system' approach is vague and doesn't address the core challenges: motion artifact handling, adaptive sampling, or power optimization. There's no mention of how to achieve >85% accuracy with 40% data loss, no discussion of Kalman filters or uncertainty quantification, and no concrete technical details about sensor fusion. The response reads like a generic AI buzzword dump rather than a serious engineering solution."
  },
  {
    "answer_id": 98,
    "candidate_id": 3,
    "candidate_name": "NAGHMEH ",
    "category": "system_design",
    "question_text": "For FDA submission, KOS AI needs to validate glucose prediction algorithms across diverse skin tones (Fitzpatrick I-VI). Design a test framework that simulates 1M+ patient scenarios with varying melanin levels, perfusion indices, and motion conditions. How would you generate synthetic PPG datasets while ensuring clinical validity and address potential demographic bias in training data?",
    "expected_answer": "Architecture should include: 1) Physics-based PPG simulator using Monte Carlo light transport for skin tone modeling, 2) Generative adversarial networks (GANs) constrained by ILP to maintain physiological validity, 3) Stratified sampling ensuring equal representation across demographics, 4) Differential testing pipeline comparing predictions against clinical reference (YSI 2300), 5) Bias detection using UMAP clustering on learned embeddings, 6) Automated retraining triggers when performance drops >5% on any demographic, 7) Regulatory documentation framework with traceable model lineage",
    "candidate_answer": "1- we build an agent to generate PPG data based on given condition.\n2- Build an agent to make sure each sample matches real human condition. \n3- make an agent that monitor the processes. For example if one skin tone has higher error it should reweigh the data set and try to improve the accuracy of that skin tone. ",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate's response is extremely vague and lacks the technical depth required for a senior-level system design question. While they correctly identified the need for synthetic data generation and bias monitoring, they failed to provide any concrete implementation details, architectural components, or validation mechanisms. The use of generic terms like 'agent' without specifying what type of agent (GAN, physics-based simulator, etc.) or how it would work shows a fundamental lack of understanding of the complex requirements. There's no mention of Monte Carlo methods for light transport modeling, no discussion of physiological constraints, no validation against clinical reference standards, and no regulatory compliance considerations. The answer reads like high-level brainstorming rather than a serious technical design."
  },
  {
    "answer_id": 444,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "brain_teaser",
    "question_text": "You have 6 PPG channels (Red, IR, Green each at 2 depths) from a MAXM86146 sensor monitoring glucose levels. Each LED has 3 intensity levels (low/medium/high). Due to motion artifacts, the sensor detects that for every 10 readings, exactly one channel becomes completely unreliable (noise >95%), but you don't know which one. You must maintain <50ms latency and can't retransmit. Design a strategy using the remaining 5 channels to reconstruct the lost signal without increasing computational complexity beyond O(n). The patient has Fitzpatrick skin type IV (medium-dark), which affects IR reflectance differently than Red/Green. How do you ensure glucose estimation accuracy remains within 15% error bounds?",
    "expected_answer": "Use weighted channel redundancy based on skin-tone coefficients. For Fitzpatrick IV, IR has 0.7 weight, Red 0.85, Green 0.9. Implement real-time adaptive weighting: when detecting channel failure via sudden SNR drop >95%, redistribute its weight across remaining channels proportionally to their skin-tone coefficients. Use precomputed lookup tables for coefficients to maintain O(n) complexity. Validate by checking if reconstructed signal maintains HRV patterns consistent with previous 3 beats.",
    "candidate_answer": "1. signal reconstruction: to keep the strategy within O(n), we can implement a weighted spatial median estimation. We know that for PPG signals, all channels share the heart rate frequency, and if 1 channel spikes from motion then we know this channel is unreliable. \n\nTo solve this, we can keep a running mean and standard deviation to z-score the incoming raw counts, scaling red, IR, and green to the same scale. For every 10-reading window, we calculate the median value across 6 channels.\n\nIf a channel's z-score deviates from the median of the group by a set threshold, we can replace the value with the weighted average of its neighbors (such as green replacing green at a different depth, or red replacing IR). Finding a median for 6 values will be in O(1) runtime.\n\n2. Fitzpatrick type and glucose accuracy\nTo account for skin with higher melanin and keep glucose accuracy within 15% error, we can scale the IR DC (as IR is reflected differently in darker skin). We can apply a skin tone scaling factor to the IR channel, where the factor is derived from initial baseline reflectance of the Green channel. \n\nBecause each LED has 3 intensity levels, we can use the Green channel at the highest intensity and IR at the lowest intensity, which will help equalize AC across different wavelengths. This comes from the logic that skin with more melanin will absorb more light in green and red spectrums, and will relatively not absorb as much light in the IR channel.\n\nWhen a channel drops out, we can pre-define channel pairs beforehand to shift the channel's weight to its most highly correlated available channel. A possible set of channel pairs can be amongst the dimensions of either same wavelength or same depth, such as shallow green and deep green; red and infrared; or a shallow vs deep version of a channel. \n\n",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate demonstrates understanding of PPG signal processing and attempts to address both motion artifacts and skin tone effects. However, the solution lacks the precision needed for glucose estimation accuracy within 15% error bounds. The weighted spatial median approach is computationally efficient (O(1) for 6 channels) but doesn't adequately account for the physiological differences between wavelengths or the specific skin tone requirements. The skin tone handling is too generic - using Green channel baseline for IR scaling is insufficient for Fitzpatrick IV skin type. The channel pairing strategy is vague and doesn't leverage the known skin-tone coefficients (IR:0.7, Red:0.85, Green:0.9) that are critical for accurate reconstruction. The solution also doesn't explicitly validate the reconstructed signal maintains glucose estimation accuracy within bounds."
  },
  {
    "answer_id": 445,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "brain_teaser",
    "question_text": "Your Argus device has detected anomalous glucose readings that correlate with accelerometer spikes during high-impact exercise. The system shows glucose dropping 30% then rebounding within 2 minutes, but patient reports no hypoglycemic symptoms. You have 3 hypotheses: (1) motion artifact corrupting PPG amplitude, (2) blood flow changes affecting optical path length, (3) actual glucose fluctuation from muscle glucose uptake. Given you can only use existing sensors (PPG 6-channels, accelerometer, temperature), design an experiment to distinguish between these cases while maintaining HIPAA compliance and not storing raw data.",
    "expected_answer": "Use accelerometer phase correlation: compute cross-correlation between accelerometer magnitude and PPG signal quality index (SQI) over 30-second windows. If motion artifact (case 1), SQI drops precede glucose changes. If blood flow (case 2), temperature sensor shows localized skin cooling during glucose drops. If real glucose (case 3), temperature remains stable and SQI shows no correlation with accelerometer. Implement edge detection on-device to avoid storing raw data, only store correlation coefficients and summary statistics.",
    "candidate_answer": "We want to find out which sensor \"caused\" the signal change. We can start with splitting the data into 30-second windows, and analyze the information from each sensor.\n\nFor hypothesis 1: We would calculate the signal quality, and if we observe that a signal quality drops below a certain threshold at the same time as a glucose drop, it is very likely a motion artifact error. \n\nFor hypothesis 2: We can use multi-wavelength ratios, assuming that rapid blood flow changes affect green light channels more than infrared channels. If the glucose drop is observed in the green channels but not the infrared channels, it is very likely that the system is misinterpreting a change in blood volume as a change in glucose.\n\nFor hypothesis 3: analyze temporal lag. Physiologically, a human cannot have their glucose levels rebound 30% within 2 minutes. A true glucose drop would have sustained high-intensity movement and a slow recovery curve. If the drop persists even when the accelerometer returns to zero, and the signal quality is high in all 6 channels, it is likely a physiological event.\n\nTo test these hypotheses, we can ask a patient to do two 60-second tasks:\n1. the patient shakes a limb around (hand, arm, etc) while sitting. This action will create high-motion noise but low metabolic demand.\n2. The patient carries something heavy while standing still. This creates a high-muscle glucose update but is low on motion/noise.\n\nThese two scenarios can cover the three hypotheses.\n\nTo keep the experiment HIPAA compliant, instead of storing raw sensor voltages we can log a state vector every 10 seconds. The state vector would contain information on the mean accel magnitude, the channel variance, the temperature changes, and the glucose changes. \n\nFrom the following information above, this is likely hypothesis 1, as the question mentions the glucose levels rebounding very quickly. Another physiological or blood flow changes would still require the glucose level to more gradually recover. ",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate provides a creative approach using controlled experiments to distinguish between the hypotheses, which is a valid method. The use of multi-wavelength ratios for blood flow detection is clever, and the temporal lag analysis for physiological events shows good biological understanding. However, the answer misses the key expected technique of using cross-correlation between accelerometer and signal quality index, which is more robust for real-world continuous monitoring. The experimental approach requires active patient participation which may not be practical for continuous monitoring scenarios. While the HIPAA compliance consideration is good, the state vector approach is less privacy-preserving than the expected edge detection method. The final conclusion about hypothesis 1 is reasonable but doesn't fully leverage the sensor data to make the distinction."
  },
  {
    "answer_id": 446,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "brain_teaser",
    "question_text": "A patient's PPG signal shows perfect 60 BPM heart rate but glucose readings fluctuate wildly between 70-180 mg/dL in 30-second intervals, which is physiologically impossible. Your debug logs show accelerometer variance is <0.1g (no motion), temperature is stable at 37\u00b0C, but SpO2 readings alternate between 98% and 85%. The device uses multi-wavelength glucose estimation. Given that oxyhemoglobin and deoxyhemoglobin have different optical properties, explain how the glucose algorithm could be fooled by oxygen saturation changes, and calculate the maximum possible glucose error from this effect alone.",
    "expected_answer": "Glucose estimation uses Beer-Lambert law: \u0394A = \u03b5\u00b7c\u00b7l where \u03b5 differs for oxy/deoxyhemoglobin. At 660nm (Red), \u03b5_oxy = 0.2, \u03b5_deoxy = 1.8 L/(mmol\u00b7cm). SpO2 cycling 98%\u219285% changes effective \u03b5 by ~15%, creating false glucose changes of ~22 mg/dL. Maximum error occurs when algorithm assumes constant \u03b5: error = (\u03b5_deoxy - \u03b5_oxy) \u00d7 (Hb_conc) \u00d7 (path_length) \u00d7 (SpO2_change) = 1.6 \u00d7 2.3 mmol/L \u00d7 0.1 cm \u00d7 0.13 \u2248 47 mg/dL. Solution: implement real-time SpO2 correction factor in glucose calculation using current SpO2 reading.",
    "candidate_answer": "The glucose estimation is being tricked because it relies on the ratios of light absorbed at specific wavelengths. However, the absorption spectra of hemoglobin and oxyhemoglobin differ depending on the level of oxygen saturation. If the algorithm was originally calibrated at a stable oxygen baseline, it will interpret a change in light intensity from a drop in oxyhemoglobin as a change in glucose concentration. Changes in oxygenation can also slightly change the scattering properties of red blood cells.\n\nAnother interference factor is the possibility of venous interference. If we observe a 30-second fluctuation, we know that this is impossible for blood glucose levels, but this is possible for phenomenon such as venous pulsations or peripheral perfusion changes. The sensor may be detecting the deoxygenated venous blood being returned to the heart. The algorithm then sees an increase in deoxyhemoglobin, and maps that absorption profile to a high glucose or low glucose state.\n\nCalculating the maximum error: in near-infrared spectroscopy, the molar absorptivity of water and hemoglobin is the most prominent absorption. We can use Modified Beer Lambert Law. A change of 1 mg/dL of glucose changes the absorption coefficient by about a factor of 10. We have a 13% drop in SpO2. The error can be calculated as follows: the change in glucose is equal to a constant k times change in absorbance. \nThe change of absorbance from SpO2 divided by the sensor's sensitivity to glucose, which yields 0.13 * 12 mg/dl (the 12 is derived from the NIR spectrum used by PPG sensors, where the amount of light blocked by 1% SpO2 is equal to the amount of light blocked by adding 12 mg/dL of glucose). This yields a maximum error of 156 mg/dL error.",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate correctly identified that oxygen saturation changes affect hemoglobin absorption properties and can fool the glucose algorithm. They also mentioned relevant factors like venous interference and scattering changes. However, the calculation is flawed - the 156 mg/dL error is unrealistically high for this mechanism alone, and the derivation lacks clear justification. The candidate didn't use the provided optical properties (\u03b5_oxy=0.2, \u03b5_deoxy=1.8) or show how the Beer-Lambert law applies. While the conceptual explanation is good, the quantitative analysis misses key details and appears to use arbitrary constants without proper derivation."
  },
  {
    "answer_id": 447,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "brain_teaser",
    "question_text": "You discover that Argus devices are experiencing 'phantom hypoglycemia' readings specifically in patients with Raynaud's phenomenon (poor peripheral circulation). The PPG signals show normal amplitude but distorted waveform morphology. Given that glucose estimation relies on pulse amplitude ratios between wavelengths, and Raynaud's affects blood vessel compliance, design a test to determine if the issue is (A) reduced perfusion affecting all wavelengths equally, or (B) wavelength-dependent absorption changes due to blood pooling. You have 5 patients, 10 minutes per test, and must use only on-device processing.",
    "expected_answer": "Implement wavelength ratio test: compute Red/IR amplitude ratio over 10 cardiac cycles. If (A), ratio remains constant despite amplitude reduction. If (B), ratio changes systematically as blood pools preferentially absorb certain wavelengths. Use on-device FFT to extract pulse amplitude at each wavelength, then compute coefficient of variation for ratios. CV <5% indicates (A), CV >15% indicates (B). For (A), apply perfusion index correction. For (B), implement wavelength-specific calibration curves based on pooled blood optical properties.",
    "candidate_answer": "We can design a stress and recovery test. Split the 10 patient groups into 2 5-patient groups each. The stimulus will be to submerge a hand (that has a sensor on it) into cool water for 2 minutes, which will trigger vessel constriction similar to Raynaud's. After 2 minutes, we will dry the hand and record the PPG waveform on device as the hand warms back up over the next 8 minutes.\n\nThe device will monitor AC and DC for 2 wavelengths: red and infrared. Results will fall into 2 cases:\n\n1. reduced perfusion: If the morphology looks flat but the ratio is stable (AC of red / AC of infrared), then it means the issue will be a low signal-to-noise. There is simply not enough blood flow to confidently predict the glucose level.\n\n2. blood pooling: if the blood pools, then the hemoglobin is losing oxygen locally. When we monitor AC and DC< we will see the red DC drop much faster and deeper than the IR DC, because deoxygenated blood absorbs significantly more red light than IR light. If the red signal disappears while the IR signal continues to be strong, the ratio-of-ratios is being skewed by the blood that is being pooled.\n\nThe phantom hypoglycemia comes from the fact that in Raynaud's, red light is being absorbed more than infrared light by the deoxygenated stagnant blood. The algorithm to detect glucose levels is likely calibrated to conclude that a higher ratio-of-ratios means there is a lower level of glucose (as in there is less sugar-based scattering or absorption).\n\nThe ratio-of-ratios can be calculated as (AC of red / DC of red) / (AC of IR / DC of IR).",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate correctly identified the need to test wavelength ratios and proposed a practical stress test using cold water to simulate Raynaud's. However, the test design has significant flaws: it uses 10 patients split into 2 groups of 5, which contradicts the constraint of only 5 patients total. The explanation of blood pooling mechanism is partially correct but oversimplifies the physics - deoxygenated blood affects both red and IR wavelengths, not just red. The ratio calculation uses AC/DC ratios which is less direct than the expected amplitude ratio approach. The candidate didn't address the 10-minute per patient constraint or specify how to make the determination within that timeframe. The solution lacks the quantitative threshold approach (CV analysis) needed for clear diagnosis."
  },
  {
    "answer_id": 448,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "code_review",
    "question_text": "Review this PPG signal processing code. Identify bugs, performance issues, and potential problems for embedded deployment.",
    "expected_answer": "Issues: 1) Memory leak: filtered_signal grows unbounded. 2) O(n) filter: recalculates entire sum each time instead of running sum. 3) List slicing creates copies: use deque or circular buffer. 4) Peak detection too simple: no threshold, catches noise. 5) Division by zero if no intervals. 6) Hardcoded sample rate. 7) No outlier rejection in HR calculation.",
    "candidate_answer": "Analysis issues:\nThe sampling rate variable is currently hardcoded at 100 Hz, which can be an issue if sensor's actual sampling rate is different.\n\nPerformance and memory: \n- self.filtered signal has no upper limit, and given the magnitude of the data stream, this will trigger an OOM error.\n- space complexity: for each sample, self.buffer[-1000:] creates a new list, which is unoptimal.\n- Currently the 10-sample window is being summed each time there is a new sample, which would be O(n) runtime. Instead, we can add the new value to the average and subtract the value no longer inside the window, which would take O(1) runtime.\n- To better preserve memory, we can use an exponential moving average.\n\nEmbedded deployment:\n- currently a floating point unit is being used for the moving averages and heart rates. To account for the ARM Cortex model, we should use fixed-point arithmetic.\n- A 10-sample moving average is not robust enough to remove motion artifacts. We should use a wider filter such as a bandpass or a lowpass.\n- The peak detection relies on local maxima. Ideally, it should use a dynamic threshold.\n- We should use circular buffers instead of lists to avoid memory issues.\n- If we utilize a window size that is a power of 2, we can replace division with bit shifts, which would improve speed on microcontrollers. \n",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate identified most of the critical issues and provided a solid high-level redesign. However, the proposed code has significant correctness problems: the peak detection logic appears inverted (detects troughs instead of peaks), the heart-rate calculation is wrong (uses fixed min_peak_dist instead of actual intervals), and the implementation details are incomplete. The answer demonstrates good understanding of embedded constraints but needs refinement in the actual implementation."
  },
  {
    "answer_id": 449,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "code_review",
    "question_text": "This code handles patient glucose data. Identify security, privacy, and reliability issues.",
    "expected_answer": "Security: SQL injection in all queries - use parameterized queries. Privacy: No encryption at rest, no access control, bulk export without audit, no data anonymization. HIPAA violations: no access logging, no encryption. Reliability: no connection error handling, no transaction rollback, single database connection (no pooling).",
    "candidate_answer": "Security issues:\n- SQL injections through f-strings. Instead, use parameterized queries.\n- Any part of the application can call delete_patient or export_all_data. Only authorized personnel should be able to do these functions.\n\nPrivacy:\n- Currently, the information in patient_data.db is stored in plaintext. If this data is leaked, all information about the patient is easily visible. Instead, sensitive fields should be encrypted, and the database file permissions should be strictly regulated.\n- The export.json should be stored in a safer, encrypted directory.\n\nReliability:\n- Use try-except blocks to manage cases where the database is inaccessible, the disk is full, or the table is nonexistent. Elsewise, the program may crash, which can result in valuable patient data being lost.\n- The sqlite3 connection is never explicitly closed. We should either use a context manager (with self.db) or make sure to call self.db.close()\n- Add audit logging to investigate data breaches\n- Add anonymization to patient data such that the data can be used for research purposes\n\n\n\n",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate correctly identified the major security vulnerabilities (SQL injection) and privacy issues (plaintext storage, lack of encryption). They provided a solid JavaScript implementation that addresses SQL injection through parameterized queries and added basic audit logging. However, the implementation has several significant gaps: encryption at rest is not actually implemented, the anonymization approach is flawed (using a hardcoded 'Secret Key' and SHA256 which is reversible), and there's no actual access control system - just audit logging. The reliability improvements are partial - error handling is added but connection management and transaction handling are missing. The code also contains syntax errors (INSERT INFO instead of INSERT INTO, db_run vs db.run) and the anonymization function is not properly scoped. While the direction is correct, the implementation doesn't fully deliver on the security and privacy promises."
  },
  {
    "answer_id": 450,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "code_review",
    "question_text": "Review this BLE communication code for a medical wearable. Identify issues with power, reliability, and real-time performance.",
    "expected_answer": "Power: 1ms polling is wasteful, use event-driven/condition variable. Queue: list.pop(0) is O(n), use deque. Thread safety: data_queue accessed without lock (race condition). Reliability: exception in thread kills transmitter silently, no reconnection logic. Real-time: blocking retries (1 sec worst case) stalls queue. Missing: queue size limit (memory), graceful shutdown, connection state machine.",
    "candidate_answer": "Power:\n- for the while loop, a 1 ms sleep is short enough that the CPU is often kept in a high power state, because context switching can take longer than 1 ms\n- conversely, even when is_connected is false or if the queue is empty, the thread still activates 1000/second to check the boolean\n\nReliability:\n- the _send_ble_packet function states that if a packet fails 10 times, the entire transmission loop freezes for 1 second. This does not prevent the data_queue from growing, so the data may grow faster than it is being cleared, which will lead to memory issues.\n- There is no try-catch block to handle the Exception, which will cause the transmission_thread to crash.\n\nPerformance:\n- sleep() is not fully accurate on time, and it can differ depending on the OS scheduler tick rate\n- the queue will take O(n) time to pop out, because all the other elements must shift to the left. This becomes a problem quickly when the queue grows, and it will slow down the transmitter. \n- the transmitter should be able to be cleanly stopped\n\n\n\n\n",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate identified most critical issues correctly (power waste from 1ms polling, O(n) queue operations, thread crashes) and provided a solid refactor using Python's queue.Queue for thread safety and O(1) operations. However, the refactor has significant bugs: the _transmit_loop has syntax errors (missing 'attempt' variable, incorrect exception handling structure), doesn't actually use the stop_event, and the queue_data overflow handling drops oldest data without notification. The power optimization is incomplete - still uses polling instead of condition variables/event-driven approach. Real-time concerns weren't fully addressed - retry backoff still blocks the transmitter. Overall good analysis but implementation needs refinement."
  },
  {
    "answer_id": 455,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "coding",
    "question_text": "Design and implement a real-time PPG signal quality classifier for Argus wearable that runs on ARM Cortex-M4 with 256KB RAM. The classifier must detect motion artifacts, poor perfusion, and ambient light interference in multi-wavelength PPG data (Red, IR, Green channels) within 20ms per window. Include memory-optimized feature extraction and a quantized neural network (<50KB model size).",
    "expected_answer": "Implement sliding window processing (64 samples at 125Hz), extract 6-channel PPG features (AC/DC ratios, skewness, zero-crossing rate), use separable CNN with int8 quantization, achieve >95% accuracy on motion artifact detection while maintaining <15ms inference time. Should include circular buffer implementation for streaming data and explain quantization strategy for edge deployment.",
    "candidate_answer": "Using math and statistics modules, we use a 2 second window of 100 samples at 50 Hz, and use a moving average high-pass DC filter for feature extraction. High ZCR is generally indicative of motion artifacts, so we use this metric to check for motion. The perfusion index is used to see if the sensor is seeing changes in blood volume. The ratio of ratios measures if there is leakage of ambient light.\n\nFor the quantized neural network, we decide to use a shallow neural network with 10-16-4 architecture, which is approximately 224 parameters. This will allow plenty of remaining space for the 50 KB limit. The processing speed would also be significantly under 20 ms.\n",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate demonstrates basic understanding of PPG signal processing concepts but falls significantly short of the requirements. The window size (100 samples at 50Hz = 2 seconds) violates the 20ms processing requirement. The feature extraction is incomplete - missing key features like skewness and AC/DC ratios for all channels. The neural network architecture is too large (32-16-4 vs expected 10-16-4) and the parameter count is underestimated. There's no mention of int8 quantization strategy, circular buffer implementation, or ARM Cortex-M4 specific optimizations. The code has bugs (undefined variables like '_ac_signal' and 'e'), lacks streaming data handling, and doesn't address the multi-wavelength classification requirements. The solution doesn't meet the 95% accuracy target or memory constraints."
  },
  {
    "answer_id": 456,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "coding",
    "question_text": "Implement a HIPAA-compliant cloud pipeline for processing Argus CGM data that handles 10,000 concurrent users with real-time glucose predictions. Design the data flow from BLE edge device to PostgreSQL with encryption at rest, including differential privacy for research datasets and automated PHI detection/removal.",
    "expected_answer": "Complete pipeline with AES-256 encryption, automated PHI detection using regex/ML, PostgreSQL with row-level security, Lambda functions for scaling, S3 for encrypted storage, implement \u03b5-differential privacy (\u03b5=1.0) for research data, include audit logs and data retention policies per FDA requirements.",
    "candidate_answer": "For this code: initially, we set up 2 security components: an encryption key via a Fernet generated key, and a PHI detector through the AI engine Presidio.\n\nWe then use a secure data ingestion pipeline: first, medical data is encrypted during transit, so we must decrypt this medical data. The real-time prediction models takes in the current glucose level reading and simulates an AI model, and currently uses a placeholder prediction value of 1.05. This prediction is logged onto the user's medical records.\n\nTo follow HIPAA guidelines, we use the phi_detector method to scan the data for any identifying information; the record is dropped entirely if any text is identified. Since running the PHI detector on 10,000+ users is computationally heavy (because it uses NLP models to scan the texts), we would ideally run the PHI scrubbing as a background job after the user has already received their real-time glucose prediction, as to prevent any lag in the patient's user experience. \n\nTo prevent researchers from figuring out what data belongs to what patient, we also add Laplacian noise to the glucose number, allowing the data to be useful for a group study but not accurate enough to identify an individual. Additionally, the user_id is replaced with a random uuid, thus keeping the patient's actual account ID hidden.\n\nFinally, the processed data is sent to a data base such as PostgreSQL; this function would interact with a database that has disk-level encryption. ",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate demonstrates basic understanding of HIPAA compliance requirements and attempts to implement key components like encryption, PHI detection, and differential privacy. However, the implementation has significant gaps and errors that would prevent it from being production-ready. The code contains syntax errors (Fernet class usage, undefined variables), incomplete implementations (store_glucose_prediction is a stub), and lacks critical components needed for a senior-level solution. The architecture doesn't address scaling to 10,000 concurrent users, real-time processing requirements, or complete HIPAA compliance measures like audit logging and data retention policies. The differential privacy implementation is incomplete and the PHI detection approach is overly aggressive (dropping entire records)."
  },
  {
    "answer_id": 457,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "coding",
    "question_text": "Given 6-channel PPG data corrupted with severe motion artifacts (accelerometer shows 3G spikes), implement a real-time adaptive filtering algorithm that reconstructs the true PPG waveform using the relationship between optical channels and motion sensors. Must handle skin tone variations (Fitzpatrick I-VI) and maintain glucose estimation accuracy within 15% error margin.",
    "expected_answer": "Implement adaptive LMS filter per channel with accelerometer as reference, skin-tone adaptive gain compensation using pre-calibrated coefficients, Kalman filter fusion across wavelengths, confidence scoring based on residual motion energy and perfusion index. Handle edge cases: complete signal loss, LED saturation, and temperature drift.",
    "candidate_answer": "This code uses a multi-channel NLMS adaptive filter. Since 3G spikes can be in any direction, we calculate a Euclidean norm to measure the intensity of a motion. For each optical channel we maintain 32 weights, where we look at at the last 32 motion readings and predict the current motion noise in the PPG signal. The predicted noise is then subtracted from the raw PPG.\n\nTo handle the 3G spikes, whenever the 3G spike happens - and thus the acc_energy \"energy\" becomes large - the step magnitude decreases, which controls for the spike not corrupting the filter's memory. The code also accounts for Fitzpatrick skin tone scaling by readjusting the mu_base learning rate depending on the skin tone input. This prevents the filter from treating a low-amplitude pulse as noise and cleaning it away. ",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate demonstrates understanding of adaptive filtering concepts and motion artifact removal, but the implementation has significant gaps. While they correctly identified the need for NLMS adaptive filtering and skin-tone compensation, the code fails to properly handle the 6-channel PPG input (only processes 6 channels instead of the required 3 cleaned channels), lacks wavelength-specific processing for glucose estimation, and uses an overly simplistic confidence metric. The motion handling approach is reasonable but the implementation details are flawed - the weights are updated with the cleaned signal rather than the error signal, which is incorrect for NLMS. The skin tone compensation is present but rudimentary. Most critically, the solution doesn't address glucose estimation accuracy requirements or provide the expected Kalman filter fusion across wavelengths."
  },
  {
    "answer_id": 441,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "system_design",
    "question_text": "Design a HIPAA-compliant architecture for KOS AI's Argus CGM device that processes 6-channel PPG data from MAXM86146 sensors with <50ms latency while maintaining continuous glucose estimation accuracy. The system must handle skin-tone adaptive algorithms (Fitzpatrick I-VI), motion artifact removal, and edge ML inference on ARM Cortex-M4 processors. How would you architect the data pipeline from sensor to cloud, considering BLE bandwidth limitations (250kbps), intermittent connectivity, and the need for real-time alerts during hypoglycemic events?",
    "expected_answer": "Candidate should propose a tiered architecture: 1) Edge preprocessing with adaptive FIR filters and motion artifact detection using accelerometer fusion, 2) Compressed glucose trend vectors with lossless encoding for BLE transmission, 3) Local fall-back inference using quantized neural networks (<2MB RAM footprint), 4) Cloud sync with encrypted buffers during connectivity gaps, 5) Emergency alert subsystem bypassing normal transmission queues. Should mention specific compression ratios for PPG data (e.g., 8:1 compression maintaining 95% signal fidelity) and discuss FDA 510(k) premarket submission requirements.",
    "candidate_answer": "First, we should understand the caveats: in the situation of continuous glucose estimation accuracy, accurate signal acquisition is especially important in synchronizing data from each of the channels. \n\nThe data pipeline would have the following components:\n\n1. Sensor level acquisition: Each sample should be timestamped at acquisition to prevent downstream errors. An example would be using a timestamp at ADC ISR. An ideal output from sensor level acquisition would be timestamp - channel ID - raw PPG value. This is especially important due to the 50 ms latency, which is very sensitive to timing errors.\n\nAt this stage, sensors should also account for different skin tones: darker skin may require a higher LED drive current, while lighter skin may require a lower LED drive current to avoid saturation. This may necessitate the use of a calibration lookup table. All of these aspects should be corrected before feature extraction is done for the ML pipeline.\n\nA possible data output could be [timestamp, channel ID, raw PPG value, LED current used, local sensor temperature].\n\n2. Preprocessing and cleaning data: before the data is sent to the ML inferences, we first sort each data by channel and align them by time. We also consider that LEDs can get brighter or dimmer with temperature, and that this may be a confound variable when determining if there was a glucose change. We can implement a corrective signal by adjusting the raw data to use the measured temperature and the known LED current, and then calculate what the LED light would have been at this specific temperature. This also mitigates any bias from temperature.\n\nBuilding off the temperature confound, we split the PPG signal into two parts: the DC baseline and the AC. The DC contains information about the light absorbed by skin and tissue, and is dependent on skin tone. We should subtract the DC baseline from the signal, which would yield the AC. This would prevent us from hard-coding skin tones.\n\nTo estimate skin tone, we can simply compare how much of the light comes back as infrared (paler skin), and how much comes back as visible light (skin with more melanin).\n\nSince cardiac signals have a frequency of approximately 0.5-8 Hz, we can apply a Butterworth filter to remove noise from motion, electronics, and other artifacts. If accelerometer data is available, we can observe if there are sudden changes to the PPG signal, which would indicate sudden movement from the user. This would allow us to cancel the motion noise.\n\nTo reduce bias, we now normalize the motion-cleaned signal across skin tones. We can implement a straightforward quality score that measures the signal's strength, consistency, motion, the sensor health, and the stability. If this quality score is low, we should either collect better data or improve our preprocessing and cleaning pipeline. If the quality score meets our standards, we can pass this data to ML inference.\n\n3. Our input is now the AC PPG (from 6 channels), timestamps, and quality score. We can convert this signal into individual heartbeats, because glucose features are typically calculated per beat and not per sample. We should detect pulse peaks, and identify the start, the peak, and the end of the beat. All 6 channels should then be aligned to the same beat. We can use the quality score metric to evaluate the beat-level signal.\n\nThe output is now the beat identity, the beat timestamp, the 6 aligned waveform segments, the beat-level quality score, and an estimate of the heart rate.\n\n4. Feature extraction: for now, we can extract the following features: pulse amplitude, AC/DC ratios, rise time and decay time, phase differences between wavelengths, spectral energy ratios, and rate of change versus previous beats. These features are less susceptible to noise and skin tone difference confounds.\n\nAn ideal output would be a feature vector, a feature-level confidence, and the timestamps of the heartbeats. We do not want to directly send a waveform to the ML pipeline, because the waveform is more susceptible to noise and will have a heavier processing strain for the Cortex-M4 processors.\n\n5. Edge ML inference: our inputs are the feature vector, the sensor temperature, the skin-tone proxy, and the heartbeat quality score. For now, we can run a quantized neural network on the MCU. We can use a regression algorithm to estimate glucose level, and we can use a classified learning system to determine if the user is at risk of hypoglycemia. Each estimates will be assigned a confidence estimation.\n\nThe outputs of this inference are a glucose estimate (measured in mg/dL), a confidence score for the glucose estimate, a confidence score for the hypoglycemia risk, trend on rising/falling/stable levels, and a risk flag (if the glucose level is normal, low, or critical).\n\nThe edge ML is crucial because the timing of glucose detection is key. Since we are relying on a cloud system and on a limited BLE, we need to implement an edge ML system that can still run even if BLE is down or if there is latency issues in cloud inference. Additionally, raw PPG data is much too heavy for BLE throughput, and the glucose information in PPG is fairly low-amplitude. The edge ML system allows more real-time inferences and can adapt dynamically to suboptimal data. \n\nTo have the Cortex-M4 handle ML, we can use a small fixed-point model (typically a 1d CNN or a small FCNN). \n\nTo calculate the compression ratio for the 250 kps BLE limit, we can do a rough calculation of: assuming 500 Hz sampling rate for a CGM-level PPG, and assuming a 16-bit per sample resolution, our raw data rate per channel is 8000 bits/second. Across 6 channels this is 48000bits/second = 48 kbps. Edge ML can convert raw samples into a single glucose estimate and confidence, at about 32 bytes per second, so a good starting ratio for compression is 6000/32 = 187.5.\n\n6. Alert system: The inputs are a current glucose estimate, the confidence score for this estimate, a confidence score for this hypoglycemia risk, trends, and quality scores. We can build a simple alert system that will alert the user if glucose is dropping below a certain level and the confidence is high, and if this pattern is sustained over multiple heartbeats. The outputs of this would be an alert trigger, an associated alert severity, and a record of this alert to be sent to a local event log.\n\n7. BLE transmission: the data that is sent through the BLE is the glucose estimation, the confidence scores for the estimation and the risk estimation, trends, and alerts. The output should be 1-5 Hz glucose updates, any immediate alert data packets, and brief timestamp summaries.\n\nTo handle BLE dropouts, we can store our data of glucose estimates, alerts, and quality scores into a circular flash buffer, and tag these packets of data with timestamps. This can be synchronized later when Internet connection returns.\n\n8. Cloud processing: we can send the log of glucose estimations, the selected feature from feature extraction, the quality score metrics, and any consented user metadata to the cloud for further storage and analysis. Depending on future uses of the data, we could use a cloud stack such as AWS Lambda, InfluxDB, PostgreSQL, RESTAPI, etc. to prioritize model retraining and keep track of model checkpoints and device-cloud communciations. \n\n\n",
    "kimi2_score": 50.0,
    "kimi2_feedback": "Unable to evaluate automatically. Manual review required."
  },
  {
    "answer_id": 442,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "system_design",
    "question_text": "You need to design a multi-tenant cloud system for KOS AI that serves both individual users and clinical trials while maintaining data isolation. How would you handle the real-time aggregation requirements for clinical trial cohorts (1000+ concurrent users) needing population-level glucose trend analysis, while ensuring individual user data remains HIPAA-compliant? Address the challenge of providing near-real-time analytics (sub-15 second latency) across aggregated PPG datasets exceeding 10TB daily.",
    "expected_answer": "Should design partitioned DynamoDB/S3 architecture with KMS encryption, implement lambda-based stream processing for PPG feature extraction, use Redis clusters for cohort-level aggregation with sliding window analysis, and propose a tiered storage system (hot/warm/cold) for cost optimization. Must address PHI tokenization for clinical trial de-identification and discuss differential privacy mechanisms for population analytics.",
    "candidate_answer": "We should implement.a decoupled architecture that separates IoT ingestion and Analytics. To make computing power cost-efficient, we should use a hybrid isolation model to allow clinical databases to use the shared compute resources while keeping sensitive data isolated.\n\n1. Data isolation: Assuming each tenant is a clinical trial, build a database-per-tenant schema such that no one can access a tenant's data without a tenant_id filter. A shared microservices cluster can use tenant context propagation such that each request has a JWT with an associated tenant_id. Finally, we can implement envelope encryption, such that each trial has its own master key in a KMS.\n\n2. Real-Time glucose trend analysis: to read and analyze high-velocity data from a CGM system, the CGM posts the data to a globally distributed IoT Gateway, which we can have validating the JWT, the tenant_id, and attach metadata to the payload. This data can be pushed through a message bus such as Apache Kafka or AWS Kinesis. We should partition by user ID to make sure that a single user's readings are processed chrnologically.\n\nFor trend analysis, we can use a stream processing engine (such as Apache Flink) to group the data into x-minute blocks. For each block, we can calculate mean glucose. We also use a sliding window approach to calculate time in range across a 24-hour window, and we can join the individual participant data with the trial group metadata. The trial group metadata is ideally stored in a fast-access cache (such as Redis).\n\nAfter the trends are aggregated, the trends are written to a time-series database. To optimize for storage, we can use a Delta-Delta encoding to compress float values for the glucose. To update the trial cohort's average glucose level,  we can set up a background process that maintains a materialized view of the average glucose level, and this view would be updated when new data is written in to the TSDB.\n\n3. Addressing concurrency: we can implement a 2-layer system to optimize for the read-strategy of handling 1000+ concurrent users. The first layer uses a caching system such as Redis Sorted Set. In the case where a user requests a dashboard, the API fetches from Redis with O(1) complexity instead of directly querying the database.\n\nThe second layer can utilize a pub/sub Websocket system that will push specific data points at critical moments. If the stream processor identifies a trend shift or a high-glucose alert, this data point will be pushed to the users' browsers.\n\n4. Audit and Security: each tenant's data in the TSDB is encrypted with a unique customer managed key. We can also implement a query filter system such that any API call to the TSDB is intercepted by some middleware that injects a WHERE trial_id = 'ABC' clause. This ensures that a researcher will not be able to access a different trial's cohort. This will allow the system to be HIPAA compliant. \n\nChallenges: to provide near-real-time analytics across large PPG datasets, the ideal architecture should not rely on traditional database indexing and standard SQL operations. \n\nFor the 10TB data stream, we can convert the aggregated PPG data with Protobuf for binary serialization. The data will be ingested into Apache Kafka, and we can partition the Kafka topics by trial ID, allowing data for a specific clinical trial to be in the same set of consumer nodes.\n\nWe can use a tool such as Apache Flink to process the stream, and use a state store to keep a tally on cohort metrics. Some approaches include using sliding windows to look at the last X seconds of data, or utilize a Flink cluster to handle noise filtering from the PPG wave. \n\nTo store the 10TB/day data, we can push Flink aggregates into Redis for live metrics, and push the brief timestamp summaries into ClickHouse. The raw data should be streamed into an object storage system (such as S3) in Parquet format. \n\nTo prevent a trial's 10TB data stream from slowing down another trial's dashbboard, we can improve a multi-tenant isolation approach by using a Kubernetes Resource Quota, and/or using a unique KMS (as mentioned above) to prevent a trial's PPG data from being decrypted by another trial's credentials. \n\nFrom the user end, we should not let the users be able to make refresh commands in a short amount of time. Every time Flink updates a cohort's trend, the update will be pushed to the Redis pub/sub channel, and the WebSocket server listens only to that channel and pushes the update to those 1000 users authorized for that specific trial. \n\n\n",
    "kimi2_score": 85.0,
    "kimi2_feedback": "The candidate demonstrates strong technical understanding of multi-tenant cloud architecture and real-time analytics. They correctly identified key components like tenant isolation, stream processing, caching layers, and encryption. The solution addresses HIPAA compliance through encryption and query filtering. However, the answer lacks specific discussion of PHI tokenization and differential privacy mechanisms for population analytics, which were explicitly requested. The architecture described is solid but could benefit from more concrete implementation details on the 15-second latency requirement and the 10TB daily data volume handling. The candidate shows good understanding of distributed systems concepts but misses some critical HIPAA compliance aspects."
  },
  {
    "answer_id": 443,
    "candidate_id": 14,
    "candidate_name": "Natalie Wang",
    "category": "system_design",
    "question_text": "KOS AI's Argus device must operate for 7+ days on a 100mAh battery while continuously sampling 6-channel PPG at 100Hz, running skin-tone adaptive algorithms, and maintaining BLE connectivity. Design a power management strategy that dynamically adjusts sampling rates based on signal quality, glucose volatility, and battery state. How would you implement adaptive duty cycling that maintains clinical-grade accuracy (>95% MARD) while extending battery life, and what fail-safes would you implement for critical glucose events?",
    "expected_answer": "Should propose adaptive sampling algorithm: 1) Variable PPG sampling (25-100Hz) based on signal SNR and glucose derivative, 2) Predictive duty cycling using glucose trend forecasting, 3) BLE connection interval negotiation (7.5ms-4s adaptive), 4) Emergency override protocol for <70mg/dL events bypassing power optimization, 5) Hardware abstraction layer for MAXM86146 power modes. Must discuss specific power budgets (e.g., 1.2mA avg current for continuous mode) and implement watchdog mechanisms for safety-critical thresholds.",
    "candidate_answer": "1. state machine: instead of using a fixed 100 Hz rate, we can modulate the system's energy budget with 3 vectors: signal quality, glucose volatility, and battery state of charge. We can filter to only use our cleaned PPG signals; to do so efficiently, we can pause the PPG sampling rate when the accelerometer detects high amounts of motion, or the sampling rate can be paused at a predetermined interval during the day where the signal data collection is likely to be poor.\n\nWe note that glucose levels do not necessarily change at the same rate, so we set two states: a stable state and a volatile state. If there is little change in glucose level, we can increase a rest interval between PPG bursts. If there is a rapid rise or fall in glucose level detected, we can tighten the interval to 1-2 minutes to maintain MARD.\n\n2. Adaptive Duty Cycling: we cannot indefinitely lower the sampling rate to manage power, so we can try segmenting the data stream into epochs. For each epoch, there can be three stages: first stage is a high-power acquisition burst to wake the MCU from a low-level or sleep activity. The AFE can then activate and fire the 6 LEDs to sample from the 6 channels, at a high frequency of 100 Hz for a fixed window of seconds. This window would ideally be a few seconds long to capture multiple cardiac cycles. \n\nThe second stage is to pull and process the data. The MCU pulls the data from the AFE's buffer, and the skin-tone algorithm adjusts LED settings for the next high-power burst. We can then conduct analyses and processes for the glucose level estimations for the first few hundred samples. These can be simple calculations done on the MCU. Only the calculated results, as well as the quality score for the signal, will be sent via BLE.\n\nThe third stage is to have the MCU shut down non-essential peripherals, and enter a deeper state of sleep. The BLE state would also enter this sleep stage; it will still be connected but will ignore connection events for a set time period. \n\nTo implement the sleep portion, this portion will be scaled depending on the previous variables of signal quality, glucose volatility, and battery condition. This allows the wearable to efficiently gather data during volatile changes in glucose levels, and preserve battery and computation power when either the data collection is poor, there is unlikely to be changes in the glucose levels, or the user is experiencing motion and thus the data collected will be too noisy.\n\nFail-safe implementations: if we have a poor signal quality metric, our system should refuse to do a reading. This will prevent any false positives or hallucinations. \nWe can also implement a double-checking strategy: if the algorithm on the MCU detects a critical value from the glucose (too low or too high), the system should trigger a second burst to confirm the reading. If the results are consistent, then the alert should be sent to the user. This reduces false positive rates. \n\nFor battery fail-safes: if a battery is critically low, we should have the system shut down all non-essential features (such as data sync or skin tone recalibration), and send a high-level alert to the mobile app to tell the user that the battery is running low. \n\nFinally, we can do a common-sense approach: if a PPG signal indicates that a glucose level rate has fallen or risen by an amount not possible in human physiology, we should set up an alert/flag system to flag a sensor error. Critical alerts to the user should be able to bypass any Do Not Disturb modes, and the wearable itself could contain a special LED monitor or a vibration feature to alert the user to a critical glucose level. This can be operable within a 100 mAh battery and the vibration function can work even if BLE connection drops. \n\n\n",
    "kimi2_score": 75.0,
    "kimi2_feedback": "The candidate demonstrates a solid understanding of adaptive power management principles and proposes a reasonable 3-stage duty cycling approach. The answer correctly identifies key variables (signal quality, glucose volatility, battery state) and provides practical fail-safes. However, the response lacks specific technical details required for a senior-level system design question - missing concrete power budgets, current consumption figures, and hardware-specific optimizations. The BLE power management is oversimplified, and the emergency override mechanism isn't clearly prioritized over power optimization. While the epoch-based approach is conceptually sound, it needs more rigorous implementation details for clinical-grade accuracy."
  },
  {
    "answer_id": 277,
    "candidate_id": 9,
    "candidate_name": "Vikas Ramachandra",
    "category": "signal_processing",
    "question_text": "Given 3-wavelength PPG data (Red, IR, Green) sampled at 100Hz with known SpO2 ground truth, design a frequency-domain approach to separate glucose-related microvascular signals from macrovascular noise. How would you handle wavelength-dependent scattering effects while preserving the subtle glucose-induced phase shifts (<0.1\u03c0 radians) across channels?",
    "expected_answer": "Propose complex FFT-based approach: Windowed FFT (64-point) with overlap-save for real-time processing, followed by wavelength-specific frequency-domain adaptive filtering. Discuss using Red/IR ratio for motion artifact detection, Green channel for glucose signal isolation due to higher dermal penetration. Address phase unwrapping techniques and how to maintain coherence between channels. Mention using Goertzel algorithm for targeted frequency bins to reduce M4 computational load.",
    "candidate_answer": "we would use the specific frequency bands and filters which are bandpass filters to narrow down bands for each signal to differentiate between them.\nwe can use amplitude and phase domain signals to better separate them out and reduce noise. also the phase signatures can be used as features to tease out higher order effects.\nfor wavelength dependent scattering, we could use some signal denoising/reconstruction approaches such as Weiner filter 'deblurring' type methods, which can also be made adaptive to the signal.\n\n",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate provided a very high-level response that lacks the technical depth expected for a senior-level signal processing question. While they mentioned frequency bands and phase domain signals, they failed to address the core challenge of glucose-related microvascular separation from macrovascular noise in the frequency domain. The answer doesn't specify how to handle the 3-wavelength PPG data, doesn't mention FFT or any frequency-domain processing techniques, and completely misses the critical requirement of preserving glucose-induced phase shifts <0.1\u03c0 radians. The Weiner filter suggestion is generic and doesn't address wavelength-dependent scattering in PPG signals specifically. There's no discussion of windowing techniques, overlap processing, motion artifact detection using Red/IR ratios, or the specialized processing needed for glucose signal isolation. The response lacks concrete implementation details and doesn't demonstrate understanding of PPG signal characteristics or the constraints of real-time processing at 100Hz sampling."
  },
  {
    "answer_id": 286,
    "candidate_id": 9,
    "candidate_name": "Vikas Ramachandra",
    "category": "system_design",
    "question_text": "Design a HIPAA-compliant cloud architecture for processing PPG-derived glucose estimates from 1M+ active Argus devices while supporting real-time alerts for hypo/hyperglycemic events. How would you handle the dual requirements of immediate alerting (<5 seconds) and long-term analytics with differential privacy guarantees?",
    "expected_answer": "Should propose: 1) Edge-first architecture with local glucose estimation and alert generation, 2) Tiered cloud system with hot path (Lambda/Kinesis) for alerts and cold path (S3/Glue) for analytics, 3) Homomorphic encryption for glucose values in transit, 4) Federated learning setup for model updates without raw PPG data sharing, 5) Time-series database (InfluxDB/TimescaleDB) with retention policies, 6) Differential privacy mechanisms (epsilon-delta) for population analytics, 7) Multi-region deployment with HIPAA-compliant VPCs and audit logging, 8) Circuit breaker for cloud API failures with local alert queue persistence",
    "candidate_answer": "edge first processing, stream computation and processing modules, encryption on the fly are the key elements.\npower aware algorithms are needed\nwe can use differential privacy and ML models which can deal with that.\nsome processing (heavy analysis such as retraining models across the population) should be done on the cloud.\nother light processing needs to be on the edge for real time insights to the end user with their own data.\nhipaa compliance is a necessary part to think of for storage and encryption modules.",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate demonstrates basic awareness of edge/cloud processing and privacy concepts but provides only high-level, fragmented thoughts rather than a coherent system design. The response lacks specific technical details, architectural components, or implementation strategies needed for a senior-level system design question. Key missing elements include: concrete architecture patterns, specific HIPAA compliance mechanisms, detailed latency optimization strategies, differential privacy implementation details, and failure handling approaches. The answer reads like a collection of buzzwords without demonstrating deep understanding of how these pieces would actually work together in a production system handling 1M+ devices with <5s alerting requirements."
  },
  {
    "answer_id": 34,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "brain_teaser",
    "question_text": "You have a medical imaging dataset with brain tumor segmentations where 95% of pixels are non-tumor (background) and 5% are tumor regions. Your nnU-Net model achieves 96% pixel-wise accuracy but fails to detect 40% of actual tumors (low recall). You have 2 weeks to improve this before a clinical trial. You can use SAM, knowledge distillation, or data augmentation. What's your systematic approach to achieve >90% tumor detection recall while maintaining reasonable precision?",
    "expected_answer": "Address severe class imbalance through focal loss/tversky loss, implement hard negative mining, use SAM for precise boundary refinement, apply aggressive data augmentation (rotation, elastic deformation, intensity variation), implement test-time augmentation, and consider teacher-student knowledge distillation from a larger model. Focus on recall improvement via ensemble of models with different augmentation strategies.",
    "candidate_answer": "Use data augmentation to generate more difficult samples to improve the model robustness.",
    "kimi2_score": 25.0,
    "kimi2_feedback": "The candidate's answer is far too brief and lacks the systematic approach required for this critical clinical application. While data augmentation is mentioned, there's no discussion of the fundamental class imbalance problem, loss function optimization, specific augmentation strategies for medical imaging, or how to specifically target the recall issue. The response doesn't address the 40% false negative rate or explain how augmentation alone would solve this. For a clinical trial with 2-week deadline, this answer shows poor understanding of medical imaging challenges and lacks actionable steps."
  },
  {
    "answer_id": 35,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "brain_teaser",
    "question_text": "Your email security system processes 100K emails/day using LLM agents for phishing detection. Suddenly, latency increases from 2s to 15s per email. You have a Docker containerized pipeline with vLLM backend on AWS. Debug systematically - what's your 30-minute troubleshooting plan?",
    "expected_answer": "Check CloudWatch for CPU/GPU metrics \u2192 verify vLLM batch size/queue depth \u2192 inspect container logs for memory leaks \u2192 test prompt length vs latency \u2192 check if rate limiting from GPT-4 API \u2192 implement request caching \u2192 fallback to smaller distilled model \u2192 scale horizontally with distributed inference",
    "candidate_answer": "First check the CPU/GPU metrics and container logs to see whether the slowdown comes from CPU/GPU fallback or system failure, then isolate stages of the whole pipeline to diagnose the problem. Finally fix it and verify using a small subset.",
    "kimi2_score": 65.0,
    "kimi2_feedback": "The candidate identified the right starting points (CPU/GPU metrics and container logs) but provided a vague, high-level approach rather than a systematic 30-minute troubleshooting plan. The response lacks specific actionable steps, concrete metrics to check, or the expected depth of debugging for a production LLM system. While mentioning pipeline isolation is good, the 'fix it and verify' step is too generic for a time-critical production issue."
  },
  {
    "answer_id": 36,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "brain_teaser",
    "question_text": "You trained a GNN-LSTM model (GraphPath) on EHR data for disease progression. It performs well on training data but fails on new patients. You suspect distribution shift. How do you validate this and adapt the model using zero-shot/one-shot learning with minimal retraining?",
    "expected_answer": "Use patient embeddings to detect distribution shift via KL divergence on learned representations. Implement zero-shot adaptation using patient similarity graphs. Apply meta-learning with few-shot examples from new patients. Use GraphSAGE inductive learning for unseen nodes. Consider federated learning for privacy-preserving adaptation.",
    "candidate_answer": "First, quantitatively validate the distribution shift by computing statistics like KL divergence. If the shift exists, train a lightweight adapter using a small calibration dataset to recover performance with minimal retraining.",
    "kimi2_score": 45.0,
    "kimi2_feedback": "The candidate correctly identifies the need to validate distribution shift using KL divergence, which is a good starting point. However, the response is overly simplistic and lacks the depth expected for a mid-level brain teaser. The proposed solution of using a 'lightweight adapter' is vague and doesn't leverage the GNN-LSTM architecture or graph-based nature of the problem. The answer misses key concepts like patient similarity graphs, meta-learning approaches, and inductive learning capabilities that are crucial for zero-shot/one-shot adaptation in GNN models. The response doesn't address how to handle unseen patients (nodes) or privacy considerations, and fails to provide concrete technical details for the adaptation strategy."
  },
  {
    "answer_id": 37,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "brain_teaser",
    "question_text": "Your transformer-based medication recommendation system (RAREMed) has 95% accuracy but doctors report it suggests rare medications too frequently. The model was trained on biased EHR data where rare diseases are overrepresented. How do you rebalance without retraining the entire model?",
    "expected_answer": "Apply post-calibration using Platt scaling or isotonic regression on prediction probabilities. Implement frequency-based threshold adjustment. Use DPO/RLOO to fine-tune just the final layers with doctor feedback. Apply Bayesian uncertainty estimation to downweight low-confidence rare predictions. Consider mixture-of-experts with disease-frequency experts.",
    "candidate_answer": "Apply post-hoc rebalancing strategy to match real-world medication priors without modifying the main model.",
    "kimi2_score": 35.0,
    "kimi2_feedback": "The candidate identified the core concept of post-hoc rebalancing but provided an extremely vague solution that lacks technical depth. While mentioning 'matching real-world medication priors' is directionally correct, the response fails to specify any concrete techniques, algorithms, or implementation details. There's no mention of specific calibration methods (Platt scaling, isotonic regression), uncertainty quantification, or fine-tuning approaches. The answer reads like a high-level concept rather than a technical solution, missing critical implementation details that would make it actionable."
  },
  {
    "answer_id": 51,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "brain_teaser",
    "question_text": "You are building a system to detect rare brain tumors in MRI scans using a zero-shot learning approach with SAM (Segment Anything Model). Your dataset has only 5 positive examples of the rare tumor type. Design a strategy to leverage SAM's capabilities along with knowledge distillation from a pre-trained brain tumor segmentation model (nnU-Net) to achieve reasonable performance. How would you handle the extreme class imbalance and limited positive samples?",
    "expected_answer": "Strategy should include: 1) Using SAM's zero-shot capabilities with carefully engineered prompts (text + point prompts), 2) Implementing knowledge distillation from nnU-Net to transfer general brain anatomy knowledge, 3) Aggressive data augmentation using SAM's mask generation for synthetic positive samples, 4) Implementing a student-teacher framework where nnU-Net is teacher, lightweight SAM adaptation is student, 5) Using DPO (Direct Preference Optimization) to rank generated masks by quality, 6) Implementing test-time augmentation with uncertainty quantification",
    "candidate_answer": "data augmentation",
    "kimi2_score": 15.0,
    "kimi2_feedback": "The candidate provided only 'data augmentation' as their answer, which is a single component of a much more comprehensive strategy needed for this complex problem. The response completely misses the core aspects of leveraging SAM's zero-shot capabilities, knowledge distillation from nnU-Net, handling class imbalance, and the specific techniques mentioned in the expected answer. This is an extremely minimal response that shows no understanding of the technical challenges or the sophisticated approaches required for zero-shot learning with SAM and knowledge distillation."
  },
  {
    "answer_id": 52,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "brain_teaser",
    "question_text": "Given patient EHR data spanning 10 years, you need to predict disease progression where some diseases have <1% prevalence. Your GraphPath model combines GNNs for relational encoding and LSTMs for temporal modeling, but suffers from extreme class imbalance. Design a training strategy that uses your experience with RAREMed for rare disease prediction while incorporating patient subgroup identification.",
    "expected_answer": "Solution should include: 1) Implementing stratified temporal sampling to balance rare disease sequences, 2) Using graph-based oversampling by identifying similar patient subgroups through learned representations, 3) Applying focal loss adapted for temporal sequences, 4) Creating synthetic patient trajectories using GNN-based interpolation between similar patients, 5) Implementing curriculum learning starting with common diseases, gradually introducing rarer ones, 6) Using patient subgroup clusters to share information across similar rare cases",
    "candidate_answer": "ASLDK;FJAS;LDKGJV;ASLJFNV;KLSJDF'OAEIW;NGKJLNLKCNA;SLKEFJ",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided no meaningful response to the technical question. The response appears to be random keyboard input rather than any attempt to address the complex problem of handling extreme class imbalance in disease progression prediction with GraphPath models. This shows a complete lack of understanding of the technical concepts involved (GNNs, LSTMs, rare disease prediction, stratified sampling, focal loss, etc.) and demonstrates no problem-solving capability."
  },
  {
    "answer_id": 59,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "coding",
    "question_text": "Implement a multi-stage email classification system that detects phishing, spam, graymail, and ham. Given a dataset with 8000+ email records, write a Python function that combines LLM-based feature extraction with XGBoost classification. The function should:\n1. Use LLM agents to extract semantic features from email content (subject and body)\n2. Perform additional feature engineering on email metadata (sender, timestamp, etc.)\n3. Train an XGBoost classifier with proper handling of imbalanced classes\n4. Include a two-phase pipeline: first phase for initial classification, second phase for refinement on uncertain cases\n5. Return evaluation metrics including precision, recall, and F1-score for each class",
    "expected_answer": "Complete implementation should include: LLM-based feature extraction using prompts to analyze email content for phishing indicators, feature engineering for metadata, XGBoost with class weights for imbalance, two-stage pipeline with confidence thresholding, and comprehensive evaluation metrics. Should handle the 4-class classification problem with proper evaluation.",
    "candidate_answer": "testtestestsetwest",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided no actual implementation - they simply repeated the test code and added meaningless text ('testtestestsetwest'). This completely fails to address any aspect of the technical question. There is no LLM integration, no feature extraction, no XGBoost classifier, no two-stage pipeline, and no evaluation metrics. The response demonstrates either a complete lack of understanding or an unwillingness to attempt the problem."
  },
  {
    "answer_id": 55,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "signal_processing",
    "question_text": "Design a real-time DSP system for detecting phishing emails using FFT-based spectral analysis of email text features. How would you handle the streaming nature of emails and maintain filter stability while processing 1000+ emails/second?",
    "expected_answer": "Design a sliding window FFT approach with overlap-add method for continuous processing. Use a bank of bandpass filters to extract spectral features from character n-gram sequences. Implement adaptive filtering with exponential moving average for stability. Consider using Goertzel algorithm for efficient computation of specific frequency bins. Buffer management with circular buffers and lock-free queues for thread safety.",
    "candidate_answer": "ggggg",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided no meaningful response to the technical question. The answer 'ggggg' shows a complete lack of understanding or engagement with the problem. This response fails to address any aspect of the DSP system design, FFT-based analysis, real-time processing requirements, or any of the technical challenges mentioned in the question."
  },
  {
    "answer_id": 57,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "signal_processing",
    "question_text": "Implement a Kalman filter-based system for real-time prediction of disease progression from irregularly sampled EHR time series data. The system should handle missing data and varying sampling rates across patients.",
    "expected_answer": "Implement continuous-discrete Kalman filter with state transition matrix varying based on time intervals. Handle missing data with prediction steps only. Use linear interpolation for missing values or skip updates entirely. Track state covariance and handle varying process noise based on elapsed time.",
    "candidate_answer": "asldfhas;lkdfjvvcna;lskdhhgxfn",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided completely nonsensical text instead of any attempt at implementing the Kalman filter. This shows either a complete lack of understanding of the problem or a failure to engage seriously with the question. There is no code, no explanation, and no attempt to address any aspect of the problem including handling irregular sampling, missing data, or disease progression modeling."
  },
  {
    "answer_id": 58,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "signal_processing",
    "question_text": "Design a multi-stage DSP pipeline for medication recommendation that combines LSTM sequence modeling with FFT-based spectral analysis of patient vital signs. How would you architect this for a cloud-based system handling 10M+ patients with sub-100ms latency?",
    "expected_answer": "Architecture: Stage 1 - FFT preprocessing on vital signs (parallel GPU processing), Stage 2 - LSTM temporal modeling with attention mechanism, Stage 3 - GNN for patient similarity graph. Use distributed processing with Spark for FFT computation, vLLM for LSTM inference, and Redis for caching. Implement model parallelism with parameter servers. Use quantization for LSTM weights and FFT pruning for efficiency.",
    "candidate_answer": "dddd",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided no meaningful response to the technical question. The answer 'dddd' is completely irrelevant and shows no understanding of the DSP pipeline architecture, LSTM modeling, FFT analysis, or cloud system design requirements. This response fails to address any aspect of the question including the multi-stage pipeline design, latency requirements, scalability for 10M+ patients, or any technical implementation details."
  },
  {
    "answer_id": 62,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "system_design",
    "question_text": "Design a distributed inference system for a multi-modal medical AI platform that processes brain tumor segmentation (using nnU-Net), disease progression prediction (using Graph Neural Networks on EHR data), and medication recommendation (using Transformer-based models). The system needs to handle 50,000+ daily requests from hospitals worldwide, ensure HIPAA compliance, and support both zero-shot learning for rare diseases and few-shot learning for new hospitals with limited data. How would you architect the system considering the computational demands of each model type?",
    "expected_answer": "Design should include: 1) Microservices architecture with separate services for each model type (nnU-Net, GNN, Transformer), 2) GPU cluster management with Kubernetes for model serving using vLLM and Hugging Face transformers, 3) Data pipeline using Apache Spark for EHR preprocessing and feature engineering, 4) HIPAA-compliant storage with encrypted S3 buckets and VPC endpoints on AWS/GCP, 5) Model registry with versioning and A/B testing framework, 6) Load balancing with geographic distribution, 7) Caching layer for frequently accessed patient data, 8) Privacy-preserving federated learning setup for new hospitals",
    "candidate_answer": "dghdf",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided a completely invalid response with 'dghdf' which shows no understanding of the system design question. This appears to be either a placeholder text, random typing, or a complete failure to engage with the technical challenge. There is no technical content to evaluate regarding distributed systems, medical AI, HIPAA compliance, or any of the specific requirements mentioned in the question."
  },
  {
    "answer_id": 63,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "system_design",
    "question_text": "You need to build a real-time phishing detection system that processes 1M+ emails daily, combining LLM-based analysis (ChatGPT/GPT-4) with XGBoost classification. Design the streaming architecture that can handle the latency requirements (<100ms per email), model drift detection, and automatic retraining. How would you handle the trade-off between LLM inference costs and detection accuracy?",
    "expected_answer": "Architecture should include: 1) Kafka-based streaming pipeline for email ingestion, 2) Two-tier classification - fast XGBoost pre-screening (~20ms) followed by LLM analysis for suspicious emails, 3) Feature store using Redis for real-time feature serving, 4) Model monitoring with Prometheus + Grafana for drift detection, 5) Incremental learning pipeline using DPO/RLOO for continuous model improvement, 6) Cost optimization with batching LLM requests and using smaller distilled models (Knowledge Distillation from ChatGPT to smaller transformer), 7) Canary deployment system for model updates",
    "candidate_answer": "bbbbbb",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided no meaningful response to the system design question. The answer 'bbbbbb' is completely irrelevant and shows no understanding of the technical requirements or any attempt to address the complex architecture needed for a real-time phishing detection system. This response fails to demonstrate any knowledge of streaming architectures, machine learning model serving, latency optimization, cost management, or any of the key components mentioned in the expected answer."
  },
  {
    "answer_id": 64,
    "candidate_id": 2,
    "candidate_name": "Yi Jing",
    "category": "system_design",
    "question_text": "Design a federated learning system for training medical imaging models (SAM, VLM, MedGemma) across multiple hospitals without sharing patient data. The system needs to handle: 1) Non-IID data distribution across hospitals, 2) Differential privacy guarantees, 3) Support for both centralized and decentralized training modes, 4) Zero-shot adaptation to new medical imaging modalities. How would you implement the communication protocol and aggregation strategy?",
    "expected_answer": "System design: 1) Federated learning coordinator using PyTorch Distributed, 2) Differential privacy with noise addition (epsilon=1.0), 3) Gradient compression using quantization for efficient communication, 4) Personalization layer using FedProx or Ditto for non-IID data, 5) Secure aggregation using homomorphic encryption or secure multi-party computation, 6) Zero-shot adaptation using meta-learning (MAML) combined with SAM's promptable segmentation, 7) Hospital-side Docker containers with automatic model updates, 8) Central dashboard for monitoring model performance across sites",
    "candidate_answer": "sasss",
    "kimi2_score": 0.0,
    "kimi2_feedback": "The candidate provided an incomplete and nonsensical response ('sasss') that does not address any aspect of the federated learning system design question. There is no technical content, no system design considerations, and no mention of any required components like differential privacy, non-IID data handling, communication protocols, or aggregation strategies. The response appears to be either a placeholder or an error."
  }
]