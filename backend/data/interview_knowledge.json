{
  "version": "1.0.0",
  "last_updated": "2024-12-08",
  "passing_score": 70,
  "total_points": 100,
  "structure": {
    "brain_teasers": {
      "points": 30,
      "question_count": 3,
      "points_per_question": 10
    },
    "technical": {
      "points": 70,
      "question_count": 7,
      "points_per_question": 10
    }
  },
  "difficulty_levels": {
    "easy": {
      "label": "Junior",
      "years_experience": "0-2",
      "description": "Entry-level questions focusing on fundamentals",
      "score_threshold": 0.6
    },
    "medium": {
      "label": "Mid-Level",
      "years_experience": "2-5",
      "description": "Intermediate questions with moderate complexity",
      "score_threshold": 0.7
    },
    "hard": {
      "label": "Senior",
      "years_experience": "5+",
      "description": "Advanced questions on architecture, optimization, edge cases",
      "score_threshold": 0.75
    }
  },
  "terminology": {
    "SNR": {
      "term": "Signal-to-Noise Ratio",
      "definition": "A measure comparing the level of desired signal to background noise. Higher SNR means cleaner signal. Calculated as SNR = 20*log10(signal_power/noise_power) in dB.",
      "example": "A PPG signal with SNR of 20dB has signal 10x stronger than noise."
    },
    "latency": {
      "term": "Latency",
      "definition": "Time delay between input and output in a system. In real-time systems, low latency is critical.",
      "example": "5ms latency means data takes 5 milliseconds from sensor to processed output."
    },
    "PPG": {
      "term": "Photoplethysmography",
      "definition": "Optical technique to detect blood volume changes in tissue using light. Used in pulse oximeters and wearables.",
      "example": "Your Apple Watch uses PPG to measure heart rate via green LEDs."
    },
    "MARD": {
      "term": "Mean Absolute Relative Difference",
      "definition": "Metric for glucose monitor accuracy. MARD = mean(|measured - reference| / reference) * 100%",
      "example": "FDA requires CGM devices to have MARD < 15% for approval."
    },
    "IEC_60601": {
      "term": "IEC 60601-1",
      "definition": "International standard for medical electrical equipment safety. Covers electrical safety, EMC, and risk management.",
      "example": "All medical devices sold in US/EU must comply with IEC 60601-1."
    },
    "ISO_14971": {
      "term": "ISO 14971",
      "definition": "International standard for medical device risk management. Defines process for identifying and mitigating risks.",
      "example": "FMEA analysis following ISO 14971 identifies potential failure modes."
    },
    "Clarke_Error_Grid": {
      "term": "Clarke Error Grid",
      "definition": "Tool for evaluating clinical accuracy of glucose meters. Divides error space into zones A-E based on clinical risk.",
      "example": "Zone A predictions are clinically accurate; Zone E predictions are dangerous."
    },
    "BLE": {
      "term": "Bluetooth Low Energy",
      "definition": "Low-power wireless protocol for IoT and wearables. Optimized for intermittent data transfer.",
      "example": "BLE 5.0 supports 2 Mbps data rate with ~10m range."
    },
    "HIPAA": {
      "term": "Health Insurance Portability and Accountability Act",
      "definition": "US law requiring protection of patient health information (PHI). Mandates encryption and access controls.",
      "example": "Storing patient glucose data requires HIPAA-compliant encryption."
    },
    "RTOS": {
      "term": "Real-Time Operating System",
      "definition": "OS designed for deterministic timing. Guarantees task execution within defined time constraints.",
      "example": "FreeRTOS is commonly used in medical wearables for predictable timing."
    }
  },
  "brain_teasers": {
    "description": "Logic puzzles testing analytical thinking. Same across all engineer types.",
    "questions": [
      {
        "id": "bt_1",
        "title": "N-Dimensional Cube Analysis",
        "difficulty": [
          "easy",
          "medium",
          "hard"
        ],
        "question_text": "How many edges are there in a 4-dimensional cube (tesseract)?",
        "rubric": {
          "understanding_approach": {
            "points": 4,
            "criteria": [
              {
                "description": "Recognizes pattern from lower dimensions",
                "points": 1
              },
              {
                "description": "Understands dimensional relationships",
                "points": 1
              },
              {
                "description": "Can explain how edges form in higher dimensions",
                "points": 1
              },
              {
                "description": "Shows systematic thinking",
                "points": 1
              }
            ]
          },
          "solution_process": {
            "points": 4,
            "criteria": [
              {
                "description": "Uses correct formula (n·2^(n-1))",
                "points": 2
              },
              {
                "description": "Correctly calculates result (32)",
                "points": 1
              },
              {
                "description": "Can verify through alternative method",
                "points": 1
              }
            ]
          },
          "explanation_quality": {
            "points": 2,
            "criteria": [
              {
                "description": "Clear communication of reasoning",
                "points": 1
              },
              {
                "description": "Ability to visualize/explain in simpler terms",
                "points": 1
              }
            ]
          }
        },
        "sample_strong_answer": "Let me solve this step by step:\n1. First, understand the pattern:\n   - 1D line segment has 1 edge\n   - 2D square has 4 edges\n   - 3D cube has 12 edges\n   - Each dimension follows pattern n·2^(n-1)\n2. For 4D: 4 × 2^(4-1) = 4 × 8 = 32 edges\n3. Verification: A tesseract has 16 vertices, each connecting to 4 others (one per dimension). Total connections = 16×4/2 = 32 edges.",
        "follow_up": "How would this pattern continue for a 5-dimensional hypercube?",
        "follow_up_answer": "For 5D: 5 × 2^(5-1) = 5 × 16 = 80 edges. Can verify: 32 vertices × 5 connections / 2 = 80."
      },
      {
        "id": "bt_2",
        "title": "Six Guards and Six Doors",
        "difficulty": [
          "medium",
          "hard"
        ],
        "question_text": "You're in a room with 6 doors—one leads to freedom, five to disaster. In front of them stand 6 guards: 2 always tell the truth, 2 always lie, and 2 answer randomly. You can ask 9 yes/no questions (no empirical questions). Which questions would you ask? What's the minimum number of questions needed to guarantee finding the correct door?",
        "rubric": {
          "initial_analysis": {
            "points": 3,
            "criteria": [
              {
                "description": "Recognizes impossibility with 9 questions",
                "points": 1
              },
              {
                "description": "Correctly calculates total possibilities (540)",
                "points": 1
              },
              {
                "description": "Uses information theory to justify minimum of 10 questions",
                "points": 1
              }
            ]
          },
          "mathematical_reasoning": {
            "points": 3,
            "criteria": [
              {
                "description": "Explains why 2^9 = 512 is insufficient",
                "points": 1
              },
              {
                "description": "Shows understanding of random guard impact",
                "points": 1
              },
              {
                "description": "Demonstrates information theory principles",
                "points": 1
              }
            ]
          },
          "solution_strategy": {
            "points": 4,
            "criteria": [
              {
                "description": "Provides strategy for identifying guard types",
                "points": 1
              },
              {
                "description": "Explains approach for handling random guards",
                "points": 1
              },
              {
                "description": "Lists actual questions that would work",
                "points": 2
              }
            ]
          }
        },
        "sample_strong_answer": "Let me analyze this:\n1. Total possibilities: (6C2)×(4C2) guard arrangements = 90, times 6 doors = 540 total states\n2. With yes/no questions: 2^n patterns for n questions\n3. Since 2^9 = 512 < 540 but 2^10 = 1024 > 540, we need minimum 10 questions\n4. Strategy: First identify random guards by asking same question twice to pairs, then use truth/liar pairs with double-negation questions.",
        "follow_up": "If you could ask 3-way questions (yes/no/maybe), how would that change the minimum?",
        "follow_up_answer": "With 3 outcomes: 3^n patterns. 3^6 = 729 > 540, so 6 questions would suffice."
      },
      {
        "id": "bt_3",
        "title": "Three Ropes Timing Puzzle",
        "difficulty": [
          "easy",
          "medium"
        ],
        "question_text": "You have three identical ropes that each take exactly 1 hour to burn from end to end. The ropes burn at non-uniform rates (i.e., half the rope might burn in 10 minutes, the other half in 50 minutes). You cannot cut, mark, or manipulate the ropes except by burning them. How can you measure exactly 15 minutes?",
        "rubric": {
          "initial_understanding": {
            "points": 3,
            "criteria": [
              {
                "description": "Recognizes that cutting/marking won't work",
                "points": 1
              },
              {
                "description": "Understands why non-uniform burning rate matters",
                "points": 1
              },
              {
                "description": "Identifies that burning from both ends creates reliable timing",
                "points": 1
              }
            ]
          },
          "solution_strategy": {
            "points": 4,
            "criteria": [
              {
                "description": "Describes correct sequence of steps",
                "points": 2
              },
              {
                "description": "Explains parallel burning concept",
                "points": 1
              },
              {
                "description": "Articulates timing synchronization",
                "points": 1
              }
            ]
          },
          "explanation_quality": {
            "points": 3,
            "criteria": [
              {
                "description": "Clear communication of solution",
                "points": 1
              },
              {
                "description": "Logical step-by-step breakdown",
                "points": 1
              },
              {
                "description": "Explains why solution works despite variable rates",
                "points": 1
              }
            ]
          }
        },
        "sample_strong_answer": "Key insight: Burning a rope from both ends always takes exactly 30 minutes, regardless of non-uniform burning.\n\nSolution:\n1. Light both ends of Rope A and one end of Rope B simultaneously\n2. When Rope A burns out (30 minutes), immediately light the other end of Rope B\n3. Rope B has 30 minutes of burn time left, but burning from both ends takes 15 minutes\n4. When Rope B burns out, exactly 30 + 15 = 45 minutes have elapsed\n\nFor exactly 15 minutes: Light both ends of Rope A and both ends of Rope B. When A burns out (30 min), light both ends of Rope C. When C burns out, 15 more minutes have passed.",
        "follow_up": "How would you measure exactly 45 minutes using the same three ropes?",
        "follow_up_answer": "Light both ends of A and one end of B. When A finishes (30 min), light the other end of B. When B finishes, 45 minutes total."
      },
      {
        "id": "bt_4",
        "title": "The Poisoned Wine Puzzle",
        "difficulty": [
          "medium",
          "hard"
        ],
        "question_text": "A king has 1000 bottles of wine, and exactly one is poisoned. The poison kills in exactly 24 hours. The king has 10 prisoners he can use as testers. He needs to find the poisoned bottle within 24 hours for a celebration. How can he identify the poisoned bottle with certainty using only these 10 prisoners?",
        "rubric": {
          "binary_insight": {
            "points": 4,
            "criteria": [
              {
                "description": "Recognizes binary encoding approach",
                "points": 2
              },
              {
                "description": "Calculates 2^10 = 1024 > 1000",
                "points": 1
              },
              {
                "description": "Understands each prisoner represents a bit",
                "points": 1
              }
            ]
          },
          "solution_method": {
            "points": 4,
            "criteria": [
              {
                "description": "Describes bottle numbering in binary",
                "points": 2
              },
              {
                "description": "Explains prisoner assignment to bit positions",
                "points": 1
              },
              {
                "description": "Shows how death pattern identifies bottle",
                "points": 1
              }
            ]
          },
          "explanation_clarity": {
            "points": 2,
            "criteria": [
              {
                "description": "Provides concrete example",
                "points": 1
              },
              {
                "description": "Explains why this guarantees success",
                "points": 1
              }
            ]
          }
        },
        "sample_strong_answer": "Use binary encoding:\n1. Number bottles 1-1000 in binary (10 bits covers up to 1024)\n2. Assign each prisoner to a bit position (prisoner 1 = bit 0, prisoner 2 = bit 1, etc.)\n3. Each prisoner drinks from all bottles where their bit is 1\n4. After 24 hours, the death pattern gives the binary number of poisoned bottle\n\nExample: If prisoners 1, 4, and 7 die, the poisoned bottle is 2^0 + 2^3 + 2^6 = 1 + 8 + 64 = bottle 73.\n\nWith 10 prisoners: 2^10 = 1024 possible patterns, sufficient for 1000 bottles.",
        "follow_up": "What if you only had 8 prisoners but needed to check 500 bottles?",
        "follow_up_answer": "2^8 = 256, insufficient for 500. You'd need 9 prisoners (2^9 = 512). Or use a 2-round strategy with 24-hour delays."
      },
      {
        "id": "bt_5",
        "title": "Light Bulb and Switches",
        "difficulty": [
          "easy"
        ],
        "question_text": "You're outside a room with 3 light switches. Inside the room is a single light bulb. You can only enter the room once. How do you determine which switch controls the bulb?",
        "rubric": {
          "insight": {
            "points": 4,
            "criteria": [
              {
                "description": "Recognizes need for non-visual information",
                "points": 2
              },
              {
                "description": "Identifies heat as usable property",
                "points": 2
              }
            ]
          },
          "solution": {
            "points": 4,
            "criteria": [
              {
                "description": "Correct procedure with timing",
                "points": 2
              },
              {
                "description": "Explains all three outcomes",
                "points": 2
              }
            ]
          },
          "communication": {
            "points": 2,
            "criteria": [
              {
                "description": "Clear step-by-step explanation",
                "points": 1
              },
              {
                "description": "Logical reasoning",
                "points": 1
              }
            ]
          }
        },
        "sample_strong_answer": "Use heat as additional information:\n1. Turn switch A ON for 10 minutes\n2. Turn switch A OFF, turn switch B ON\n3. Enter the room immediately\n\nDetermine:\n- If bulb is ON → Switch B\n- If bulb is OFF and WARM → Switch A\n- If bulb is OFF and COLD → Switch C\n\nThis works because incandescent bulbs retain heat after being turned off.",
        "follow_up": "What if it's an LED bulb that doesn't produce heat?",
        "follow_up_answer": "LED approach: Use a helper or timer. Or turn A on/off rapidly to make it flicker, B steady on, C off. Enter quickly to observe flickering vs steady vs off."
      },
      {
        "id": "bt_6",
        "title": "Two Eggs, 100 Floors",
        "difficulty": [
          "medium",
          "hard"
        ],
        "question_text": "You have 2 identical eggs and a 100-floor building. You need to find the highest floor from which an egg can be dropped without breaking. What's the minimum number of drops needed in the worst case to guarantee finding this floor?",
        "rubric": {
          "approach": {
            "points": 4,
            "criteria": [
              {
                "description": "Recognizes binary search won't work with 2 eggs",
                "points": 1
              },
              {
                "description": "Understands need for decreasing intervals",
                "points": 2
              },
              {
                "description": "Sets up optimization equation",
                "points": 1
              }
            ]
          },
          "solution": {
            "points": 4,
            "criteria": [
              {
                "description": "Derives n(n+1)/2 ≥ 100 formula",
                "points": 2
              },
              {
                "description": "Calculates n = 14 correctly",
                "points": 1
              },
              {
                "description": "Describes drop sequence (14, 27, 39...)",
                "points": 1
              }
            ]
          },
          "explanation": {
            "points": 2,
            "criteria": [
              {
                "description": "Explains why intervals decrease",
                "points": 1
              },
              {
                "description": "Verifies worst case is 14",
                "points": 1
              }
            ]
          }
        },
        "sample_strong_answer": "Key insight: If first egg breaks, we must check linearly with second egg. Use decreasing intervals.\n\nLet n = max drops. First drop at floor n. If breaks, check floors 1 to n-1 (n-1 more drops, total n).\nIf survives, next drop at n + (n-1). Pattern continues.\n\nNeed: n + (n-1) + (n-2) + ... + 1 ≥ 100\nn(n+1)/2 ≥ 100\nn ≥ 13.7, so n = 14\n\nDrop sequence: 14, 27, 39, 50, 60, 69, 77, 84, 90, 95, 99, 100\nWorst case: 14 drops guaranteed.",
        "follow_up": "What if you had 3 eggs?",
        "follow_up_answer": "With 3 eggs, use similar logic but can afford binary-like search until down to 2 eggs. Answer is 9 drops for 100 floors."
      },
      {
        "id": "bt_7",
        "title": "Probability of Consecutive Heads",
        "category": "math_calculation",
        "difficulty": ["easy", "medium"],
        "question_text": "You flip a fair coin repeatedly until you get two consecutive heads (HH). What is the expected number of flips needed?",
        "rubric": {
          "problem_setup": {
            "points": 3,
            "criteria": [
              {"description": "Identifies this as a Markov chain / state-based problem", "points": 1},
              {"description": "Correctly defines states (Start, H, HH)", "points": 1},
              {"description": "Sets up recurrence relations", "points": 1}
            ]
          },
          "calculation": {
            "points": 4,
            "criteria": [
              {"description": "Writes correct equations: E[Start] = 1 + 0.5*E[H] + 0.5*E[Start]", "points": 2},
              {"description": "Correctly solves: E[H] = 1 + 0.5*0 + 0.5*E[Start]", "points": 1},
              {"description": "Gets correct answer: 6 flips", "points": 1}
            ]
          },
          "explanation": {
            "points": 3,
            "criteria": [
              {"description": "Explains intuition behind state transitions", "points": 1},
              {"description": "Can verify answer or provide alternative approach", "points": 1},
              {"description": "Clear mathematical reasoning", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "This is a Markov chain problem with states:\n- S: Start (or just got T)\n- H: Just got one H\n- HH: Done (absorbing state)\n\nLet E[S] = expected flips from S, E[H] = expected flips from H.\n\nFrom S: flip once, get H with prob 0.5 (go to H), get T with prob 0.5 (stay at S)\nE[S] = 1 + 0.5*E[H] + 0.5*E[S]\n\nFrom H: flip once, get H with prob 0.5 (done!), get T with prob 0.5 (back to S)\nE[H] = 1 + 0.5*0 + 0.5*E[S]\n\nSolving: E[H] = 1 + 0.5*E[S]\nSubstitute: E[S] = 1 + 0.5*(1 + 0.5*E[S]) + 0.5*E[S]\nE[S] = 1 + 0.5 + 0.25*E[S] + 0.5*E[S]\n0.25*E[S] = 1.5\nE[S] = 6\n\nExpected number of flips is 6.",
        "follow_up": "What if we wanted three consecutive heads (HHH)?",
        "follow_up_answer": "Similar approach with 3 states. Answer is 14 flips. General formula for n consecutive heads: 2^(n+1) - 2."
      },
      {
        "id": "bt_8",
        "title": "The Monty Hall Variant",
        "category": "math_calculation",
        "difficulty": ["medium"],
        "question_text": "In a variant of the Monty Hall problem, there are 100 doors. Behind one door is a car, behind the rest are goats. You pick door #1. The host, who knows what's behind each door, opens 98 doors showing goats, leaving only your door and door #57 closed. Should you switch to door #57? What is the probability of winning if you switch?",
        "rubric": {
          "understanding": {
            "points": 3,
            "criteria": [
              {"description": "Recognizes this amplifies the Monty Hall effect", "points": 1},
              {"description": "Understands host's knowledge is key", "points": 1},
              {"description": "Correctly identifies that switching is better", "points": 1}
            ]
          },
          "calculation": {
            "points": 4,
            "criteria": [
              {"description": "Calculates initial probability: 1/100 for your door", "points": 1},
              {"description": "Recognizes remaining probability concentrates on #57", "points": 2},
              {"description": "Gets correct answer: 99/100 = 99%", "points": 1}
            ]
          },
          "explanation": {
            "points": 3,
            "criteria": [
              {"description": "Explains why probability doesn't redistribute equally", "points": 1},
              {"description": "Uses conditional probability reasoning", "points": 1},
              {"description": "Can extend reasoning to general case", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "Yes, definitely switch!\n\nInitial probability your door has car: 1/100\nInitial probability car is behind one of the other 99 doors: 99/100\n\nThe host opening 98 goat doors doesn't change these probabilities—it just concentrates the 99/100 probability onto the one remaining door (#57).\n\nP(car behind #1) = 1/100 = 1%\nP(car behind #57) = 99/100 = 99%\n\nSwitching gives you 99% chance of winning vs. 1% if you stay.\n\nThis is the same logic as the 3-door Monty Hall (33% vs 67%), but with 100 doors the effect is much more dramatic. The host's action of revealing goats transfers probability from those doors to the remaining unchosen door.",
        "follow_up": "What if the host opened doors randomly (might reveal the car)?",
        "follow_up_answer": "If host opens randomly and happens to show only goats, switching doesn't help—both remaining doors have 50% probability. The host's knowledge is crucial to the Monty Hall effect."
      },
      {
        "id": "bt_9",
        "title": "Birthday Problem Variant",
        "category": "math_calculation",
        "difficulty": ["easy", "medium"],
        "question_text": "In a room of 30 people, what is the probability that at least two people share the same birthday? (Assume 365 days, ignore leap years, and uniform distribution of birthdays.)",
        "rubric": {
          "approach": {
            "points": 3,
            "criteria": [
              {"description": "Uses complement: P(at least 2 same) = 1 - P(all different)", "points": 2},
              {"description": "Understands why direct calculation is harder", "points": 1}
            ]
          },
          "calculation": {
            "points": 4,
            "criteria": [
              {"description": "Sets up product: (365/365) × (364/365) × ... × (336/365)", "points": 2},
              {"description": "Gets approximately 0.706 or 70.6%", "points": 2}
            ]
          },
          "intuition": {
            "points": 3,
            "criteria": [
              {"description": "Explains why result is counterintuitively high", "points": 1},
              {"description": "Notes there are 30C2 = 435 pairs to compare", "points": 1},
              {"description": "Can discuss threshold (23 people for 50%)", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "Use the complement: P(at least 2 share) = 1 - P(all different)\n\nP(all different) = (365/365) × (364/365) × (363/365) × ... × (336/365)\n= 365! / (335! × 365^30)\n\nCalculating:\n= (365 × 364 × 363 × ... × 336) / 365^30\n≈ 0.294\n\nSo P(at least 2 share) ≈ 1 - 0.294 = 0.706 or about 70.6%\n\nThis seems high because with 30 people, there are 30C2 = 435 pairs, giving many opportunities for a match. The 50% threshold is just 23 people.",
        "follow_up": "How many people do you need for a 99% probability of a shared birthday?",
        "follow_up_answer": "You need 57 people for 99% probability. The formula is approximately n ≈ √(2 × 365 × ln(1/(1-p))) for probability p."
      },
      {
        "id": "bt_10",
        "title": "Expected Value of Maximum",
        "category": "math_calculation",
        "difficulty": ["medium", "hard"],
        "question_text": "You roll a fair 6-sided die 3 times. What is the expected value of the maximum of the three rolls?",
        "rubric": {
          "approach": {
            "points": 3,
            "criteria": [
              {"description": "Uses P(max ≤ k) or P(max = k) approach", "points": 2},
              {"description": "Recognizes this requires inclusion-exclusion or CDF method", "points": 1}
            ]
          },
          "calculation": {
            "points": 4,
            "criteria": [
              {"description": "Correctly computes P(max = k) = (k/6)^3 - ((k-1)/6)^3", "points": 2},
              {"description": "Sets up E[max] = Σ k × P(max = k)", "points": 1},
              {"description": "Gets correct answer: 119/24 ≈ 4.958", "points": 1}
            ]
          },
          "verification": {
            "points": 3,
            "criteria": [
              {"description": "Shows work clearly", "points": 1},
              {"description": "Can verify reasonableness (> 3.5, < 6)", "points": 1},
              {"description": "Alternative: uses E[max] = Σ P(max ≥ k)", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "P(max ≤ k) = (k/6)^3 (all three rolls ≤ k)\nP(max = k) = P(max ≤ k) - P(max ≤ k-1) = (k/6)^3 - ((k-1)/6)^3\n\nCalculate for each k:\nP(max=1) = (1/6)^3 - 0 = 1/216\nP(max=2) = (2/6)^3 - (1/6)^3 = 8/216 - 1/216 = 7/216\nP(max=3) = 27/216 - 8/216 = 19/216\nP(max=4) = 64/216 - 27/216 = 37/216\nP(max=5) = 125/216 - 64/216 = 61/216\nP(max=6) = 216/216 - 125/216 = 91/216\n\nE[max] = 1×(1/216) + 2×(7/216) + 3×(19/216) + 4×(37/216) + 5×(61/216) + 6×(91/216)\n= (1 + 14 + 57 + 148 + 305 + 546)/216 = 1071/216 = 119/24 ≈ 4.958\n\nThis is higher than the single-roll expected value of 3.5, which makes sense since we're taking the maximum.",
        "follow_up": "What about the expected minimum of three rolls?",
        "follow_up_answer": "By symmetry: E[min] = 7 - E[max] = 7 - 119/24 = 49/24 ≈ 2.042. Or compute directly using P(min ≥ k)."
      },
      {
        "id": "bt_11",
        "title": "Ant on a Triangle",
        "category": "math_calculation",
        "difficulty": ["easy", "medium"],
        "question_text": "An ant starts at one vertex of an equilateral triangle. Each second, it randomly moves to one of the two adjacent vertices with equal probability. What is the expected number of seconds for the ant to return to its starting vertex?",
        "rubric": {
          "setup": {
            "points": 3,
            "criteria": [
              {"description": "Identifies states: Start, Other1, Other2", "points": 1},
              {"description": "Recognizes symmetry: Other1 and Other2 are equivalent", "points": 1},
              {"description": "Sets up recurrence relations", "points": 1}
            ]
          },
          "calculation": {
            "points": 4,
            "criteria": [
              {"description": "Let E = expected return time from Start", "points": 1},
              {"description": "E = 1 + 0.5*E_other + 0.5*E_other where E_other is from other vertex", "points": 1},
              {"description": "E_other = 1 + 0.5*0 + 0.5*E (return or go to third vertex)", "points": 1},
              {"description": "Gets correct answer: 3 seconds", "points": 1}
            ]
          },
          "explanation": {
            "points": 3,
            "criteria": [
              {"description": "Clear state transition reasoning", "points": 1},
              {"description": "Explains symmetry simplification", "points": 1},
              {"description": "Can extend to other polygons", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "By symmetry, let E = expected return time from start, and E' = expected time to reach start from either other vertex.\n\nFrom Start: move to either adjacent vertex (prob 1 each)\nE = 1 + E' (must move, then return from other vertex)\n\nFrom Other vertex: 0.5 chance to return to Start, 0.5 chance to go to third vertex\nE' = 1 + 0.5×0 + 0.5×E'\nE' = 1 + 0.5×E'\n0.5×E' = 1\nE' = 2\n\nTherefore: E = 1 + E' = 1 + 2 = 3\n\nExpected return time is 3 seconds.\n\nAlternatively: By theory of random walks on graphs, expected return time equals n (number of vertices) for a symmetric random walk. Here n=3, confirming our answer.",
        "follow_up": "What if it's a square instead of a triangle?",
        "follow_up_answer": "For a square (4 vertices), expected return time is 4 seconds. For any regular n-gon with movement to adjacent vertices only, expected return time is n."
      },
      {
        "id": "bt_12",
        "title": "Coupon Collector Problem",
        "category": "math_calculation",
        "difficulty": ["medium", "hard"],
        "question_text": "A cereal box contains one of 5 different collectible cards, each equally likely. On average, how many boxes do you need to buy to collect all 5 different cards?",
        "rubric": {
          "understanding": {
            "points": 3,
            "criteria": [
              {"description": "Recognizes this as coupon collector problem", "points": 1},
              {"description": "Understands geometric distribution for each new card", "points": 1},
              {"description": "Breaks into stages: 1st card, 2nd unique, etc.", "points": 1}
            ]
          },
          "calculation": {
            "points": 4,
            "criteria": [
              {"description": "E[total] = E[1st] + E[2nd] + ... + E[5th]", "points": 1},
              {"description": "E[k-th new card] = 5/(5-k+1)", "points": 2},
              {"description": "Gets correct answer: 5(1 + 1/2 + 1/3 + 1/4 + 1/5) = 137/12 ≈ 11.42", "points": 1}
            ]
          },
          "generalization": {
            "points": 3,
            "criteria": [
              {"description": "Can express general formula: n × H_n", "points": 1},
              {"description": "Explains harmonic series connection", "points": 1},
              {"description": "Discusses variance or confidence bounds", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "Break into stages based on how many unique cards we have:\n\nStage 1: Need 1st unique card → P=5/5=1, E[boxes]=1\nStage 2: Need 2nd unique card → P=4/5, E[boxes]=5/4\nStage 3: Need 3rd unique card → P=3/5, E[boxes]=5/3\nStage 4: Need 4th unique card → P=2/5, E[boxes]=5/2\nStage 5: Need 5th unique card → P=1/5, E[boxes]=5/1\n\nE[total] = 1 + 5/4 + 5/3 + 5/2 + 5/1\n= 5(1/5 + 1/4 + 1/3 + 1/2 + 1)\n= 5(1 + 1/2 + 1/3 + 1/4 + 1/5)\n= 5 × H_5\n= 5 × (60 + 30 + 20 + 15 + 12)/60\n= 5 × 137/60\n= 137/12 ≈ 11.42 boxes\n\nGeneral formula: For n cards, E = n × H_n ≈ n × (ln(n) + γ) where γ ≈ 0.5772 is Euler's constant.",
        "follow_up": "What's the probability of collecting all 5 cards in exactly 10 boxes?",
        "follow_up_answer": "Use inclusion-exclusion: P(all 5 in n boxes) = Σ(-1)^k × C(5,k) × ((5-k)/5)^n. For n=10: ≈ 0.153 or about 15.3%."
      }
    ]
  },
  "engineer_tracks": {
    "ml_engineer": {
      "track_id": "ml_engineer",
      "title": "Machine Learning Engineer",
      "description": "ML Engineer assessment focusing on signal processing ML, time-series prediction, and edge deployment",
      "keywords": [
        "machine learning",
        "deep learning",
        "neural networks",
        "pytorch",
        "tensorflow",
        "sklearn",
        "pandas",
        "numpy",
        "signal processing",
        "time series"
      ],
      "technical_questions": [
        {
          "id": "ml_1",
          "title": "Data Harmonization",
          "category": "data_engineering",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You have five PPG datasets with different sampling rates (25 Hz to 1000 Hz), timestamp formats, and glucose measurement intervals (5-min to 15-min). Describe your approach to harmonizing these datasets for training a glucose prediction model. What target sampling rate would you choose and why?",
          "rubric": {
            "resampling_strategy": {
              "points": 3,
              "criteria": [
                {
                  "description": "Justifies target frequency based on signal bandwidth",
                  "points": 1
                },
                {
                  "description": "Discusses anti-aliasing before downsampling",
                  "points": 1
                },
                {
                  "description": "Addresses interpolation method selection",
                  "points": 1
                }
              ]
            },
            "synchronization": {
              "points": 3,
              "criteria": [
                {
                  "description": "Handles timezone and timestamp format differences",
                  "points": 1
                },
                {
                  "description": "Addresses label alignment (continuous PPG to discrete glucose)",
                  "points": 1
                },
                {
                  "description": "Discusses handling missing data",
                  "points": 1
                }
              ]
            },
            "schema_design": {
              "points": 2,
              "criteria": [
                {
                  "description": "Creates unified schema preserving metadata",
                  "points": 1
                },
                {
                  "description": "Handles dataset-specific features",
                  "points": 1
                }
              ]
            },
            "practical_considerations": {
              "points": 2,
              "criteria": [
                {
                  "description": "Considers memory efficiency",
                  "points": 1
                },
                {
                  "description": "Discusses validation of harmonized data",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "For harmonizing multi-source PPG data:\n\n1. Target Sampling Rate: 25-50 Hz. PPG heart rate content is 0.5-4 Hz, so by Nyquist we need >8 Hz minimum. 25 Hz preserves waveform morphology while being memory-efficient. Higher rates (100+ Hz) offer no benefit and 40x more storage.\n\n2. Resampling Pipeline:\n   - High→Low: Apply anti-aliasing lowpass filter (cutoff = target_fs/2), then decimate\n   - Low→High: Use cubic spline interpolation (preserves waveform shape better than linear)\n\n3. Timestamp Synchronization:\n   - Convert all to UTC epoch milliseconds\n   - Handle timezone metadata separately\n   - For glucose labels: use window-based alignment (e.g., 5-min window around each glucose reading)\n\n4. Unified Schema:\n```python\n{\n  'subject_id': str,\n  'dataset': str,\n  'timestamp_utc': int64,\n  'ppg_ir': float32,\n  'ppg_red': float32,\n  'ppg_green': float32,\n  'glucose_mg_dl': float32,  # NaN if no reading\n  'glucose_timestamp': int64,\n  'quality_flag': int8\n}\n```\n\n5. Validation: Check spectral content before/after resampling, verify glucose alignment visually on subset.",
          "follow_up": "How would you handle the case where glucose measurements are only available every 15 minutes but you want to train a model that predicts every 5 minutes?",
          "follow_up_answer": "Options: 1) Interpolate glucose labels (risky - assumes linear change), 2) Train on 15-min windows only, predict at 5-min (temporal extrapolation), 3) Use physiological constraints (glucose changes max 3 mg/dL/min) to bound interpolated values, 4) Multi-task learning predicting both absolute and delta glucose.",
          "hints": [
            "Consider Nyquist theorem for minimum rate",
            "Think about how to align discrete glucose with continuous PPG"
          ]
        },
        {
          "id": "ml_2",
          "title": "Signal Quality Assessment",
          "category": "signal_processing",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "Design a signal quality index (SQI) for PPG signals that can be computed in real-time on an embedded device. The SQI should identify segments corrupted by motion artifacts. What features would you use and how would you combine them?",
          "rubric": {
            "feature_selection": {
              "points": 4,
              "criteria": [
                {
                  "description": "Identifies relevant time-domain features (SNR, kurtosis)",
                  "points": 1
                },
                {
                  "description": "Includes frequency-domain features",
                  "points": 1
                },
                {
                  "description": "Uses accelerometer correlation",
                  "points": 1
                },
                {
                  "description": "Considers template matching",
                  "points": 1
                }
              ]
            },
            "combination_method": {
              "points": 3,
              "criteria": [
                {
                  "description": "Proposes weighted combination or ML approach",
                  "points": 1
                },
                {
                  "description": "Discusses threshold calibration",
                  "points": 1
                },
                {
                  "description": "Handles multi-class quality levels",
                  "points": 1
                }
              ]
            },
            "embedded_constraints": {
              "points": 3,
              "criteria": [
                {
                  "description": "Considers computational complexity",
                  "points": 1
                },
                {
                  "description": "Uses fixed-point or efficient algorithms",
                  "points": 1
                },
                {
                  "description": "Discusses memory requirements",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "Real-time PPG Signal Quality Index:\n\n**Features (computed on 5-second windows):**\n1. **Perfusion Index**: AC/DC ratio - low PI indicates poor contact\n2. **Spectral Purity**: Power in HR band (0.5-4Hz) / total power\n3. **Kurtosis**: Clean PPG has consistent peak heights (kurtosis ~3)\n4. **Zero-Crossing Rate**: Motion causes irregular crossings\n5. **Accelerometer Correlation**: High correlation = motion artifact\n\n**Combination (embedded-friendly):**\n```\nSQI = w1*(PI > PI_thresh) + \n      w2*(spectral_purity > 0.7) + \n      w3*(2.5 < kurtosis < 4) + \n      w4*(accel_corr < 0.3)\n```\nWeights learned via logistic regression on labeled data.\n\n**Embedded Implementation:**\n- Use fixed-point Q15 arithmetic\n- FFT: 256-point (5s @ 50Hz), pre-computed twiddle factors\n- Running statistics with Welford's algorithm (O(1) per sample)\n- Total: ~2KB RAM, ~5ms per 5-second window on Cortex-M4\n\n**Output: 3 levels**\n- SQI > 0.8: High quality\n- 0.5 < SQI < 0.8: Moderate (use with caution)\n- SQI < 0.5: Reject segment",
          "follow_up": "How would you collect ground truth labels for training your SQI classifier?",
          "follow_up_answer": "1) Expert annotation of segments as clean/noisy, 2) Synthetic corruption (add motion data to clean signals), 3) Use clinical-grade device as reference - segments where wearable differs significantly are noisy, 4) Active learning: have model flag uncertain segments for human review.",
          "hints": [
            "Consider both statistical and spectral features",
            "Accelerometer data can indicate motion"
          ]
        },
        {
          "id": "ml_3",
          "title": "Feature Engineering for Glucose Prediction",
          "category": "feature_engineering",
          "difficulty": [
            "hard"
          ],
          "question_text": "Design a feature extraction pipeline for predicting blood glucose from PPG signals. Describe features across time, frequency, and nonlinear domains. How would you handle the weak signal-to-glucose relationship?",
          "rubric": {
            "time_domain": {
              "points": 2,
              "criteria": [
                {
                  "description": "Statistical moments (mean, variance, skewness)",
                  "points": 1
                },
                {
                  "description": "Peak/morphological features",
                  "points": 1
                }
              ]
            },
            "frequency_domain": {
              "points": 2,
              "criteria": [
                {
                  "description": "PSD characteristics",
                  "points": 1
                },
                {
                  "description": "Spectral entropy/complexity",
                  "points": 1
                }
              ]
            },
            "nonlinear_features": {
              "points": 2,
              "criteria": [
                {
                  "description": "Entropy measures (sample, approximate)",
                  "points": 1
                },
                {
                  "description": "DFA or recurrence metrics",
                  "points": 1
                }
              ]
            },
            "weak_signal_handling": {
              "points": 4,
              "criteria": [
                {
                  "description": "Multi-wavelength ratios",
                  "points": 1
                },
                {
                  "description": "Temporal context/history",
                  "points": 1
                },
                {
                  "description": "Feature selection/importance methods",
                  "points": 1
                },
                {
                  "description": "Domain knowledge integration",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "Comprehensive PPG feature extraction for glucose:\n\n**Time Domain (per 5-min window):**\n- Statistical: mean, std, skewness, kurtosis of AC component\n- Peaks: amplitude, rise time, fall time, peak-to-peak interval variability\n- Morphological: systolic/diastolic ratio, dicrotic notch position\n\n**Frequency Domain:**\n- PSD in bands: VLF (0-0.04Hz), LF (0.04-0.15Hz), HF (0.15-0.4Hz), HR (0.5-4Hz)\n- Spectral entropy, dominant frequency, harmonic ratios\n- LF/HF ratio (autonomic balance indicator)\n\n**Nonlinear:**\n- Sample entropy (m=2, r=0.2*std)\n- Detrended Fluctuation Analysis (α1, α2 scaling exponents)\n- Poincaré plot features (SD1, SD2)\n\n**Handling Weak Signal-Glucose Relationship:**\n\n1. **Multi-wavelength ratios**: R = (AC_red/DC_red)/(AC_ir/DC_ir) - used for SpO2 but sensitive to blood composition\n\n2. **Temporal context**: Include features from past 30-60 minutes (glucose is slow-changing). Use delta features: feature(t) - feature(t-15min)\n\n3. **Activity contextualization**: Segment by activity state (rest, walking, exercise). Train separate models or include activity as feature\n\n4. **Feature selection**: Use SHAP values or mutual information to identify most predictive features. Expect ~10-20 features from hundreds\n\n5. **Domain constraints**: Glucose changes max 3-4 mg/dL/min. Use as regularization or post-processing\n\n6. **Multi-modal fusion**: Combine PPG with temperature, HRV, time-of-day, recent meal indicators",
          "follow_up": "If you could only use 5 features due to embedded constraints, which would you choose?",
          "follow_up_answer": "1) Multi-wavelength ratio R, 2) HRV (RMSSD), 3) Perfusion index, 4) Spectral entropy, 5) Time since last meal (if available). These capture blood composition, autonomic state, and contextual factors with minimal computation.",
          "hints": [
            "Consider what physiological changes glucose causes",
            "Think about temporal patterns"
          ]
        },
        {
          "id": "ml_4",
          "title": "Model Selection and Validation",
          "category": "modeling",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You're building a glucose prediction model that will be deployed on both cloud (for research) and edge (for real-time wearable use). What model architectures would you consider for each? How would you validate to ensure the model generalizes across subjects?",
          "rubric": {
            "cloud_model": {
              "points": 3,
              "criteria": [
                {
                  "description": "Considers appropriate architectures (LSTM, Transformer, etc.)",
                  "points": 1
                },
                {
                  "description": "Discusses ensemble or uncertainty quantification",
                  "points": 1
                },
                {
                  "description": "Addresses training strategy",
                  "points": 1
                }
              ]
            },
            "edge_model": {
              "points": 3,
              "criteria": [
                {
                  "description": "Considers model compression/quantization",
                  "points": 1
                },
                {
                  "description": "Proposes efficient architecture",
                  "points": 1
                },
                {
                  "description": "Discusses latency/memory constraints",
                  "points": 1
                }
              ]
            },
            "validation_strategy": {
              "points": 4,
              "criteria": [
                {
                  "description": "Uses leave-one-subject-out or grouped CV",
                  "points": 2
                },
                {
                  "description": "Addresses distribution shift across datasets",
                  "points": 1
                },
                {
                  "description": "Uses appropriate metrics (MARD, Clarke grid)",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Cloud Model Architecture:**\n- Primary: Temporal Fusion Transformer (TFT) for interpretable multi-horizon prediction\n- Alternative: LSTM with attention for sequence modeling\n- Ensemble: Combine with gradient boosting (XGBoost) on engineered features for robustness\n- Include uncertainty: MC Dropout or ensemble variance for confidence intervals\n\n**Edge Model (Cortex-M4, 256KB RAM, 50ms latency):**\n- Architecture: 1D CNN (3 conv layers) → small LSTM (32 units) → Dense\n- Total params: <50K (fits in 200KB with quantization)\n- Quantization: INT8 with TensorFlow Lite Micro\n- Latency: ~20ms inference on M4 @ 100MHz\n- Alternative: Use cloud model for training, distill to edge model\n\n**Validation Strategy:**\n\n1. **Subject-level splits**: Leave-One-Subject-Out (LOSO) or Leave-One-Dataset-Out (LODO) cross-validation. NEVER split time series randomly!\n\n2. **Stratification**: Ensure each fold has balanced:\n   - Glucose ranges (hypo/normal/hyper)\n   - Skin tone distribution\n   - Activity levels\n\n3. **Metrics**:\n   - MARD (Mean Absolute Relative Difference) - primary\n   - Clarke Error Grid zones (target: >95% in A+B)\n   - RMSE, MAE for comparison\n   - Time-in-range correlation\n\n4. **Domain shift testing**:\n   - Train on 4 datasets, test on held-out dataset\n   - Test with different devices, skin tones\n   - Monitor calibration drift over time",
          "follow_up": "How would you handle the significant class imbalance where most glucose readings are in the normal range (70-140 mg/dL)?",
          "follow_up_answer": "1) Weighted loss function (higher weight for hypo/hyper), 2) Focal loss for hard examples, 3) Oversampling rare glucose ranges (SMOTE-like for time series), 4) Clinical risk weighting (missing hypoglycemia is worse than missing hyperglycemia), 5) Evaluate with stratified metrics: MARD-by-range, detection rate for hypo events.",
          "hints": [
            "Consider the difference between research and production requirements",
            "Think about subject variability"
          ]
        },
        {
          "id": "ml_5",
          "title": "Motion Artifact Handling with ML",
          "category": "signal_processing",
          "difficulty": [
            "medium"
          ],
          "question_text": "Design a machine learning approach to detect and compensate for motion artifacts in PPG signals. You have synchronized PPG (100 Hz) and 3-axis accelerometer data (50 Hz). What's your approach?",
          "rubric": {
            "detection": {
              "points": 3,
              "criteria": [
                {
                  "description": "Uses appropriate detection model",
                  "points": 1
                },
                {
                  "description": "Handles multi-scale artifacts",
                  "points": 1
                },
                {
                  "description": "Real-time capable",
                  "points": 1
                }
              ]
            },
            "compensation": {
              "points": 4,
              "criteria": [
                {
                  "description": "Adaptive filtering approach",
                  "points": 1
                },
                {
                  "description": "Or ML-based denoising (autoencoder, U-Net)",
                  "points": 1
                },
                {
                  "description": "Preserves physiological content",
                  "points": 1
                },
                {
                  "description": "Handles severe corruption gracefully",
                  "points": 1
                }
              ]
            },
            "training_data": {
              "points": 3,
              "criteria": [
                {
                  "description": "Discusses ground truth collection",
                  "points": 1
                },
                {
                  "description": "Augmentation strategies",
                  "points": 1
                },
                {
                  "description": "Validation methodology",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "Motion Artifact Detection & Compensation:\n\n**Data Preparation:**\n- Upsample accelerometer to 100Hz (cubic interpolation)\n- Compute accel magnitude: sqrt(ax² + ay² + az²)\n- Windows: 2-second overlapping (50% overlap)\n\n**Detection Model:**\n- 1D CNN binary classifier (clean vs. artifact)\n- Input: [PPG, accel_x, accel_y, accel_z, accel_mag] - 5 channels\n- Architecture: Conv1D(32) → Conv1D(64) → GlobalAvgPool → Dense(1, sigmoid)\n- Output: probability of motion artifact per window\n- Real-time: ~2ms inference per window\n\n**Compensation Approaches:**\n\n*Option 1: Adaptive LMS (lightweight)*\n```python\nclass AdaptiveLMS:\n    def __init__(self, order=32, mu=0.01):\n        self.w = np.zeros(order)\n        self.buffer = np.zeros(order)\n    \n    def process(self, ppg, accel):\n        self.buffer = np.roll(self.buffer, 1)\n        self.buffer[0] = accel\n        estimate = np.dot(self.w, self.buffer)\n        error = ppg - estimate  # cleaned signal\n        self.w += mu * error * self.buffer\n        return error\n```\n\n*Option 2: U-Net Denoiser (more powerful)*\n- Input: corrupted PPG segment (256 samples)\n- Output: clean PPG estimate\n- Train on pairs: (clean + synthetic_motion, clean)\n- Loss: MSE + spectral loss (preserve HR frequency)\n\n**Graceful Degradation:**\n- If artifact probability > 0.9 for >5 seconds: mark as unreliable, don't estimate glucose\n- Use multi-wavelength voting: if only one channel corrupted, use others\n\n**Training Data:**\n- Collect clean PPG from stationary subjects\n- Synthetically add real motion data (from walking/exercise)\n- Validate on held-out subjects with controlled motion protocols",
          "follow_up": "What if the accelerometer itself is noisy or has its own motion artifacts?",
          "follow_up_answer": "1) Multi-sensor fusion: use multiple accelerometers if available, 2) Cross-validate with PPG spectral changes (motion should cause broadband noise), 3) Sanity checks: accel magnitude should be ~1g at rest, 4) Low-pass filter accelerometer (motion is typically <10Hz), 5) Use gyroscope as independent motion reference.",
          "hints": [
            "Consider both detection and compensation",
            "How do you get training labels?"
          ]
        },
        {
          "id": "ml_6",
          "title": "Edge Deployment Optimization",
          "category": "deployment",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You need to deploy a glucose prediction model on an ARM Cortex-M4 microcontroller (256KB RAM, 1MB Flash, 100MHz). The model must run inference in <50ms and consume minimal power. How would you optimize your model and inference pipeline?",
          "rubric": {
            "model_optimization": {
              "points": 4,
              "criteria": [
                {
                  "description": "Quantization (INT8, INT16)",
                  "points": 1
                },
                {
                  "description": "Pruning or knowledge distillation",
                  "points": 1
                },
                {
                  "description": "Architecture design for efficiency",
                  "points": 1
                },
                {
                  "description": "Memory-efficient operators",
                  "points": 1
                }
              ]
            },
            "inference_optimization": {
              "points": 3,
              "criteria": [
                {
                  "description": "Batch processing strategy",
                  "points": 1
                },
                {
                  "description": "Memory reuse and buffer management",
                  "points": 1
                },
                {
                  "description": "CMSIS-NN or similar optimizations",
                  "points": 1
                }
              ]
            },
            "power_considerations": {
              "points": 3,
              "criteria": [
                {
                  "description": "Duty cycling inference",
                  "points": 1
                },
                {
                  "description": "Early exit strategies",
                  "points": 1
                },
                {
                  "description": "Sleep mode integration",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "Edge ML Optimization for Cortex-M4:\n\n**Model Architecture (designed for edge):**\n- Replace LSTM with 1D CNN (better parallelization)\n- Depthwise separable convolutions (reduce params 8-9x)\n- Architecture: DS-Conv(16) → DS-Conv(32) → GlobalPool → Dense(16) → Dense(1)\n- Params: ~15K (vs 100K+ for LSTM equivalent)\n\n**Quantization:**\n- Post-training quantization to INT8\n- Use TensorFlow Lite Micro or TinyEngine\n- Quantization-aware training for <1% accuracy loss\n- Weights: INT8 (1 byte each)\n- Activations: INT8 with per-layer scale factors\n\n**Memory Layout:**\n```\nFlash (1MB):\n├── Model weights: 15KB (INT8)\n├── Lookup tables: 2KB  \n└── Code: 50KB\n\nRAM (256KB):\n├── Input buffer: 2KB (256 samples × 4 channels × INT16)\n├── Activation buffers: 32KB (reused between layers)\n├── Intermediate: 16KB\n└── Stack + heap: 8KB\n```\n\n**CMSIS-NN Optimization:**\n- Use ARM CMSIS-NN library for optimized kernels\n- SIMD instructions (2x INT16 or 4x INT8 per cycle)\n- Fused activation functions\n- Expected speedup: 3-5x vs naive implementation\n\n**Power Optimization:**\n1. Run inference every 5 minutes (not continuously)\n2. Early exit: if signal quality low, skip inference\n3. Tiered inference:\n   - Fast model for routine checks (5ms)\n   - Full model only when change detected (50ms)\n4. Sleep between samples, wake on interrupt\n\n**Inference Pipeline:**\n```\nWake → Sample 5s data → Quality check → Preprocess (fixed-point)\n→ Inference (50ms) → Post-process → BLE transmit → Sleep\n\nTotal active time: ~100ms per 5-minute cycle\nDuty cycle: 0.03% → ~1 year battery on coin cell\n```",
          "follow_up": "How would you validate that the quantized model maintains accuracy compared to the floating-point version?",
          "follow_up_answer": "1) Compare predictions on held-out test set: require <2% MARD increase, 2) Check per-range accuracy (hypo/normal/hyper), 3) Analyze layer-by-layer quantization error, 4) A/B test on real device vs cloud model, 5) Use mixed precision if specific layers are sensitive (keep them FP16).",
          "hints": [
            "Consider both model size and inference speed",
            "Power is often the primary constraint"
          ]
        },
        {
          "id": "ml_7",
          "title": "Clarke Error Grid Analysis",
          "category": "evaluation",
          "difficulty": [
            "medium"
          ],
          "question_text": "Explain the Clarke Error Grid for glucose monitor evaluation. Your model achieves 85% Zone A, 12% Zone B, 2% Zone C, 1% Zone D, 0% Zone E. Is this acceptable for FDA approval? How would you improve Zone C/D results?",
          "rubric": {
            "understanding": {
              "points": 3,
              "criteria": [
                {
                  "description": "Correctly explains zone definitions",
                  "points": 1
                },
                {
                  "description": "Understands clinical significance of each zone",
                  "points": 1
                },
                {
                  "description": "Knows FDA requirements",
                  "points": 1
                }
              ]
            },
            "analysis": {
              "points": 3,
              "criteria": [
                {
                  "description": "Correctly evaluates given results",
                  "points": 1
                },
                {
                  "description": "Identifies Zone C/D as problematic",
                  "points": 1
                },
                {
                  "description": "Discusses clinical risk",
                  "points": 1
                }
              ]
            },
            "improvement": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes error analysis approach",
                  "points": 1
                },
                {
                  "description": "Suggests model improvements",
                  "points": 1
                },
                {
                  "description": "Considers calibration strategies",
                  "points": 1
                },
                {
                  "description": "Discusses per-subject adaptation",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Clarke Error Grid Explanation:**\n- Zone A: Clinically accurate (within 20% or 20 mg/dL for low values)\n- Zone B: Benign errors (would not lead to incorrect treatment)\n- Zone C: Overcorrection errors (might cause unnecessary treatment)\n- Zone D: Dangerous failure to detect (missing hypo/hyper events)\n- Zone E: Opposite treatment (predicting hypo when hyper, or vice versa)\n\n**FDA Requirements:**\n- Typically require >95% in Zone A+B combined (your model: 97% ✓)\n- Zone D+E should be <1% (your model: 1% Zone D, borderline ✗)\n- MARD <15% for CGM devices\n\n**Your Results Analysis:**\n- 97% A+B: Good overall accuracy\n- 2% Zone C: Moderate concern - could cause overtreament\n- 1% Zone D: Critical issue - missing dangerous events\n- 0% Zone E: Good - no opposite predictions\n\n**Verdict: Borderline for FDA.** Zone D is the biggest concern.\n\n**Improvement Strategies:**\n\n1. **Error Analysis:**\n   - Plot Zone D points: are they clustered in hypo or hyper range?\n   - Check if specific subjects, activities, or times cause errors\n   - Likely cause: model underestimates at extremes\n\n2. **Model Improvements:**\n   - Asymmetric loss function: penalize hypo misses more heavily\n   - Focal loss on hard examples (extreme glucose values)\n   - Ensemble with model specifically trained on extreme ranges\n\n3. **Calibration:**\n   - Per-subject calibration period with finger-prick reference\n   - Recalibrate daily for first week, then weekly\n   - Use Platt scaling for better probability calibration\n\n4. **Safety Layer:**\n   - Conservative thresholds for hypo alerts\n   - Require trend confirmation before clearing hypo warning\n   - Display confidence intervals, not just point estimates",
          "follow_up": "If you had to choose between improving Zone C or Zone D performance, which would you prioritize and why?",
          "follow_up_answer": "Zone D, absolutely. Zone C means unnecessary treatment (patient takes sugar when not needed - inconvenient but not dangerous). Zone D means missing a dangerous event (patient doesn't treat hypoglycemia, risking coma/death). Safety-critical systems must prioritize avoiding dangerous false negatives over false positives.",
          "hints": [
            "Consider the clinical implications of each zone",
            "Think about what makes some errors more dangerous"
          ]
        }
      ],
      "trial_assignment": {
        "title": "Non-invasive Blood Glucose Monitoring Challenge",
        "duration_hours": 8,
        "deliverables": [
          {
            "id": "d1",
            "title": "Data Harmonization",
            "weight": 0.2,
            "duration_hours": 2,
            "requirements": [
              "Load all five datasets programmatically",
              "Align different sampling rates (25 Hz to 1000 Hz)",
              "Synchronize PPG signals with glucose measurements",
              "Handle missing data and outliers",
              "Output standardized format for all datasets"
            ],
            "evaluation_criteria": [
              "Completeness",
              "Robustness",
              "Efficiency"
            ]
          },
          {
            "id": "d2",
            "title": "Signal Processing Pipeline",
            "weight": 0.25,
            "duration_hours": 2,
            "requirements": [
              "Develop signal quality assessment metrics",
              "Implement baseline wander removal",
              "Add noise filtering with justified filter choices",
              "Motion artifact detection and suppression",
              "Address inter-subject normalization"
            ],
            "evaluation_criteria": [
              "Technical soundness",
              "Artifact handling"
            ]
          },
          {
            "id": "d3",
            "title": "Feature Engineering",
            "weight": 0.25,
            "duration_hours": 2,
            "requirements": [
              "Extract time domain features (statistical, peak, morphological)",
              "Extract frequency domain features (PSD, entropy)",
              "Extract nonlinear features (sample entropy, DFA)",
              "Cross-modal features if applicable"
            ],
            "evaluation_criteria": [
              "Creativity",
              "Domain relevance",
              "Interpretability"
            ]
          },
          {
            "id": "d4",
            "title": "Model Development",
            "weight": 0.2,
            "duration_hours": 1.5,
            "requirements": [
              "Justify model selection",
              "Handle dataset imbalance and subject variability",
              "Implement appropriate cross-validation",
              "Report MAE, MARD, RMSE, Clarke Error Grid"
            ],
            "evaluation_criteria": [
              "Performance",
              "Validation methodology"
            ]
          },
          {
            "id": "d5",
            "title": "Documentation",
            "weight": 0.1,
            "duration_hours": 0.5,
            "requirements": [
              "Technical report (3-5 pages)",
              "Clear code documentation",
              "Visualizations of results"
            ],
            "evaluation_criteria": [
              "Clarity",
              "Insights",
              "Reproducibility"
            ]
          }
        ]
      }
    },
    "biomedical_engineer": {
      "track_id": "biomedical_engineer",
      "title": "Biomedical Engineer",
      "description": "Biomedical Engineer assessment focusing on biosignal processing, regulatory compliance, sensor fusion, and medical device safety",
      "keywords": [
        "biomedical",
        "biosignal",
        "ECG",
        "PPG",
        "FDA",
        "regulatory",
        "medical device",
        "ISO 13485",
        "IEC 60601",
        "signal processing",
        "healthcare"
      ],
      "technical_questions": [
        {
          "id": "bio_1",
          "title": "BioSignal Processing",
          "category": "signal_processing",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You receive raw ECG and PPG signals from a wearable device with significant motion artifacts and baseline wander. Describe your approach to preprocessing these signals for accurate heart rate and heart rate variability (HRV) analysis.",
          "rubric": {
            "signal_processing_fundamentals": {
              "points": 4,
              "criteria": [
                {
                  "description": "Mentions appropriate filtering techniques (bandpass, notch)",
                  "points": 1
                },
                {
                  "description": "Discusses baseline wander removal",
                  "points": 1
                },
                {
                  "description": "Addresses motion artifact reduction",
                  "points": 1
                },
                {
                  "description": "Explains R-peak detection approach",
                  "points": 1
                }
              ]
            },
            "advanced_processing": {
              "points": 3,
              "criteria": [
                {
                  "description": "Discusses adaptive filtering or wavelet denoising",
                  "points": 1
                },
                {
                  "description": "Mentions signal quality assessment",
                  "points": 1
                },
                {
                  "description": "Describes HRV-specific requirements",
                  "points": 1
                }
              ]
            },
            "implementation_understanding": {
              "points": 3,
              "criteria": [
                {
                  "description": "Discusses practical implementation considerations",
                  "points": 1
                },
                {
                  "description": "Mentions real-time processing requirements",
                  "points": 1
                },
                {
                  "description": "Addresses computational efficiency",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "For preprocessing biosignals from a wearable:\n\n**Filtering Strategy:**\n1. ECG: Bandpass 0.5-40Hz (preserves QRS while removing muscle noise)\n2. PPG: Bandpass 0.5-10Hz (heart rate content)\n3. Notch filter at 50/60Hz for powerline interference\n\n**Baseline Wander Removal:**\n- Moving average subtraction (window = 2× max RR interval)\n- Or high-pass filter with very low cutoff (0.5Hz)\n- Cubic spline fitting for severe drift\n\n**Motion Artifact Handling:**\n- Adaptive filter using accelerometer as reference (LMS/NLMS)\n- Wavelet denoising: decompose, threshold detail coefficients, reconstruct\n- Signal quality index to mark unreliable segments\n\n**R-Peak Detection (ECG):**\n- Pan-Tompkins algorithm: bandpass → derivative → squaring → integration → adaptive threshold\n- For PPG: systolic peak detection with adaptive thresholding\n\n**HRV Analysis:**\n- Ectopic beat detection and correction\n- Interpolation for missing beats\n- Time-domain: RMSSD, SDNN, pNN50\n- Frequency-domain: LF/HF ratio (requires 5+ min windows)\n\n**Real-time Considerations:**\n- Use IIR filters for efficiency\n- Circular buffers for sliding windows\n- ~10ms processing budget per sample on Cortex-M4",
          "follow_up": "How would you modify your signal processing pipeline to handle pediatric patients, where heart rates and signal characteristics can be significantly different from adults?",
          "follow_up_answer": "For pediatric patients: 1) Extend bandpass upper cutoff to 100Hz for sharper QRS, 2) Adjust Pan-Tompkins refractory period for HR up to 180bpm, 3) Recalibrate adaptive thresholds for smaller amplitude signals, 4) Use age-specific HRV normative ranges, 5) Account for higher baseline movement in artifact detection.",
          "hints": [
            "Consider the frequency content of physiological signals vs noise",
            "Think about what makes motion artifacts different from the signal of interest"
          ]
        },
        {
          "id": "bio_2",
          "title": "Regulatory Compliance",
          "category": "regulatory",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You're developing a wearable ECG device for continuous heart rhythm monitoring. What are the key regulatory considerations and standards you need to address for FDA clearance?",
          "rubric": {
            "regulatory_knowledge": {
              "points": 4,
              "criteria": [
                {
                  "description": "Identifies correct device classification (Class II)",
                  "points": 1
                },
                {
                  "description": "Mentions relevant FDA guidance documents",
                  "points": 1
                },
                {
                  "description": "Discusses 510(k) requirements",
                  "points": 1
                },
                {
                  "description": "Understands substantial equivalence",
                  "points": 1
                }
              ]
            },
            "standards_compliance": {
              "points": 3,
              "criteria": [
                {
                  "description": "Mentions IEC 60601-1",
                  "points": 1
                },
                {
                  "description": "Discusses ISO 14971 risk management",
                  "points": 1
                },
                {
                  "description": "Addresses software validation requirements",
                  "points": 1
                }
              ]
            },
            "documentation_requirements": {
              "points": 3,
              "criteria": [
                {
                  "description": "Describes testing documentation needs",
                  "points": 1
                },
                {
                  "description": "Mentions quality system requirements (ISO 13485)",
                  "points": 1
                },
                {
                  "description": "Discusses clinical validation requirements",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**FDA Regulatory Pathway:**\n\n**Device Classification:** Class II medical device (ECG monitor)\n- Requires 510(k) premarket notification\n- Must demonstrate substantial equivalence to predicate device\n\n**Key Standards:**\n1. IEC 60601-1: General electrical safety for medical devices\n2. IEC 60601-2-47: Specific requirements for ambulatory ECG\n3. ISO 14971: Risk management framework (FMEA, hazard analysis)\n4. ISO 13485: Quality management system\n5. IEC 62304: Software lifecycle for medical device software\n\n**510(k) Submission Components:**\n- Device description and intended use\n- Substantial equivalence comparison\n- Performance testing data (accuracy, precision)\n- Biocompatibility testing (ISO 10993)\n- Electrical safety testing\n- EMC testing (IEC 60601-1-2)\n- Software documentation (level of concern, V&V)\n- Labeling and instructions for use\n\n**Special Controls for ECG:**\n- Clinical performance testing with diverse population\n- Algorithm validation against annotated databases (MIT-BIH)\n- False positive/negative rate documentation\n- Cybersecurity considerations\n\n**Quality System:**\n- Design controls (21 CFR 820.30)\n- Document management\n- Complaint handling and post-market surveillance\n- CAPA procedures",
          "follow_up": "If you plan to market this device internationally, what additional regulatory considerations would you need to address for CE marking in Europe?",
          "follow_up_answer": "For EU CE marking under MDR 2017/745: 1) Higher clinical evidence requirements vs MDD, 2) Clinical Evaluation Report (CER) with EU-specific data, 3) Post-market surveillance plan and PSURs, 4) EUDAMED registration, 5) UDI compliance, 6) EU Authorized Representative, 7) Notified Body audit, 8) Technical documentation per MDR Annex II/III.",
          "hints": [
            "Think about the regulatory pathway for a Class II device",
            "Consider both safety and efficacy requirements"
          ]
        },
        {
          "id": "bio_3",
          "title": "Sensor Fusion",
          "category": "signal_processing",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "Describe how you would implement sensor fusion between ECG, PPG, and accelerometer data to improve the accuracy of heart rate monitoring during physical activity.",
          "rubric": {
            "sensor_understanding": {
              "points": 3,
              "criteria": [
                {
                  "description": "Explains characteristics of each sensor",
                  "points": 1
                },
                {
                  "description": "Discusses limitations and strengths",
                  "points": 1
                },
                {
                  "description": "Addresses sampling rate considerations",
                  "points": 1
                }
              ]
            },
            "fusion_strategy": {
              "points": 4,
              "criteria": [
                {
                  "description": "Describes synchronization approach",
                  "points": 1
                },
                {
                  "description": "Explains fusion algorithm selection (Kalman, weighted)",
                  "points": 1
                },
                {
                  "description": "Discusses motion artifact handling",
                  "points": 1
                },
                {
                  "description": "Addresses real-time processing requirements",
                  "points": 1
                }
              ]
            },
            "implementation_details": {
              "points": 3,
              "criteria": [
                {
                  "description": "Mentions performance evaluation",
                  "points": 1
                },
                {
                  "description": "Discusses error handling",
                  "points": 1
                },
                {
                  "description": "Addresses computational efficiency",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Multi-Sensor Fusion for Heart Rate:**\n\n**Sensor Characteristics:**\n- ECG (250Hz): Gold standard, but electrode motion causes artifacts\n- PPG (100Hz): Optical, affected by motion and perfusion\n- Accelerometer (50Hz): Detects motion for artifact compensation\n\n**Synchronization:**\n- Align all sensors to common timebase (UTC timestamps)\n- Upsample accelerometer to PPG rate (interpolation)\n- Use hardware trigger for precise alignment\n\n**Fusion Architecture:**\n```\nECG → R-peak detect → HR_ecg → ─┐\n                                  ├→ Kalman Filter → HR_fused\nPPG → Peak detect → HR_ppg → ────┘\n                                  ↑\nAccel → Motion detect → Quality weights\n```\n\n**Extended Kalman Filter:**\n- State: [HR, HR_rate_of_change]\n- Measurements: HR from ECG, HR from PPG\n- Dynamic measurement noise: increase when motion detected\n- Process model: HR changes slowly (<3 bpm/sec typically)\n\n**Motion-Adaptive Weighting:**\n```python\ndef compute_weights(accel_mag, ppg_quality, ecg_quality):\n    if accel_mag > MOTION_THRESHOLD:\n        # During motion, ECG often more reliable\n        w_ecg = 0.8 * ecg_quality\n        w_ppg = 0.2 * ppg_quality\n    else:\n        # At rest, weight by signal quality\n        w_ecg = ecg_quality / (ecg_quality + ppg_quality)\n        w_ppg = ppg_quality / (ecg_quality + ppg_quality)\n    return w_ecg, w_ppg\n```\n\n**Graceful Degradation:**\n- If ECG fails: rely on PPG with increased uncertainty\n- If both corrupted: hold last valid HR, flag as uncertain\n- Use physiological limits (30-220 bpm) as sanity check",
          "follow_up": "How would you modify your sensor fusion approach to handle scenarios where one of the sensors completely fails or becomes highly unreliable for an extended period?",
          "follow_up_answer": "For sensor failure: 1) Implement sensor health monitoring (check signal amplitude, noise floor), 2) Automatic fallback to single-sensor mode with widened confidence intervals, 3) Kalman filter reconfiguration with modified measurement model, 4) User notification of degraded accuracy, 5) Attempt sensor recovery (reset, recalibration), 6) Log failure for post-hoc analysis.",
          "hints": [
            "Consider how each sensor performs differently during motion",
            "Think about how to weight unreliable measurements"
          ]
        },
        {
          "id": "bio_4",
          "title": "Safety and Risk Management",
          "category": "safety",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "What are the key safety considerations and potential failure modes you would address in the design of a wearable medical device? How would you document and mitigate these risks?",
          "rubric": {
            "risk_identification": {
              "points": 3,
              "criteria": [
                {
                  "description": "Identifies electrical safety risks",
                  "points": 1
                },
                {
                  "description": "Discusses biocompatibility concerns",
                  "points": 1
                },
                {
                  "description": "Addresses software failure modes",
                  "points": 1
                }
              ]
            },
            "mitigation_strategy": {
              "points": 4,
              "criteria": [
                {
                  "description": "Describes risk control measures",
                  "points": 1
                },
                {
                  "description": "Explains verification methods",
                  "points": 1
                },
                {
                  "description": "Discusses fail-safe mechanisms",
                  "points": 1
                },
                {
                  "description": "Addresses user safety features",
                  "points": 1
                }
              ]
            },
            "documentation": {
              "points": 3,
              "criteria": [
                {
                  "description": "Mentions risk management file",
                  "points": 1
                },
                {
                  "description": "Discusses traceability",
                  "points": 1
                },
                {
                  "description": "Describes validation protocols",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Wearable Medical Device Risk Management (ISO 14971):**\n\n**Key Hazard Categories:**\n\n1. **Electrical Safety:**\n   - Leakage current (<10µA for patient contact)\n   - Defibrillation protection\n   - Battery thermal runaway\n   - Mitigation: Isolation barriers, current limiting, thermal cutoffs\n\n2. **Biocompatibility:**\n   - Skin irritation from prolonged contact\n   - Allergic reactions to materials\n   - Mitigation: ISO 10993 testing, hypoallergenic materials, user warnings\n\n3. **Software Failures:**\n   - Algorithm provides incorrect reading\n   - System crash during critical measurement\n   - Mitigation: Watchdog timers, graceful degradation, plausibility checks\n\n4. **Mechanical:**\n   - Device breaks and causes injury\n   - Ingestion hazard (small parts)\n   - Mitigation: Drop testing, sharp edge avoidance, secure assembly\n\n**FMEA Process:**\n| Failure Mode | Severity | Probability | Detection | RPN | Mitigation |\n|-------------|----------|-------------|-----------|-----|------------|\n| False low HR alert | 7 | 3 | 5 | 105 | Algorithm validation, clinical testing |\n| Battery fire | 10 | 1 | 4 | 40 | Certified cells, thermal protection |\n| Skin rash | 4 | 4 | 2 | 32 | Material testing, user instructions |\n\n**Risk Management File:**\n- Hazard identification worksheet\n- Risk estimation for each hazard\n- Risk control measures and verification\n- Residual risk assessment\n- Risk-benefit analysis\n- Post-market surveillance plan\n\n**Traceability:**\nRequirements → Design → Verification → Validation → Risk Controls",
          "follow_up": "How would you handle a post-market safety incident where a device reports an incorrect heart rhythm classification that leads to unnecessary medical intervention?",
          "follow_up_answer": "Post-market incident response: 1) Immediate: Collect device data, notify regulatory affairs, assess scope, 2) Investigation: Root cause analysis, algorithm review, check for similar cases in complaints, 3) Reporting: MDR to FDA within timeframe based on severity, 4) CAPA: Algorithm fix, validation, field update plan, 5) Communication: Healthcare provider notification, user instructions, 6) Prevention: Update FMEA, improve validation protocols.",
          "hints": [
            "Think about electrical, biological, and software risks",
            "Consider the ISO 14971 risk management framework"
          ]
        },
        {
          "id": "bio_5",
          "title": "AFib Detection Algorithm",
          "category": "algorithm",
          "difficulty": [
            "hard"
          ],
          "question_text": "Design an algorithm to detect atrial fibrillation (AFib) using ECG data from a wearable device. What features would you extract and what classification approach would you use?",
          "rubric": {
            "feature_engineering": {
              "points": 3,
              "criteria": [
                {
                  "description": "Identifies relevant ECG features (RR variability)",
                  "points": 1
                },
                {
                  "description": "Discusses P-wave analysis",
                  "points": 1
                },
                {
                  "description": "Addresses signal quality metrics",
                  "points": 1
                }
              ]
            },
            "classification_strategy": {
              "points": 4,
              "criteria": [
                {
                  "description": "Explains algorithm selection",
                  "points": 1
                },
                {
                  "description": "Discusses training approach",
                  "points": 1
                },
                {
                  "description": "Addresses real-time processing",
                  "points": 1
                },
                {
                  "description": "Mentions performance metrics",
                  "points": 1
                }
              ]
            },
            "implementation_considerations": {
              "points": 3,
              "criteria": [
                {
                  "description": "Discusses validation strategy",
                  "points": 1
                },
                {
                  "description": "Addresses false positive control",
                  "points": 1
                },
                {
                  "description": "Mentions regulatory requirements",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**AFib Detection Algorithm Design:**\n\n**Physiological Basis:**\n- AFib: Irregular, chaotic atrial activity → irregular RR intervals\n- Absence of organized P-waves\n- Fibrillatory waves (f-waves) instead of P-waves\n\n**Feature Extraction (30-second windows):**\n\n1. **RR Interval Features:**\n   - RMSSD (root mean square of successive differences)\n   - pNN50 (percentage of NN intervals >50ms different)\n   - Shannon entropy of RR histogram\n   - Turning point ratio\n   - Coefficient of variation\n\n2. **P-wave Features:**\n   - P-wave presence/absence detection\n   - P-wave morphology consistency\n   - PR interval variability\n\n3. **Signal Quality:**\n   - SNR estimation\n   - Baseline wander magnitude\n   - Motion artifact indicator\n\n**Classification Approach:**\n\n*Two-Stage Detection:*\n```\nStage 1: RR irregularity detector (fast, high sensitivity)\n  - Random Forest on RR features\n  - If irregularity detected → Stage 2\n\nStage 2: Morphological analysis (accurate)\n  - CNN on raw ECG segment\n  - Or detailed P-wave analysis\n  - Combines with Stage 1 probability\n```\n\n**Model Architecture (CNN option):**\n- Input: 10s ECG segment (2500 samples @ 250Hz)\n- Conv1D(32) → Conv1D(64) → Conv1D(128) → GlobalPool → Dense(64) → Dense(2)\n- Output: [P(Normal), P(AFib)]\n\n**Validation:**\n- Train on MIT-BIH AFib database, PhysioNet Challenge data\n- Test on held-out clinical data with diverse demographics\n- Metrics: Sensitivity >95%, Specificity >90%, PPV, NPV\n- Per-episode and per-patient sensitivity\n\n**Regulatory Considerations:**\n- FDA requires clinical validation in target population\n- Compare to 12-lead Holter as reference\n- Document sensitivity/specificity for labeling claims",
          "follow_up": "How would you adapt your AFib detection algorithm to handle patients with implanted cardiac devices like pacemakers?",
          "follow_up_answer": "For pacemaker patients: 1) Paced beat detection using high-frequency spike detection (>1kHz sampling), 2) Classify beats as paced vs intrinsic, 3) Analyze only intrinsic beats for AFib features, 4) Modified RR analysis accounting for pacing rate, 5) Different thresholds for paced rhythms, 6) Flag pacemaker-detected and recommend clinician review, 7) May not be able to reliably detect AFib in 100% paced rhythms.",
          "hints": [
            "Consider the defining characteristics of AFib rhythm",
            "Think about the balance between sensitivity and specificity"
          ]
        },
        {
          "id": "bio_6",
          "title": "Design for Manufacturing",
          "category": "manufacturing",
          "difficulty": [
            "medium"
          ],
          "question_text": "You're transitioning a wearable biosensor from prototype to mass production. What design changes and considerations would you address to ensure consistent quality and manufacturability?",
          "rubric": {
            "dfm_principles": {
              "points": 4,
              "criteria": [
                {
                  "description": "Discusses component standardization",
                  "points": 1
                },
                {
                  "description": "Addresses tolerance analysis",
                  "points": 1
                },
                {
                  "description": "Mentions testability design",
                  "points": 1
                },
                {
                  "description": "Considers assembly optimization",
                  "points": 1
                }
              ]
            },
            "quality_considerations": {
              "points": 3,
              "criteria": [
                {
                  "description": "Discusses calibration procedures",
                  "points": 1
                },
                {
                  "description": "Addresses in-line testing",
                  "points": 1
                },
                {
                  "description": "Mentions statistical process control",
                  "points": 1
                }
              ]
            },
            "supply_chain": {
              "points": 3,
              "criteria": [
                {
                  "description": "Considers component availability",
                  "points": 1
                },
                {
                  "description": "Discusses second-source options",
                  "points": 1
                },
                {
                  "description": "Addresses documentation requirements",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Prototype to Production Transition:**\n\n**Design for Manufacturing (DFM):**\n\n1. **Component Selection:**\n   - Replace prototype components with production-grade\n   - Ensure automotive/medical grade where needed\n   - Verify long-term availability (3+ year roadmap)\n   - Identify second sources for critical components\n\n2. **Tolerance Analysis:**\n   - Stack-up analysis for mechanical fits\n   - Electrical tolerance analysis (worst-case, Monte Carlo)\n   - Sensor calibration range must cover production variation\n\n3. **Assembly Optimization:**\n   - Reduce unique part count\n   - Design for automated assembly (pick-and-place friendly)\n   - Minimize manual operations\n   - Standardize fasteners and connectors\n\n**Testability Design:**\n- Built-in self-test (BIST) for production testing\n- Test points for key signals\n- Calibration interface (JTAG, serial)\n- Go/no-go fixture design\n\n**Production Test Strategy:**\n```\n1. Bare PCB test: shorts, opens\n2. Post-SMT: functional test, calibration\n3. Final assembly: full system test\n4. Burn-in: accelerated aging (optional for medical)\n5. Final QC: cosmetic, labeling\n```\n\n**Quality System (ISO 13485):**\n- Device History Record (DHR) for each unit\n- Lot traceability for all components\n- Statistical process control on critical parameters\n- Calibration procedures with uncertainty analysis\n- Non-conformance handling procedures\n\n**Documentation:**\n- Bill of Materials (BOM) with approved vendor list\n- Assembly drawings and work instructions\n- Test specifications and acceptance criteria\n- Calibration procedures",
          "follow_up": "How would you approach component obsolescence for a medical device that needs to remain in production for 10+ years?",
          "follow_up_answer": "Obsolescence management: 1) Lifecycle monitoring service (e.g., SiliconExpert) for early warning, 2) Last-time-buy evaluation and buffer stock, 3) Design with form-fit-function equivalent acceptance criteria, 4) Qualification process for alternate components, 5) Regular design refresh cycle (every 3-4 years), 6) Platform approach allowing component swaps without full revalidation.",
          "hints": [
            "Think about what changes between prototype and production",
            "Consider testability and quality control"
          ]
        },
        {
          "id": "bio_7",
          "title": "Clinical Study Design",
          "category": "clinical",
          "difficulty": [
            "hard"
          ],
          "question_text": "You need to design a clinical validation study for a new non-invasive glucose monitoring device. What study design would you propose, what endpoints would you measure, and how would you ensure the study meets FDA requirements?",
          "rubric": {
            "study_design": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes appropriate study type",
                  "points": 1
                },
                {
                  "description": "Discusses population selection",
                  "points": 1
                },
                {
                  "description": "Addresses sample size justification",
                  "points": 1
                },
                {
                  "description": "Considers control conditions",
                  "points": 1
                }
              ]
            },
            "endpoints": {
              "points": 3,
              "criteria": [
                {
                  "description": "Defines primary endpoint (MARD)",
                  "points": 1
                },
                {
                  "description": "Includes secondary endpoints",
                  "points": 1
                },
                {
                  "description": "Addresses safety endpoints",
                  "points": 1
                }
              ]
            },
            "regulatory_alignment": {
              "points": 3,
              "criteria": [
                {
                  "description": "References FDA guidance documents",
                  "points": 1
                },
                {
                  "description": "Discusses IRB requirements",
                  "points": 1
                },
                {
                  "description": "Addresses data quality and integrity",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Clinical Validation Study Design:**\n\n**Study Type:** Prospective, multi-site, paired comparison study\n\n**Population:**\n- N=350+ subjects (power analysis for MARD precision)\n- Inclusion: Adults with Type 1 or Type 2 diabetes\n- Stratification: Age, diabetes type, skin tone (Fitzpatrick I-VI), BMI\n- Exclusion: Conditions affecting peripheral circulation\n\n**Reference Standard:**\n- Yellow Springs Instrument (YSI) glucose analyzer\n- Or FDA-cleared laboratory analyzer\n\n**Study Procedure:**\n1. Device placement and equilibration (1 hour)\n2. Glucose challenges (meals, insulin adjustments)\n3. Paired samples: Device reading vs YSI every 15 minutes\n4. Minimum 8 hours per subject\n5. Target glucose range coverage: <70, 70-180, >180 mg/dL\n\n**Primary Endpoint:**\n- MARD (Mean Absolute Relative Difference)\n- Target: MARD <15% for CGM claims (FDA guidance)\n\n**Secondary Endpoints:**\n- %20/20 (within 20% or 20 mg/dL)\n- Consensus Error Grid zones (>95% in A+B)\n- Lag time analysis\n- Sensor wear duration\n\n**Safety Endpoints:**\n- Adverse events (skin reactions, device failures)\n- Serious adverse events\n\n**FDA Alignment:**\n- Follow FDA Guidance: \"Blood Glucose Monitoring Test Systems\"\n- IDE (Investigational Device Exemption) if significant risk\n- IRB approval at all sites\n- GCP compliance\n- 21 CFR Part 11 compliant data systems\n\n**Statistical Analysis Plan:**\n- Pre-specified in protocol\n- MARD with 95% CI\n- Subgroup analyses (skin tone, glucose range)\n- Sensitivity analyses for missing data",
          "follow_up": "How would you handle a situation where your device shows good accuracy overall but performs poorly in the hypoglycemic range (<70 mg/dL)?",
          "follow_up_answer": "Hypoglycemia performance issue: 1) Root cause analysis (signal quality, algorithm behavior at low values), 2) Consider separate algorithm for low-range detection, 3) May need to add labeling restriction ('not for hypoglycemia detection'), 4) Discuss with FDA pre-submission, 5) Post-market commitment for improvement, 6) Risk-benefit analysis: is overall benefit sufficient despite limitation? 7) User warnings and clinical decision support integration.",
          "hints": [
            "Consider FDA guidance for CGM devices",
            "Think about the populations that need to be represented"
          ]
        }
      ]
    },
    "electrical_engineer": {
      "track_id": "electrical_engineer",
      "title": "Electrical Engineer",
      "description": "Electrical Engineer assessment focusing on analog design, PCB layout, power management, EMC, and mixed-signal systems",
      "keywords": [
        "electrical",
        "analog",
        "PCB",
        "power",
        "EMC",
        "signal integrity",
        "schematic",
        "layout",
        "amplifier",
        "ADC",
        "DAC",
        "filter"
      ],
      "technical_questions": [
        {
          "id": "ee_1",
          "title": "Low Noise Amplifier Design",
          "category": "analog_design",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "Design a low-noise analog front-end for acquiring PPG signals from a photodiode. The signal bandwidth is 0.5-10 Hz with expected photocurrent of 100nA-10µA. What topology would you use and how would you minimize noise?",
          "rubric": {
            "topology_selection": {
              "points": 4,
              "criteria": [
                {
                  "description": "Chooses transimpedance amplifier (TIA)",
                  "points": 1
                },
                {
                  "description": "Explains TIA advantages for photodiode",
                  "points": 1
                },
                {
                  "description": "Discusses gain and bandwidth tradeoffs",
                  "points": 1
                },
                {
                  "description": "Considers subsequent filtering stages",
                  "points": 1
                }
              ]
            },
            "noise_analysis": {
              "points": 4,
              "criteria": [
                {
                  "description": "Identifies noise sources (thermal, shot, op-amp)",
                  "points": 1
                },
                {
                  "description": "Discusses op-amp selection criteria",
                  "points": 1
                },
                {
                  "description": "Calculates or estimates SNR",
                  "points": 1
                },
                {
                  "description": "Proposes noise reduction techniques",
                  "points": 1
                }
              ]
            },
            "practical_considerations": {
              "points": 2,
              "criteria": [
                {
                  "description": "Considers power consumption",
                  "points": 1
                },
                {
                  "description": "Addresses dynamic range requirements",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**PPG Analog Front-End Design:**\n\n**Topology: Transimpedance Amplifier (TIA)**\n- Converts photodiode current directly to voltage\n- Maintains photodiode at virtual ground (low bias = low dark current)\n- Gain: Rf (feedback resistor) typically 100kΩ-10MΩ\n\n**Circuit:**\n```\nPhotodiode → TIA (Rf=1MΩ) → Highpass (0.5Hz) → Lowpass (10Hz) → ADC\n                ↓\n            Gain: 1V/µA\n```\n\n**Noise Analysis:**\n\n1. **Johnson noise (Rf):** √(4kTRfBW) = √(4×1.38e-23×300×1e6×10) = 12.8µV\n2. **Shot noise (photodiode):** √(2qIdBW) = √(2×1.6e-19×1e-6×10) = 1.8nV (negligible)\n3. **Op-amp noise:** Choose low-noise op-amp (e.g., OPA2140: 5.1nV/√Hz)\n   Input-referred: 5.1×√10 = 16nV (negligible vs Rf noise)\n\n**Op-Amp Selection Criteria:**\n- Input bias current < 1pA (CMOS/JFET input)\n- Low voltage noise: <10nV/√Hz\n- GBW > 1MHz for stability with large Rf\n- Low power: <1mA for wearable\n- Rail-to-rail output for maximum dynamic range\n\n**Noise Reduction Techniques:**\n- Narrow bandwidth filtering (reduces noise BW)\n- Shield photodiode and TIA stage\n- Use guard ring on PCB\n- Consider chopper-stabilized op-amp for DC precision\n\n**Dynamic Range:**\n- Signal: 100nA-10µA → 100mV-10V (with Rf=1MΩ)\n- Need programmable gain or log amp for 100:1 range\n- Or use two TIA gains with auto-ranging",
          "follow_up": "How would you modify this design to handle the large DC component from ambient light while preserving the small AC cardiac signal?",
          "follow_up_answer": "Ambient light rejection: 1) AC-coupled design with high-pass filter after TIA, 2) Ambient light cancellation with dark reference photodiode, 3) Synchronous detection with modulated LED, 4) Two-stage amplifier: TIA with low gain, then AC-coupled high-gain stage, 5) Digital DC removal in DSP if ADC has sufficient dynamic range.",
          "hints": [
            "Consider transimpedance vs voltage amplifier topology",
            "Think about all noise sources in the signal chain"
          ]
        },
        {
          "id": "ee_2",
          "title": "Mixed-Signal PCB Design",
          "category": "pcb_design",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You're designing a 4-layer PCB for a wearable with sensitive analog (PPG), digital (MCU, BLE), and power sections. Describe your layer stack-up, grounding strategy, and techniques to minimize crosstalk between analog and digital domains.",
          "rubric": {
            "layer_stackup": {
              "points": 3,
              "criteria": [
                {
                  "description": "Proposes appropriate 4-layer stack",
                  "points": 1
                },
                {
                  "description": "Places ground plane adjacent to signal layers",
                  "points": 1
                },
                {
                  "description": "Considers impedance control",
                  "points": 1
                }
              ]
            },
            "grounding_strategy": {
              "points": 4,
              "criteria": [
                {
                  "description": "Discusses star vs split ground",
                  "points": 1
                },
                {
                  "description": "Explains single-point connection for analog/digital",
                  "points": 1
                },
                {
                  "description": "Addresses ground return paths",
                  "points": 1
                },
                {
                  "description": "Considers ground plane slots",
                  "points": 1
                }
              ]
            },
            "isolation_techniques": {
              "points": 3,
              "criteria": [
                {
                  "description": "Physical separation of analog/digital",
                  "points": 1
                },
                {
                  "description": "Proper routing practices",
                  "points": 1
                },
                {
                  "description": "Discusses filtering and decoupling",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**4-Layer Mixed-Signal PCB Design:**\n\n**Layer Stack-Up:**\n```\nLayer 1 (Top): Signal + Components\nLayer 2: Ground Plane (continuous)\nLayer 3: Power Plane (split as needed)\nLayer 4 (Bottom): Signal + Components\n```\n- Critical: Continuous ground plane on L2 for return currents\n- Controlled impedance: 50Ω traces over ground (0.2mm trace, 0.2mm dielectric)\n\n**Grounding Strategy:**\n```\n        ┌─────────────────────────────┐\n        │      Solid Ground L2        │\n        │  ┌─────────┬─────────┐     │\n        │  │ ANALOG  │ DIGITAL │     │\n        │  │ section │ section │     │\n        │  └────┬────┴────┬────┘     │\n        │       │         │          │\n        │       └────┬────┘          │\n        │         Single             │\n        │         Point              │\n        │        (under ADC)         │\n        └─────────────────────────────┘\n```\n- Single solid ground plane (NOT split)\n- Connect analog and digital domains at one point (under ADC)\n- This allows return currents to flow naturally\n\n**Isolation Techniques:**\n\n1. **Physical Separation:**\n   - Analog section in one corner\n   - Digital/BLE away from analog\n   - Power supply between (acts as buffer)\n\n2. **Routing Rules:**\n   - No digital traces crossing analog area\n   - Keep analog traces short\n   - Use ground-signal-ground for sensitive analog\n   - Route clock signals away from analog\n\n3. **Decoupling:**\n   - 100nF ceramic on every power pin\n   - 10µF bulk near power entry\n   - Ferrite bead between analog/digital power\n   - Filter on VREF lines\n\n4. **Component Placement:**\n   - Crystal/oscillator away from analog inputs\n   - Switching regulator away from TIA\n   - Shield can over analog section if needed",
          "follow_up": "If you had to add a switching power supply to this board, where would you place it and how would you manage the EMI?",
          "follow_up_answer": "Switching supply placement and EMI: 1) Place as far as possible from analog section, 2) Shield with ground pour and keep-out zone, 3) Minimize hot loop area (input cap-switch-inductor-output cap), 4) Use spread-spectrum modulation if available, 5) Input/output EMI filters with ferrite beads, 6) Ground plane stitching around switcher, 7) Consider LDO as post-regulator for analog supply.",
          "hints": [
            "Think about return current paths",
            "Consider where the sensitive components are"
          ]
        },
        {
          "id": "ee_3",
          "title": "Power Management",
          "category": "power",
          "difficulty": [
            "medium"
          ],
          "question_text": "Design the power architecture for a battery-powered wearable device. The device has: MCU (1.8V, 10mA avg), BLE radio (3.3V, 15mA during TX), analog front-end (3.3V, 5mA, needs <10µV ripple), and LEDs (3.6V, 50mA peak). Battery is 3.7V LiPo (100mAh). Target 7-day battery life.",
          "rubric": {
            "architecture_design": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes appropriate regulator types",
                  "points": 1
                },
                {
                  "description": "Separates noisy and sensitive supplies",
                  "points": 1
                },
                {
                  "description": "Addresses efficiency considerations",
                  "points": 1
                },
                {
                  "description": "Considers voltage headroom",
                  "points": 1
                }
              ]
            },
            "power_budget": {
              "points": 3,
              "criteria": [
                {
                  "description": "Calculates average power consumption",
                  "points": 1
                },
                {
                  "description": "Considers duty cycling",
                  "points": 1
                },
                {
                  "description": "Verifies battery life target",
                  "points": 1
                }
              ]
            },
            "low_power_techniques": {
              "points": 3,
              "criteria": [
                {
                  "description": "Discusses sleep modes",
                  "points": 1
                },
                {
                  "description": "Proposes duty cycling strategy",
                  "points": 1
                },
                {
                  "description": "Addresses standby current",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Wearable Power Architecture:**\n\n**Power Tree:**\n```\nLiPo 3.7V (2.8-4.2V)\n├── Buck 1.8V (MCU) - high efficiency for always-on\n├── LDO 3.3V_ANALOG (AFE) - low noise, <10µV ripple\n├── Buck/LDO 3.3V_DIGITAL (BLE) - can tolerate ripple\n└── Direct or boost for LEDs (50mA pulsed)\n```\n\n**Regulator Selection:**\n1. **MCU (1.8V, 10mA):** Buck converter (90%+ efficiency)\n   - Example: TPS62840 (60nA quiescent)\n2. **Analog AFE (3.3V, 5mA, low noise):** LDO with <10µV ripple\n   - Example: TPS7A20 (PSRR >60dB @ 1kHz)\n   - Fed from battery directly (needs 3.3V+dropout headroom)\n3. **BLE (3.3V, 15mA TX):** LDO or Buck-Boost\n   - LDO simpler but less efficient at low battery\n4. **LEDs (50mA):** Direct drive from battery or boost\n   - Current-mode drive for consistent brightness\n\n**Power Budget:**\n```\nComponent     Active(mA)  Duty    Avg(mA)\nMCU           10          100%    10.0\nBLE TX        15          0.1%    0.015\nBLE RX        8           0.5%    0.04\nBLE Sleep     0.002       99.4%   0.002\nAFE           5           10%     0.5\nLED           50          1%      0.5\nRegulators    -           -       0.1 (quiescent)\n─────────────────────────────────────\nTotal Average                     ~11.2 mA\n```\n\n**Battery Life:**\n- 100mAh / 11.2mA = 8.9 hours (NOT 7 days!)\n- Need aggressive duty cycling!\n\n**Revised Strategy for 7 Days:**\n```\n7 days × 24h = 168 hours\n100mAh / 168h = 0.6mA budget\n\nMCU: 100µA (deep sleep with periodic wake)\nBLE: 0.1mA (advertise only, 1Hz)\nAFE: 0.3mA (sample 1s every 10s = 10% duty)\nLED: 0.1mA (sample 100ms every 10s)\n───────────────────────────────────\nTotal: ~0.6mA ✓\n```",
          "follow_up": "How would you handle the low-battery condition where the LiPo voltage drops below the 3.3V LDO dropout?",
          "follow_up_answer": "Low battery handling: 1) Buck-boost for critical 3.3V rails (operates down to 2.8V), 2) Monitor battery with ADC and warn user, 3) Graceful shutdown sequence saving state, 4) Brown-out detection with clean reset, 5) Reduce functionality progressively (disable LEDs first), 6) Use LDO with very low dropout (<100mV) or PMOS pass element.",
          "hints": [
            "Consider which loads need clean power vs can tolerate ripple",
            "Calculate the power budget carefully"
          ]
        },
        {
          "id": "ee_4",
          "title": "EMC and Signal Integrity",
          "category": "emc",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "Your wearable device is failing radiated emissions tests at 125MHz (and harmonics) during CE/FCC testing. The device uses a 25MHz crystal. What's your approach to diagnosing and fixing this issue?",
          "rubric": {
            "diagnosis": {
              "points": 4,
              "criteria": [
                {
                  "description": "Identifies clock as likely source",
                  "points": 1
                },
                {
                  "description": "Discusses diagnostic techniques (near-field probe)",
                  "points": 1
                },
                {
                  "description": "Understands harmonic relationship",
                  "points": 1
                },
                {
                  "description": "Considers antenna structures",
                  "points": 1
                }
              ]
            },
            "solutions": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes spread-spectrum clocking",
                  "points": 1
                },
                {
                  "description": "Discusses shielding options",
                  "points": 1
                },
                {
                  "description": "Suggests PCB modifications",
                  "points": 1
                },
                {
                  "description": "Considers filtering",
                  "points": 1
                }
              ]
            },
            "prevention": {
              "points": 2,
              "criteria": [
                {
                  "description": "Discusses design-for-EMC practices",
                  "points": 1
                },
                {
                  "description": "Mentions early testing",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**EMC Failure Diagnosis and Resolution:**\n\n**Analysis:**\n- 125MHz = 5 × 25MHz (5th harmonic of crystal)\n- Likely source: clock distribution or high-speed digital signals\n- Harmonics extend to 150MHz, 175MHz, 200MHz (likely visible)\n\n**Diagnostic Steps:**\n\n1. **Near-field probing:**\n   - Use H-field and E-field probes with spectrum analyzer\n   - Scan around crystal, MCU clock pins, power pins\n   - Identify hot spots\n\n2. **Isolation testing:**\n   - Disable subsystems one at a time\n   - Does emission disappear when BLE disabled?\n   - Test with external clock to rule out crystal\n\n3. **Antenna identification:**\n   - Long traces, cables, or enclosure slots\n   - Check if cable routing changes emissions\n\n**Solutions (in order of ease):**\n\n1. **Spread-spectrum clocking:**\n   - If MCU supports it, enable ±1-2% spread\n   - Reduces peak energy by 10-20dB\n   - Verify system still functions with jitter\n\n2. **Filtering:**\n   - Add ferrite bead on clock line (100MHz impedance)\n   - RC snubber on fast edges (10-33Ω + 10-33pF)\n   - Common-mode choke on cables/connections\n\n3. **PCB Rework:**\n   - Ensure clock trace has solid ground return\n   - Add ground stitching vias around clock\n   - Shorten clock trace if possible\n\n4. **Shielding:**\n   - Add shield can over clock/MCU section\n   - EMI gasket on enclosure seams\n   - Conductive coating on plastic housing\n\n5. **Design Changes (if needed):**\n   - Use slower clock edge rate (series resistor)\n   - Route clock on inner layer\n   - Consider different crystal/oscillator package\n\n**Prevention:**\n- Pre-compliance testing during development\n- Follow EMC design rules from start\n- Ground plane integrity\n- Controlled impedance for high-speed",
          "follow_up": "The fix works for CE testing but now you're seeing ESD failures at the USB connector. How would you address this?",
          "follow_up_answer": "USB ESD protection: 1) Add TVS diode array at connector (TPD2E009 or similar), 2) Place protection as close to connector as possible, 3) Ensure ground connection is solid and short, 4) Consider series resistors on data lines for additional damping, 5) Route ESD current away from sensitive circuits, 6) Add common-mode choke for ESD and EMI, 7) Test per IEC 61000-4-2 at ±8kV contact discharge.",
          "hints": [
            "Think about the harmonic relationship to the crystal",
            "Consider both the source and the antenna"
          ]
        },
        {
          "id": "ee_5",
          "title": "Analog Frontend Design",
          "category": "analog_design",
          "difficulty": [
            "hard"
          ],
          "question_text": "Design a complete 6-channel PPG acquisition system with 3 LED wavelengths (Red, IR, Green) and 2 photodiodes. The system needs to multiplex LEDs, synchronously sample, and handle a 100:1 signal variation across skin types. Specify the key components and timing.",
          "rubric": {
            "system_architecture": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes appropriate LED drive scheme",
                  "points": 1
                },
                {
                  "description": "Designs photodiode frontend",
                  "points": 1
                },
                {
                  "description": "Addresses synchronous sampling",
                  "points": 1
                },
                {
                  "description": "Handles ambient light",
                  "points": 1
                }
              ]
            },
            "dynamic_range": {
              "points": 3,
              "criteria": [
                {
                  "description": "Proposes gain control strategy",
                  "points": 1
                },
                {
                  "description": "Addresses ADC resolution needs",
                  "points": 1
                },
                {
                  "description": "Discusses calibration",
                  "points": 1
                }
              ]
            },
            "timing_design": {
              "points": 3,
              "criteria": [
                {
                  "description": "Specifies LED on-time and sequencing",
                  "points": 1
                },
                {
                  "description": "Discusses sample timing vs LED",
                  "points": 1
                },
                {
                  "description": "Considers settling time",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**6-Channel PPG Acquisition System:**\n\n**Architecture:**\n```\n┌─────────────────────────────────────────────────┐\n│  LED Driver (programmable current, 0-50mA)     │\n│  ├── Red LED                                    │\n│  ├── IR LED                                     │\n│  └── Green LED                                  │\n├─────────────────────────────────────────────────┤\n│  Photodiode 1 → TIA → PGA → ADC_CH1            │\n│  Photodiode 2 → TIA → PGA → ADC_CH2            │\n├─────────────────────────────────────────────────┤\n│  MCU: LED timing, AGC, data processing         │\n└─────────────────────────────────────────────────┘\n```\n\n**LED Drive:**\n- Current DAC per LED (0-50mA, 8-bit resolution)\n- Pulsed operation: 100-500µs ON time\n- Sequence: Ambient → Red → Ambient → IR → Ambient → Green\n- Total cycle: 20ms (50Hz sampling)\n\n**Timing Diagram:**\n```\nTime→  |--AMB--|--RED--|--AMB--|--IR---|--AMB--|--GRN--|\n       |  1ms  | 0.5ms |  1ms  | 0.5ms |  1ms  | 0.5ms |\n       |       |       |       |       |       |       |\nSample:   ↑        ↑       ↑       ↑       ↑       ↑\n          S1       S2      S3      S4      S5      S6\n```\n\n**Ambient Light Cancellation:**\n- Sample with LED OFF before each LED measurement\n- Subtract: Signal = LED_ON - LED_OFF\n- Removes DC ambient and 50/60Hz flicker\n\n**Dynamic Range Handling (100:1):**\n\n1. **Programmable Gain Amplifier (PGA):**\n   - Gains: ×1, ×2, ×4, ×8, ×16\n   - TIA fixed at 500kΩ\n   - PGA adjusts based on signal level\n\n2. **LED Current AGC:**\n   - Start at mid-current (25mA)\n   - Adjust based on signal amplitude\n   - Target: 70% of ADC range\n\n3. **ADC Requirements:**\n   - 16-bit for headroom\n   - Sample rate: >200ksps (for 6 channels × 50Hz × oversampling)\n   - Sigma-Delta recommended for low noise\n\n**Component Selection:**\n- AFE IC: MAX86146 (integrated, optimized for PPG)\n- Or discrete: OPA2140 (TIA), AD8253 (PGA), ADS1298 (ADC)\n- LED Driver: LP55231 or discrete MOSFETs",
          "follow_up": "How would you calibrate this system for different skin tones to ensure consistent signal quality?",
          "follow_up_answer": "Skin tone calibration: 1) Initial calibration sequence on device attach, 2) Sweep LED current and measure response, 3) Select current giving target SNR or amplitude, 4) Store per-wavelength calibration, 5) Run periodically to adapt to changes, 6) Use Green/Red ratio as skin tone indicator (Green more affected by melanin), 7) Adjust algorithm parameters based on detected skin type.",
          "hints": [
            "Think about time-multiplexing the LEDs",
            "Consider how to handle the large dynamic range"
          ]
        },
        {
          "id": "ee_6",
          "title": "Debugging Methodology",
          "category": "debugging",
          "difficulty": [
            "medium"
          ],
          "question_text": "You have a prototype wearable with an intermittent issue: the PPG signal occasionally shows large noise spikes that don't correlate with motion. The issue appears randomly, maybe once every few hours. How would you systematically debug this?",
          "rubric": {
            "methodology": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes systematic debugging approach",
                  "points": 1
                },
                {
                  "description": "Identifies likely categories of causes",
                  "points": 1
                },
                {
                  "description": "Discusses logging/monitoring strategy",
                  "points": 1
                },
                {
                  "description": "Considers environmental factors",
                  "points": 1
                }
              ]
            },
            "tools_techniques": {
              "points": 3,
              "criteria": [
                {
                  "description": "Mentions appropriate test equipment",
                  "points": 1
                },
                {
                  "description": "Proposes signal correlation methods",
                  "points": 1
                },
                {
                  "description": "Discusses isolation techniques",
                  "points": 1
                }
              ]
            },
            "root_cause_analysis": {
              "points": 3,
              "criteria": [
                {
                  "description": "Lists plausible root causes",
                  "points": 1
                },
                {
                  "description": "Proposes verification methods",
                  "points": 1
                },
                {
                  "description": "Discusses fix validation",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Intermittent Noise Debugging:**\n\n**Debugging Methodology:**\n\n1. **Characterize the Issue:**\n   - Log raw ADC data continuously with timestamps\n   - Capture spikes with automated detection\n   - Document: frequency, duration, amplitude, waveform shape\n   - Check correlation with other signals (accelerometer, temperature)\n\n2. **Categorize Potential Causes:**\n   - **Electrical:** Power supply transients, digital crosstalk, ESD\n   - **Mechanical:** Connector intermittent, solder joint crack\n   - **Environmental:** RF interference, temperature effects\n   - **Software:** DMA overrun, interrupt collision, buffer overflow\n\n3. **Systematic Isolation:**\n\n   a) **Power Supply:**\n   - Monitor all supply rails with scope\n   - Capture during spike event\n   - Add large capacitors temporarily\n\n   b) **Digital Interference:**\n   - Log MCU activity (BLE TX, flash write)\n   - Correlate with noise events\n   - Disable subsystems one at a time\n\n   c) **Mechanical:**\n   - Flex board, wiggle connectors while monitoring\n   - Thermal cycling (heat gun/freeze spray)\n   - Visual inspection with microscope\n\n   d) **Software:**\n   - Add watchpoints on buffer boundaries\n   - Log interrupt timing\n   - Review DMA configuration\n\n**Test Setup:**\n```\nPrototype → Scope (4ch) → Long-term capture\n            ├── CH1: PPG signal\n            ├── CH2: Power supply\n            ├── CH3: MCU GPIO toggle (activity indicator)\n            └── CH4: BLE activity\n\n+ Logic analyzer on SPI/I2C buses\n+ Automated trigger on spike detection\n```\n\n**Likely Causes (ranked by probability):**\n1. BLE TX causing supply droop\n2. Flash write accessing same bus as ADC\n3. Marginal solder joint on analog section\n4. External RF interference\n5. ESD event from user contact\n\n**Verification:**\n- Once identified, must reproduce reliably\n- Implement fix and run extended soak test (48+ hours)\n- Statistical comparison: spike rate before/after",
          "follow_up": "You determine the spikes correlate with BLE transmission. How would you fix this without significantly impacting BLE performance?",
          "follow_up_answer": "BLE-PPG interference fix: 1) Add bulk capacitor (100µF+) on supply near BLE, 2) Separate power planes with ferrite bead, 3) Schedule PPG sampling between BLE TX bursts (connection interval aware), 4) Use BLE connection parameters to create predictable TX windows, 5) Digital filtering to blank known BLE periods, 6) LDO as post-regulator for analog supply, 7) Shield analog section from BLE antenna.",
          "hints": [
            "Consider what makes intermittent issues hard to debug",
            "Think about what could cause random noise spikes"
          ]
        },
        {
          "id": "ee_7",
          "title": "Firmware-Hardware Interface",
          "category": "system_design",
          "difficulty": [
            "medium"
          ],
          "question_text": "You're specifying the hardware-software interface for a PPG sensor IC. What registers and control mechanisms would you define? How would you handle the LED timing, gain control, and data readout?",
          "rubric": {
            "register_design": {
              "points": 4,
              "criteria": [
                {
                  "description": "Defines appropriate register set",
                  "points": 1
                },
                {
                  "description": "Considers read/write access",
                  "points": 1
                },
                {
                  "description": "Includes status/interrupt registers",
                  "points": 1
                },
                {
                  "description": "Addresses calibration data",
                  "points": 1
                }
              ]
            },
            "timing_control": {
              "points": 3,
              "criteria": [
                {
                  "description": "Specifies LED timing parameters",
                  "points": 1
                },
                {
                  "description": "Discusses synchronization mechanisms",
                  "points": 1
                },
                {
                  "description": "Addresses sample rate configuration",
                  "points": 1
                }
              ]
            },
            "data_interface": {
              "points": 3,
              "criteria": [
                {
                  "description": "Proposes data readout method (FIFO, DMA)",
                  "points": 1
                },
                {
                  "description": "Discusses interrupt strategy",
                  "points": 1
                },
                {
                  "description": "Considers power modes",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**PPG Sensor IC Hardware-Software Interface:**\n\n**Communication:** I2C (standard mode 100kHz, fast mode 400kHz)\n**Address:** 0x57 (7-bit)\n\n**Register Map:**\n```\nAddr  Name              Access  Description\n────────────────────────────────────────────────\n0x00  STATUS            R       Interrupt status, data ready\n0x01  INT_ENABLE        R/W     Interrupt enable mask\n0x02  INT_CLEAR         W       Write to clear interrupts\n\n0x10  MODE_CONFIG       R/W     Operating mode, sample rate\n0x11  LED_CONFIG        R/W     Active LEDs, sequence\n0x12  LED1_CURRENT      R/W     Red LED current (0-50mA)\n0x13  LED2_CURRENT      R/W     IR LED current\n0x14  LED3_CURRENT      R/W     Green LED current\n0x15  LED_PULSE_WIDTH   R/W     LED on-time (50-500µs)\n\n0x20  PD1_GAIN          R/W     Photodiode 1 PGA gain\n0x21  PD2_GAIN          R/W     Photodiode 2 PGA gain\n0x22  ADC_CONFIG        R/W     Resolution, averaging\n\n0x30  FIFO_CONFIG       R/W     FIFO enable, watermark\n0x31  FIFO_WR_PTR       R       FIFO write pointer\n0x32  FIFO_RD_PTR       R/W     FIFO read pointer\n0x33  FIFO_COUNT        R       Samples in FIFO\n0x34  FIFO_DATA         R       FIFO data readout\n\n0x40  CALIB_DATA_0      R/W     Calibration coefficients\n...\n0x4F  CALIB_DATA_15     R/W     \n\n0xFE  DEVICE_ID         R       Fixed ID for identification\n0xFF  REVISION          R       Silicon revision\n```\n\n**Operating Modes (MODE_CONFIG):**\n- Shutdown: <1µA, all off\n- Standby: Configuration active, no sampling\n- Continuous: Free-running at configured rate\n- One-shot: Single sample on trigger\n\n**LED Timing Control:**\n```\nLED_CONFIG bits:\n[7:6] - Sequence mode (single/dual/triple)\n[5:4] - LED1 slot assignment\n[3:2] - LED2 slot assignment\n[1:0] - LED3 slot assignment\n\nTiming derived from sample rate:\nSample Rate | LED Pulse | Integration\n25 Hz       | 500µs     | 400µs\n50 Hz       | 200µs     | 150µs\n100 Hz      | 100µs     | 75µs\n```\n\n**Data Readout:**\n- FIFO holds 32 samples (6 channels × 24-bit = 18 bytes/sample)\n- Interrupt on watermark (configurable 1-32)\n- Burst read from FIFO_DATA register\n- Auto-increment read pointer\n\n**Interrupt Strategy:**\n- FIFO almost full (watermark)\n- New sample ready\n- Ambient light overflow\n- Temperature threshold",
          "follow_up": "How would you design the power-on sequence and reset behavior for reliable operation?",
          "follow_up_answer": "Power-on/reset design: 1) POR circuit with voltage threshold, 2) Defined reset state for all registers, 3) Configuration lock bit to prevent accidental changes, 4) Soft reset command via register write, 5) Boot time specification (<10ms to ready), 6) Brownout detection with interrupt, 7) FIFO flush on mode change, 8) State machine diagram in datasheet, 9) Recommended initialization sequence in documentation.",
          "hints": [
            "Think about what a firmware developer needs to control the hardware",
            "Consider how to make the interface robust and easy to use"
          ]
        }
      ]
    },
    "firmware_engineer": {
      "track_id": "firmware_engineer",
      "title": "Firmware Engineer",
      "description": "Firmware Engineer assessment focusing on RTOS, memory management, interrupts, power optimization, and communication protocols",
      "keywords": [
        "firmware",
        "embedded",
        "RTOS",
        "FreeRTOS",
        "C",
        "microcontroller",
        "ARM",
        "Cortex",
        "interrupt",
        "DMA",
        "SPI",
        "I2C",
        "UART",
        "BLE"
      ],
      "technical_questions": [
        {
          "id": "fw_1",
          "title": "RTOS and Task Scheduling",
          "category": "rtos",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You're designing a wearable device with FreeRTOS. The system has these tasks: PPG sampling (100Hz, hard real-time), BLE communication (soft real-time), UI updates (low priority), and data logging to flash. How would you structure your tasks and handle priority inversion?",
          "rubric": {
            "task_design": {
              "points": 4,
              "criteria": [
                {
                  "description": "Assigns appropriate priorities",
                  "points": 1
                },
                {
                  "description": "Identifies real-time requirements",
                  "points": 1
                },
                {
                  "description": "Discusses task communication",
                  "points": 1
                },
                {
                  "description": "Considers stack sizes",
                  "points": 1
                }
              ]
            },
            "priority_inversion": {
              "points": 4,
              "criteria": [
                {
                  "description": "Explains priority inversion problem",
                  "points": 1
                },
                {
                  "description": "Discusses mutex with priority inheritance",
                  "points": 1
                },
                {
                  "description": "Proposes alternative solutions",
                  "points": 1
                },
                {
                  "description": "Addresses deadlock prevention",
                  "points": 1
                }
              ]
            },
            "practical_considerations": {
              "points": 2,
              "criteria": [
                {
                  "description": "Considers CPU utilization",
                  "points": 1
                },
                {
                  "description": "Addresses timing analysis",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**FreeRTOS Task Design:**\n\n**Task Structure:**\n```c\n// Priority: Higher number = higher priority\nTask              Priority  Period    Stack   Notes\n─────────────────────────────────────────────────────\nPPG_Sampling      5 (High)  10ms      512B    Hard RT, ISR+task\nBLE_Handler       4         Event     1024B   Soft RT\nData_Processing   3         50ms      2048B   CPU intensive\nFlash_Logger      2         100ms     512B    I/O bound\nUI_Update         1 (Low)   200ms     512B    Background\nIdle              0         -         128B    Power management\n```\n\n**PPG Sampling (Hard Real-Time):**\n```c\n// Use timer ISR for precise timing\nvoid TIM2_IRQHandler(void) {\n    BaseType_t xHigherPriorityTaskWoken = pdFALSE;\n    \n    // Trigger ADC (hardware), minimal ISR work\n    ADC_StartConversion();\n    \n    // Signal task to process\n    xSemaphoreGiveFromISR(ppgSemaphore, &xHigherPriorityTaskWoken);\n    portYIELD_FROM_ISR(xHigherPriorityTaskWoken);\n}\n\nvoid PPG_Task(void *pvParameters) {\n    while(1) {\n        xSemaphoreTake(ppgSemaphore, portMAX_DELAY);\n        // Read ADC, basic filtering, queue to processing\n        xQueueSend(dataQueue, &sample, 0);\n    }\n}\n```\n\n**Priority Inversion Problem:**\n- Occurs when high-priority task waits for resource held by low-priority task\n- Medium-priority task preempts low-priority, blocking high-priority indefinitely\n\n**Solutions:**\n\n1. **Priority Inheritance Mutex:**\n```c\nSemaphoreHandle_t flashMutex;\nflashMutex = xSemaphoreCreateMutex();  // Has priority inheritance\n\n// When high-priority task blocks on mutex held by low-priority,\n// low-priority temporarily inherits high priority\n```\n\n2. **Priority Ceiling Protocol:**\n- Set mutex ceiling to highest priority task that uses it\n- Task acquiring mutex runs at ceiling priority\n\n3. **Design Alternatives:**\n- Minimize shared resources\n- Use lock-free queues where possible\n- Dedicate peripherals to single tasks\n\n**Deadlock Prevention:**\n- Always acquire mutexes in same order\n- Use timeout on mutex acquire\n- Avoid holding multiple mutexes\n- Use binary semaphores for signaling (not mutexes)",
          "follow_up": "How would you measure and verify that your PPG task is meeting its 10ms deadline in the deployed system?",
          "follow_up_answer": "Deadline verification: 1) Toggle GPIO at task start/end, measure with scope, 2) Use FreeRTOS trace hooks (traceTASK_SWITCHED_IN), 3) Log timestamp deltas to buffer, 4) Calculate jitter statistics, 5) Set up watchdog to catch deadline misses, 6) Use SEGGER SystemView for real-time tracing, 7) Worst-case analysis during stress testing (all tasks active).",
          "hints": [
            "Consider what happens when tasks share resources",
            "Think about ISR vs task partitioning"
          ]
        },
        {
          "id": "fw_2",
          "title": "Memory Management",
          "category": "memory",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You're developing firmware for a Cortex-M4 with 256KB RAM. Describe your memory management strategy including stack allocation, heap usage, and techniques to prevent memory-related bugs. How would you handle a scenario where you need to buffer 30 seconds of PPG data?",
          "rubric": {
            "memory_architecture": {
              "points": 4,
              "criteria": [
                {
                  "description": "Describes memory map layout",
                  "points": 1
                },
                {
                  "description": "Discusses static vs dynamic allocation",
                  "points": 1
                },
                {
                  "description": "Addresses stack overflow prevention",
                  "points": 1
                },
                {
                  "description": "Considers memory fragmentation",
                  "points": 1
                }
              ]
            },
            "buffer_design": {
              "points": 3,
              "criteria": [
                {
                  "description": "Calculates buffer size requirements",
                  "points": 1
                },
                {
                  "description": "Proposes circular buffer or similar",
                  "points": 1
                },
                {
                  "description": "Addresses concurrent access",
                  "points": 1
                }
              ]
            },
            "debugging_tools": {
              "points": 3,
              "criteria": [
                {
                  "description": "Mentions stack watermarking",
                  "points": 1
                },
                {
                  "description": "Discusses memory analysis tools",
                  "points": 1
                },
                {
                  "description": "Proposes runtime checks",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Memory Management Strategy:**\n\n**Memory Map (256KB RAM):**\n```\n0x20000000 ┌─────────────────────┐\n           │  Vector Table       │  256B\n           ├─────────────────────┤\n           │  .data (initialized)│  ~4KB\n           ├─────────────────────┤\n           │  .bss (zero-init)   │  ~8KB\n           ├─────────────────────┤\n           │  PPG Buffer (static)│  60KB (30s @ 100Hz × 4ch × 2B × 2.5x)\n           ├─────────────────────┤\n           │  FreeRTOS Heap      │  32KB (heap_4.c)\n           ├─────────────────────┤\n           │  Task Stacks        │  ~16KB (allocated from heap)\n           ├─────────────────────┤\n           │  DMA Buffers        │  4KB\n           ├─────────────────────┤\n           │  Free               │  ~132KB\n           ├─────────────────────┤\n           │  Main Stack (MSP)   │  2KB\n0x2003FFFF └─────────────────────┘\n```\n\n**Allocation Strategy:**\n\n1. **Static Allocation (Preferred):**\n```c\n// Large buffers allocated statically\nstatic PPGSample_t ppgBuffer[PPG_BUFFER_SIZE] __attribute__((section(\".ppg_data\")));\n```\n\n2. **FreeRTOS Heap:**\n- Use heap_4.c (coalesces free blocks)\n- Configure: configTOTAL_HEAP_SIZE = 32768\n- Allocate tasks, queues, semaphores at init only\n- No runtime malloc/free after initialization\n\n3. **Stack Protection:**\n```c\n// Enable MPU stack guards\nvoid vApplicationStackOverflowHook(TaskHandle_t xTask, char *pcTaskName) {\n    // Log error, trigger safe reset\n    fault_handler(FAULT_STACK_OVERFLOW, pcTaskName);\n}\n\n// Stack watermarking\nuxTaskGetStackHighWaterMark(NULL);  // Check remaining stack\n```\n\n**30-Second PPG Buffer:**\n```c\n// Requirements: 100Hz × 4 channels × 2 bytes × 30s = 24KB\n// With overhead: 30KB circular buffer\n\ntypedef struct {\n    volatile uint32_t head;\n    volatile uint32_t tail;\n    uint16_t data[PPG_SAMPLES_30S][4];  // 30000 samples × 4 channels\n} PPGRingBuffer_t;\n\n// Lock-free SPSC (single producer, single consumer)\nbool ppg_buffer_write(PPGRingBuffer_t *buf, uint16_t *sample) {\n    uint32_t next = (buf->head + 1) % PPG_SAMPLES_30S;\n    if (next == buf->tail) return false;  // Full\n    memcpy(buf->data[buf->head], sample, 8);\n    __DMB();  // Memory barrier\n    buf->head = next;\n    return true;\n}\n```\n\n**Bug Prevention:**\n- Static analysis (PC-lint, Polyspace)\n- Runtime bounds checking in debug builds\n- Canary values at buffer boundaries\n- MPU configuration for stack protection",
          "follow_up": "How would you detect and handle a memory leak in a long-running embedded system?",
          "follow_up_answer": "Memory leak detection: 1) Track heap high-water mark over time (xPortGetFreeHeapSize), 2) Periodic heap integrity check (vPortGetHeapStats), 3) Custom malloc wrapper with allocation tracking, 4) Log all allocations with file/line info, 5) Watchdog task monitoring free heap, 6) Design: no dynamic allocation after init eliminates leak possibility, 7) If leak detected: safe restart with error logging.",
          "hints": [
            "Consider the difference between static and dynamic allocation",
            "Think about how to make the system deterministic"
          ]
        },
        {
          "id": "fw_3",
          "title": "Interrupt Handling",
          "category": "interrupts",
          "difficulty": [
            "medium"
          ],
          "question_text": "Design the interrupt architecture for a wearable device acquiring PPG (via ADC), ECG (via SPI), accelerometer data (via I2C), with BLE communication. How would you prioritize interrupts and handle the data flow?",
          "rubric": {
            "priority_assignment": {
              "points": 4,
              "criteria": [
                {
                  "description": "Assigns logical interrupt priorities",
                  "points": 1
                },
                {
                  "description": "Understands NVIC priority grouping",
                  "points": 1
                },
                {
                  "description": "Considers latency requirements",
                  "points": 1
                },
                {
                  "description": "Addresses interrupt nesting",
                  "points": 1
                }
              ]
            },
            "isr_design": {
              "points": 3,
              "criteria": [
                {
                  "description": "Minimizes ISR execution time",
                  "points": 1
                },
                {
                  "description": "Uses DMA appropriately",
                  "points": 1
                },
                {
                  "description": "Discusses deferred processing",
                  "points": 1
                }
              ]
            },
            "data_flow": {
              "points": 3,
              "criteria": [
                {
                  "description": "Proposes buffering strategy",
                  "points": 1
                },
                {
                  "description": "Handles concurrent access",
                  "points": 1
                },
                {
                  "description": "Considers data loss prevention",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Interrupt Architecture Design:**\n\n**NVIC Priority Configuration (Cortex-M4):**\n```c\n// Using 4 priority bits, preempt/subpriority = 3/1\nNVIC_SetPriorityGrouping(NVIC_PRIORITYGROUP_4);\n\n// Priority 0 = highest, 15 = lowest\nInterrupt          Priority  Preempt  Notes\n─────────────────────────────────────────────\nSysTick            15        15       RTOS tick (1ms)\nBLE_Radio          2         2        Time-critical RF\nDMA1_Stream0       3         3        ADC PPG complete\nSPI1 (ECG)         4         4        ECG data ready\nI2C1 (Accel)       5         5        Accelerometer\nTimer2 (PPG trig)  3         3        ADC trigger\nUSART (Debug)      14        14       Non-critical\n```\n\n**ISR Design Principles:**\n\n1. **Minimal ISR Work:**\n```c\nvoid DMA1_Stream0_IRQHandler(void) {\n    if (DMA1->LISR & DMA_LISR_TCIF0) {\n        DMA1->LIFCR = DMA_LIFCR_CTCIF0;  // Clear flag\n        \n        // Swap ping-pong buffers\n        ppg_active_buffer ^= 1;\n        DMA1_Stream0->M0AR = (uint32_t)ppg_buffers[ppg_active_buffer];\n        \n        // Signal task (no processing in ISR)\n        BaseType_t xHigherPriorityTaskWoken = pdFALSE;\n        xSemaphoreGiveFromISR(ppgDataReady, &xHigherPriorityTaskWoken);\n        portYIELD_FROM_ISR(xHigherPriorityTaskWoken);\n    }\n}\n```\n\n2. **DMA Configuration:**\n```c\n// PPG: ADC → DMA → Memory (no CPU involvement)\nDMA_Config:\n  - Circular mode with double buffer\n  - Transfer complete interrupt\n  - 4 channels × 16-bit = 8 bytes per sample\n  - 100 samples per DMA completion = 1 second batches\n\n// ECG: SPI RX → DMA → Memory\n// Accelerometer: I2C too slow for DMA benefit, use ISR\n```\n\n**Data Flow Architecture:**\n```\n                    ┌─────────────────┐\nADC ──DMA──────────►│ PPG Buffer[0]   │──┐\n                    │ PPG Buffer[1]   │──┼──► Processing Task\n                    └─────────────────┘  │\n                                         │\nSPI ──DMA──────────►│ ECG Buffer[0]  │───┤\n       (ECG)        │ ECG Buffer[1]  │───┘\n                    └────────────────┘\n                                         \nI2C ──ISR──────────►│ Accel Queue    │───► Processing Task\n                    └────────────────┘\n```\n\n**Critical Sections:**\n```c\n// Use BASEPRI for selective masking (not PRIMASK)\ntaskENTER_CRITICAL_FROM_ISR();  // Masks below configMAX_SYSCALL_INTERRUPT_PRIORITY\n// Access shared resource\ntaskEXIT_CRITICAL_FROM_ISR();\n```\n\n**Interrupt Latency Targets:**\n- BLE Radio: <10µs (handled by BLE stack)\n- PPG DMA: <50µs (just buffer swap)\n- ECG SPI: <100µs\n- Total CPU in ISR: <5% of CPU time",
          "follow_up": "What would happen if the BLE interrupt takes too long and causes the PPG DMA to miss its deadline?",
          "follow_up_answer": "Priority inversion at interrupt level: 1) If BLE blocks PPG DMA completion ISR, data could be overwritten before buffer swap, 2) Solution: BLE should be lower priority than sensor acquisition, or BLE ISR should be split into critical (very short) and deferrable parts, 3) Use DMA double buffering to provide margin, 4) Monitor for overruns with counter, 5) In worst case, drop BLE packet rather than lose sensor data.",
          "hints": [
            "Consider how DMA can offload the CPU",
            "Think about the relationship between ISR and task processing"
          ]
        },
        {
          "id": "fw_4",
          "title": "Power Management",
          "category": "power",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "Design the power management firmware for a battery-powered wearable that needs 7-day battery life. The MCU is a Cortex-M4 with multiple sleep modes. Describe your sleep mode strategy, wake sources, and peripheral power control.",
          "rubric": {
            "sleep_strategy": {
              "points": 4,
              "criteria": [
                {
                  "description": "Identifies appropriate sleep modes",
                  "points": 1
                },
                {
                  "description": "Discusses wake source configuration",
                  "points": 1
                },
                {
                  "description": "Addresses transition latency",
                  "points": 1
                },
                {
                  "description": "Considers peripheral state",
                  "points": 1
                }
              ]
            },
            "implementation": {
              "points": 3,
              "criteria": [
                {
                  "description": "Proposes state machine approach",
                  "points": 1
                },
                {
                  "description": "Handles wake-up sequence",
                  "points": 1
                },
                {
                  "description": "Addresses clock configuration",
                  "points": 1
                }
              ]
            },
            "power_optimization": {
              "points": 3,
              "criteria": [
                {
                  "description": "Discusses peripheral power gating",
                  "points": 1
                },
                {
                  "description": "Proposes duty cycling strategy",
                  "points": 1
                },
                {
                  "description": "Considers leakage current",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Power Management Firmware Design:**\n\n**Sleep Mode Analysis (STM32L4 example):**\n```\nMode        Current   Wake Time   Wake Sources         Use Case\n─────────────────────────────────────────────────────────────────\nRun         10mA      -           -                    Active processing\nSleep       2mA       <1µs        Any interrupt        Short idle\nLow-Power   100µA     <100µs      EXTI, RTC, LPUART   Medium idle\nStop2       1µA       <50µs       EXTI, RTC, LPTIM    Long idle, retain RAM\nStandby     300nA     ~1ms        WKUP pins, RTC      Deep sleep, lose RAM\nShutdown    30nA      ~2ms        WKUP pins           Off\n```\n\n**Power State Machine:**\n```c\ntypedef enum {\n    PWR_STATE_ACTIVE,      // Full processing\n    PWR_STATE_IDLE,        // Between samples, Sleep mode\n    PWR_STATE_LOW_POWER,   // No BLE, sensor sampling only\n    PWR_STATE_STANDBY      // User not wearing, periodic check\n} PowerState_t;\n\nvoid power_manager_task(void *pvParameters) {\n    while(1) {\n        switch(current_power_state) {\n            case PWR_STATE_ACTIVE:\n                // Run at full speed\n                if (idle_time > IDLE_THRESHOLD) {\n                    transition_to(PWR_STATE_IDLE);\n                }\n                break;\n                \n            case PWR_STATE_IDLE:\n                // Enter Sleep between RTOS ticks\n                // Wake on any interrupt\n                __WFI();  // Wait For Interrupt\n                break;\n                \n            case PWR_STATE_LOW_POWER:\n                // Disable BLE, reduce sample rate\n                enter_stop2_mode();\n                // Wake by LPTIM (sensor sampling) or accelerometer\n                break;\n        }\n    }\n}\n```\n\n**Peripheral Power Control:**\n```c\nvoid enter_low_power_mode(void) {\n    // Disable unused peripherals\n    __HAL_RCC_SPI1_CLK_DISABLE();\n    __HAL_RCC_I2C1_CLK_DISABLE();\n    __HAL_RCC_ADC_CLK_DISABLE();\n    \n    // Configure GPIO to minimize leakage\n    GPIO_InitTypeDef GPIO_InitStruct = {0};\n    GPIO_InitStruct.Mode = GPIO_MODE_ANALOG;\n    GPIO_InitStruct.Pull = GPIO_NOPULL;\n    HAL_GPIO_Init(GPIOA, &GPIO_InitStruct);  // All unused pins\n    \n    // Power down external sensors\n    sensor_power_enable(false);\n    \n    // Configure wake sources\n    HAL_RTCEx_SetWakeUpTimer_IT(&hrtc, WAKE_PERIOD, RTC_WAKEUPCLOCK_RTCCLK_DIV16);\n    \n    // Enter Stop2\n    HAL_PWREx_EnterSTOP2Mode(PWR_STOPENTRY_WFI);\n    \n    // --- WAKE UP HERE ---\n    SystemClock_Config();  // Restore clocks\n    restore_peripherals();\n}\n```\n\n**Duty Cycle Strategy:**\n```\nNormal Operation (user active):\n├── Sample PPG: 1s every 10s (10% duty)\n├── BLE advertise: 20ms every 1s (2% duty)\n├── Process data: 50ms every 10s (0.5% duty)\n└── Average current: ~500µA\n\nLow Power (device idle):\n├── Check accelerometer: 100ms every 60s\n├── No BLE, no PPG\n└── Average current: ~10µA\n```\n\n**Power Budget Verification:**\n```\n7 days = 168 hours\n100mAh battery\nTarget average: 100mAh / 168h = 0.6mA\n\nMeasure with power analyzer (µCurrent Gold or similar)\nTrack current in each state and duty cycle\n```",
          "follow_up": "How would you handle the case where the device needs to wake up to receive an incoming BLE connection while in deep sleep?",
          "follow_up_answer": "BLE wake from deep sleep: 1) Use BLE SoC's built-in low-power advertising (nRF52: System ON with radio wake), 2) Configure periodic advertising with long interval, 3) Use sub-GHz wake-up radio for longer range wake, 4) Accelerometer interrupt as proxy for 'user picked up device', 5) Trade-off: deeper sleep = longer connection latency, 6) Implement connection request queue that's processed on next scheduled wake.",
          "hints": [
            "Consider the trade-off between sleep depth and wake-up time",
            "Think about what needs to stay powered in each mode"
          ]
        },
        {
          "id": "fw_5",
          "title": "Communication Protocols",
          "category": "protocols",
          "difficulty": [
            "medium"
          ],
          "question_text": "You need to interface with three sensors: a PPG AFE (SPI, 4MHz), an ECG chip (SPI, 8MHz), and an accelerometer (I2C, 400kHz). Design the communication architecture including bus sharing, DMA usage, and error handling.",
          "rubric": {
            "bus_architecture": {
              "points": 4,
              "criteria": [
                {
                  "description": "Decides on shared vs separate buses",
                  "points": 1
                },
                {
                  "description": "Addresses chip select management",
                  "points": 1
                },
                {
                  "description": "Considers bus contention",
                  "points": 1
                },
                {
                  "description": "Discusses clock configuration",
                  "points": 1
                }
              ]
            },
            "dma_strategy": {
              "points": 3,
              "criteria": [
                {
                  "description": "Identifies which transfers benefit from DMA",
                  "points": 1
                },
                {
                  "description": "Discusses DMA channel allocation",
                  "points": 1
                },
                {
                  "description": "Addresses synchronization",
                  "points": 1
                }
              ]
            },
            "error_handling": {
              "points": 3,
              "criteria": [
                {
                  "description": "Proposes timeout handling",
                  "points": 1
                },
                {
                  "description": "Discusses bus recovery procedures",
                  "points": 1
                },
                {
                  "description": "Addresses data integrity verification",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Communication Architecture Design:**\n\n**Bus Topology:**\n```\n┌─────────┐     SPI1 (8MHz)     ┌─────────┐\n│         │◄───────────────────►│ ECG AFE │\n│         │     CS_ECG          └─────────┘\n│         │\n│   MCU   │     SPI2 (4MHz)     ┌─────────┐\n│         │◄───────────────────►│ PPG AFE │\n│         │     CS_PPG          └─────────┘\n│         │\n│         │     I2C1 (400kHz)   ┌─────────┐\n│         │◄───────────────────►│ Accel   │\n└─────────┘                     └─────────┘\n```\n\n**Design Decision: Separate SPI Buses**\n- ECG and PPG have different speed requirements\n- Avoids CS arbitration complexity\n- Allows simultaneous DMA transfers\n- Trade-off: Uses 2 SPI peripherals (usually acceptable)\n\n**SPI Configuration:**\n```c\n// ECG SPI - High speed for high sample rate\nspi_ecg_config = {\n    .BaudRatePrescaler = SPI_BAUDRATEPRESCALER_8,  // 64MHz/8 = 8MHz\n    .DataSize = SPI_DATASIZE_16BIT,\n    .CLKPolarity = SPI_POLARITY_LOW,\n    .CLKPhase = SPI_PHASE_2EDGE,  // Per ECG AFE datasheet\n};\n\n// PPG SPI - Moderate speed\nspi_ppg_config = {\n    .BaudRatePrescaler = SPI_BAUDRATEPRESCALER_16, // 64MHz/16 = 4MHz\n    .DataSize = SPI_DATASIZE_8BIT,\n    .CLKPolarity = SPI_POLARITY_HIGH,\n    .CLKPhase = SPI_PHASE_1EDGE,\n};\n```\n\n**DMA Strategy:**\n```c\n// ECG: High data rate, definitely use DMA\nDMA1_Stream3 → SPI1_TX\nDMA1_Stream0 → SPI1_RX\n// Transfer: 24-bit sample × 8 channels = 24 bytes @ 500Hz\n\n// PPG: Moderate rate, DMA beneficial\nDMA1_Stream4 → SPI2_TX  \nDMA1_Stream1 → SPI2_RX\n// Transfer: 20 bytes register read @ 100Hz\n\n// Accelerometer: Low rate, interrupt-driven sufficient\n// I2C DMA adds complexity for small transfers\n// Use blocking with timeout in dedicated task\n```\n\n**Error Handling:**\n```c\ntypedef struct {\n    uint32_t timeout_count;\n    uint32_t crc_error_count;\n    uint32_t bus_error_count;\n    uint32_t last_error_time;\n} SensorErrorStats_t;\n\nHAL_StatusTypeDef read_ecg_with_recovery(uint8_t *data, uint16_t len) {\n    HAL_StatusTypeDef status;\n    int retries = 3;\n    \n    while (retries--) {\n        status = HAL_SPI_TransmitReceive_DMA(&hspi1, cmd, data, len);\n        \n        // Wait with timeout\n        if (xSemaphoreTake(ecgDmaDone, pdMS_TO_TICKS(100)) == pdTRUE) {\n            if (verify_ecg_checksum(data, len)) {\n                return HAL_OK;\n            }\n            ecg_stats.crc_error_count++;\n        } else {\n            ecg_stats.timeout_count++;\n            // Bus recovery\n            HAL_SPI_Abort(&hspi1);\n            spi_bus_reset(&hspi1);\n        }\n    }\n    \n    // Sensor failure - log and notify\n    log_sensor_failure(SENSOR_ECG, ecg_stats);\n    return HAL_ERROR;\n}\n\nvoid spi_bus_reset(SPI_HandleTypeDef *hspi) {\n    HAL_SPI_DeInit(hspi);\n    HAL_Delay(1);\n    HAL_SPI_Init(hspi);\n}\n```\n\n**I2C Bus Recovery:**\n```c\nvoid i2c_bus_recovery(void) {\n    // Bit-bang 9 clocks to release stuck slave\n    GPIO_InitTypeDef GPIO_InitStruct = {0};\n    \n    // Configure SCL as output\n    GPIO_InitStruct.Pin = I2C_SCL_PIN;\n    GPIO_InitStruct.Mode = GPIO_MODE_OUTPUT_OD;\n    HAL_GPIO_Init(I2C_PORT, &GPIO_InitStruct);\n    \n    for (int i = 0; i < 9; i++) {\n        HAL_GPIO_WritePin(I2C_PORT, I2C_SCL_PIN, GPIO_PIN_RESET);\n        delay_us(5);\n        HAL_GPIO_WritePin(I2C_PORT, I2C_SCL_PIN, GPIO_PIN_SET);\n        delay_us(5);\n    }\n    \n    // Reinitialize I2C\n    HAL_I2C_Init(&hi2c1);\n}\n```",
          "follow_up": "How would you handle the situation where you need to add a fourth sensor but have run out of SPI peripherals?",
          "follow_up_answer": "Adding sensor with limited SPI: 1) Share SPI bus with CS arbitration (mutex protected), 2) Use software SPI (bit-bang) for slow sensor, 3) Use I2C if sensor supports it, 4) SPI-to-I2C bridge IC, 5) Evaluate if existing sensor can be removed/combined, 6) Consider MCU upgrade if design phase allows, 7) Time-multiplex with careful scheduling if sensors have different duty cycles.",
          "hints": [
            "Consider the data rate requirements of each sensor",
            "Think about when DMA provides real benefit"
          ]
        },
        {
          "id": "fw_6",
          "title": "Debugging Techniques",
          "category": "debugging",
          "difficulty": [
            "medium"
          ],
          "question_text": "You're debugging a wearable device that occasionally freezes after running for several hours. The watchdog doesn't trigger, suggesting the main loop is still running. What's your systematic approach to identifying and fixing this issue?",
          "rubric": {
            "diagnosis_approach": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes systematic debugging methodology",
                  "points": 1
                },
                {
                  "description": "Identifies likely categories of issues",
                  "points": 1
                },
                {
                  "description": "Discusses logging/instrumentation",
                  "points": 1
                },
                {
                  "description": "Considers timing-related issues",
                  "points": 1
                }
              ]
            },
            "tools_techniques": {
              "points": 3,
              "criteria": [
                {
                  "description": "Mentions appropriate debugging tools",
                  "points": 1
                },
                {
                  "description": "Proposes memory analysis approaches",
                  "points": 1
                },
                {
                  "description": "Discusses trace/profiling methods",
                  "points": 1
                }
              ]
            },
            "resolution": {
              "points": 3,
              "criteria": [
                {
                  "description": "Lists plausible root causes",
                  "points": 1
                },
                {
                  "description": "Proposes prevention strategies",
                  "points": 1
                },
                {
                  "description": "Discusses validation approach",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Systematic Debug Approach:**\n\n**Characterize the Symptom:**\n- \"Freeze\" but watchdog OK = task(s) still running but not doing useful work\n- Possible: deadlock, infinite loop, resource exhaustion, priority inversion\n\n**Diagnostic Instrumentation:**\n\n1. **Task Heartbeat Monitoring:**\n```c\n// Each task updates its heartbeat\nvoid ppg_task(void *pvParameters) {\n    while(1) {\n        task_heartbeats[TASK_PPG] = xTaskGetTickCount();\n        // ... task work ...\n    }\n}\n\n// Monitor task checks for stale heartbeats\nvoid monitor_task(void *pvParameters) {\n    while(1) {\n        for (int i = 0; i < NUM_TASKS; i++) {\n            if ((xTaskGetTickCount() - task_heartbeats[i]) > HEARTBEAT_TIMEOUT) {\n                log_stalled_task(i);\n                capture_task_state(i);\n            }\n        }\n        vTaskDelay(pdMS_TO_TICKS(1000));\n    }\n}\n```\n\n2. **Resource Usage Tracking:**\n```c\n// Track mutex hold times\nvoid mutex_take_instrumented(SemaphoreHandle_t mutex, const char* name) {\n    TickType_t start = xTaskGetTickCount();\n    xSemaphoreTake(mutex, portMAX_DELAY);\n    TickType_t wait_time = xTaskGetTickCount() - start;\n    if (wait_time > MUTEX_WARN_THRESHOLD) {\n        log_long_mutex_wait(name, wait_time);\n    }\n}\n```\n\n3. **Memory Monitoring:**\n```c\nvoid periodic_health_check(void) {\n    // Heap status\n    size_t free_heap = xPortGetFreeHeapSize();\n    size_t min_heap = xPortGetMinimumEverFreeHeapSize();\n    \n    // Stack status per task\n    for (int i = 0; i < NUM_TASKS; i++) {\n        UBaseType_t stack_remaining = uxTaskGetStackHighWaterMark(task_handles[i]);\n        if (stack_remaining < STACK_WARN_THRESHOLD) {\n            log_low_stack(i, stack_remaining);\n        }\n    }\n    \n    // Queue status\n    UBaseType_t queue_spaces = uxQueueSpacesAvailable(dataQueue);\n    if (queue_spaces < QUEUE_WARN_THRESHOLD) {\n        log_queue_filling(queue_spaces);\n    }\n}\n```\n\n**Likely Root Causes:**\n\n1. **Deadlock:**\n   - Two tasks waiting for each other's mutex\n   - Detection: Check mutex holders when freeze occurs\n\n2. **Queue Full:**\n   - Producer faster than consumer\n   - Consumer blocked, producer fills queue, blocks\n   - Detection: Monitor queue levels over time\n\n3. **Memory Fragmentation:**\n   - Heap fragmenting over time\n   - Eventually allocation fails, task behavior undefined\n   - Detection: Track min free heap, allocation patterns\n\n4. **Priority Inversion:**\n   - Low-priority task holds resource indefinitely\n   - Detection: Mutex hold time monitoring\n\n**Tools:**\n- SEGGER SystemView: Real-time task tracing\n- J-Link: Live memory inspection\n- Serial logging with timestamps\n- GPIO toggle + logic analyzer for timing\n\n**Resolution Validation:**\n- Run stress test for 48+ hours\n- Accelerated testing (increase task rates)\n- Memory stress (allocate/free patterns)\n- Verify fix addresses root cause, not just symptom",
          "follow_up": "You've determined the freeze is caused by a deadlock between the BLE task and the logging task. How would you restructure the code to prevent this?",
          "follow_up_answer": "Deadlock prevention: 1) Establish strict lock ordering (always acquire mutex_A before mutex_B), 2) Use timeout on mutex acquire with retry, 3) Reduce mutex scope (hold for minimum time), 4) Use lock-free queues instead of shared buffers, 5) Consider single writer per resource (no mutex needed), 6) For logging: use dedicated logging queue, only logging task writes to flash, 7) Static analysis tools to detect lock order violations.",
          "hints": [
            "Consider what 'not frozen but not working' tells you",
            "Think about resource exhaustion scenarios"
          ]
        },
        {
          "id": "fw_7",
          "title": "Safety-Critical Software",
          "category": "safety",
          "difficulty": [
            "hard"
          ],
          "question_text": "You're developing firmware for a medical wearable that provides heart rate alerts. What software safety measures would you implement to ensure reliable operation? How would you handle a detected fault condition?",
          "rubric": {
            "safety_architecture": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes defensive programming techniques",
                  "points": 1
                },
                {
                  "description": "Discusses redundancy and cross-checking",
                  "points": 1
                },
                {
                  "description": "Addresses watchdog strategy",
                  "points": 1
                },
                {
                  "description": "Considers graceful degradation",
                  "points": 1
                }
              ]
            },
            "fault_handling": {
              "points": 3,
              "criteria": [
                {
                  "description": "Defines fault detection methods",
                  "points": 1
                },
                {
                  "description": "Proposes fault response strategy",
                  "points": 1
                },
                {
                  "description": "Addresses user notification",
                  "points": 1
                }
              ]
            },
            "regulatory_awareness": {
              "points": 3,
              "criteria": [
                {
                  "description": "Mentions relevant standards (IEC 62304)",
                  "points": 1
                },
                {
                  "description": "Discusses software classification",
                  "points": 1
                },
                {
                  "description": "Addresses documentation requirements",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Medical Device Software Safety:**\n\n**Software Classification (IEC 62304):**\n- Heart rate alerts affect clinical decisions → Class B or C\n- Requires: Risk management, design documentation, V&V, maintenance\n\n**Safety Architecture:**\n\n1. **Defensive Programming:**\n```c\n// Input validation\nuint8_t calculate_heart_rate(int32_t rr_intervals[], uint8_t count) {\n    // Bounds checking\n    ASSERT(count > 0 && count <= MAX_RR_INTERVALS);\n    ASSERT(rr_intervals != NULL);\n    \n    // Plausibility check\n    for (int i = 0; i < count; i++) {\n        if (rr_intervals[i] < MIN_RR_MS || rr_intervals[i] > MAX_RR_MS) {\n            log_implausible_data(rr_intervals[i]);\n            return HR_INVALID;\n        }\n    }\n    \n    // Actual calculation\n    int32_t sum = 0;\n    for (int i = 0; i < count; i++) {\n        sum += rr_intervals[i];\n    }\n    int32_t avg_rr = sum / count;\n    uint8_t hr = 60000 / avg_rr;\n    \n    // Output validation\n    if (hr < MIN_VALID_HR || hr > MAX_VALID_HR) {\n        return HR_INVALID;\n    }\n    \n    return hr;\n}\n```\n\n2. **Redundancy and Cross-Checking:**\n```c\ntypedef struct {\n    uint8_t hr_from_ecg;\n    uint8_t hr_from_ppg;\n    uint8_t hr_consensus;\n    uint8_t confidence;\n} HeartRateResult_t;\n\nHeartRateResult_t fused_heart_rate(void) {\n    HeartRateResult_t result;\n    \n    result.hr_from_ecg = calculate_hr_ecg();\n    result.hr_from_ppg = calculate_hr_ppg();\n    \n    // Cross-check\n    int diff = abs(result.hr_from_ecg - result.hr_from_ppg);\n    if (diff < HR_AGREEMENT_THRESHOLD) {\n        result.hr_consensus = (result.hr_from_ecg + result.hr_from_ppg) / 2;\n        result.confidence = CONFIDENCE_HIGH;\n    } else if (result.hr_from_ecg != HR_INVALID) {\n        result.hr_consensus = result.hr_from_ecg;  // Trust ECG more\n        result.confidence = CONFIDENCE_MEDIUM;\n    } else {\n        result.hr_consensus = HR_INVALID;\n        result.confidence = CONFIDENCE_LOW;\n    }\n    \n    return result;\n}\n```\n\n3. **Multi-Level Watchdog:**\n```c\n// Hardware watchdog: catches complete system hang\nIWDG_Config(IWDG_TIMEOUT_4S);\n\n// Software watchdog: catches task-level issues\nvoid software_watchdog_task(void *pvParameters) {\n    while(1) {\n        // Check each critical task checked in\n        if (!all_tasks_healthy()) {\n            // Don't kick hardware watchdog\n            // → System will reset\n            log_watchdog_failure();\n        } else {\n            HAL_IWDG_Refresh(&hiwdg);\n        }\n        vTaskDelay(pdMS_TO_TICKS(1000));\n    }\n}\n```\n\n**Fault Response:**\n```c\ntypedef enum {\n    FAULT_NONE,\n    FAULT_SENSOR_DEGRADED,  // One sensor failed, continue with other\n    FAULT_SENSOR_FAILED,    // All sensors failed\n    FAULT_MEMORY_ERROR,\n    FAULT_CRITICAL\n} FaultLevel_t;\n\nvoid handle_fault(FaultLevel_t level, uint32_t fault_code) {\n    log_fault(level, fault_code);\n    \n    switch(level) {\n        case FAULT_SENSOR_DEGRADED:\n            // Continue operation, notify user\n            notify_user(NOTIFICATION_REDUCED_ACCURACY);\n            break;\n            \n        case FAULT_SENSOR_FAILED:\n            // Stop HR measurement, alert user\n            stop_hr_monitoring();\n            alert_user(ALERT_SENSOR_FAILURE);\n            break;\n            \n        case FAULT_CRITICAL:\n            // Safe shutdown\n            save_critical_data();\n            alert_user(ALERT_DEVICE_ERROR);\n            enter_safe_mode();\n            break;\n    }\n}\n```\n\n**User Notification Hierarchy:**\n1. Normal operation with confidence indicator\n2. Reduced accuracy warning\n3. Measurement unavailable\n4. Device error - seek medical attention if symptoms",
          "follow_up": "How would you test and validate that your safety mechanisms work correctly?",
          "follow_up_answer": "Safety validation: 1) Fault injection testing (corrupt RAM, simulate sensor failure), 2) Code coverage targeting safety paths, 3) Static analysis (MISRA-C compliance), 4) Unit tests for all boundary conditions, 5) Integration tests simulating fault scenarios, 6) Long-duration stress testing, 7) Formal verification of critical algorithms, 8) Independent V&V per IEC 62304, 9) Traceability: requirements → tests → results.",
          "hints": [
            "Consider what could go wrong and how to detect it",
            "Think about graceful degradation vs hard failure"
          ]
        },
        {
          "id": "firmware_engineer_auto_1765269419",
          "question_text": "Explain the difference between preemptive and cooperative scheduling in RTOS. How does FreeRTOS implement task priority-based preemptive scheduling?",
          "difficulty": [
            "medium"
          ],
          "category": "system_design",
          "rubric": {
            "correctness": {
              "points": 5,
              "criteria": [
                {
                  "description": "Correct understanding",
                  "points": 3
                },
                {
                  "description": "Accurate details",
                  "points": 2
                }
              ]
            },
            "depth": {
              "points": 5,
              "criteria": [
                {
                  "description": "Thorough explanation",
                  "points": 3
                },
                {
                  "description": "Practical examples",
                  "points": 2
                }
              ]
            }
          },
          "sample_strong_answer": "Preemptive scheduling allows higher priority tasks to interrupt lower priority ones immediately, while cooperative scheduling requires tasks to voluntarily yield. FreeRTOS uses priority-based preemptive scheduling where: 1) Tasks have priorities 0-31 (higher number = higher priority), 2) The scheduler runs the highest priority ready task, 3) Context switches occur on: task blocking calls (vTaskDelay), interrupts releasing higher priority tasks, or explicit yields. Key components include the ready list array (indexed by priority), pxCurrentTCB pointer, and vTaskSwitchContext() function.",
          "hints": [
            "Consider how FreeRTOS manages the ready state tasks",
            "Think about what triggers a context switch",
            "Remember the role of the tick interrupt"
          ],
          "auto_added": true,
          "added_at": "2025-12-09T00:36:59.920201"
        }
      ]
    },
    "mechanical_engineer": {
      "track_id": "mechanical_engineer",
      "title": "Mechanical Design Engineer",
      "description": "Mechanical Design Engineer assessment focusing on DFM, materials selection, thermal management, structural analysis, and IP rating design",
      "keywords": [
        "mechanical",
        "CAD",
        "SolidWorks",
        "injection molding",
        "DFM",
        "tolerance",
        "GD&T",
        "thermal",
        "FEA",
        "IP rating",
        "materials"
      ],
      "technical_questions": [
        {
          "id": "me_1",
          "title": "DFM and Tolerance Analysis",
          "category": "manufacturing",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "You're designing a wearable device housing using injection molding. The design has a snap-fit battery door, a display window, and multiple internal mounting features. Describe your DFM considerations and how you would perform tolerance stack-up analysis for critical interfaces.",
          "rubric": {
            "dfm_principles": {
              "points": 4,
              "criteria": [
                {
                  "description": "Discusses wall thickness uniformity",
                  "points": 1
                },
                {
                  "description": "Addresses draft angles",
                  "points": 1
                },
                {
                  "description": "Considers parting line and gate location",
                  "points": 1
                },
                {
                  "description": "Mentions sink marks and warpage prevention",
                  "points": 1
                }
              ]
            },
            "tolerance_analysis": {
              "points": 4,
              "criteria": [
                {
                  "description": "Explains stack-up methodology",
                  "points": 1
                },
                {
                  "description": "Considers manufacturing tolerances",
                  "points": 1
                },
                {
                  "description": "Addresses thermal expansion",
                  "points": 1
                },
                {
                  "description": "Discusses statistical vs worst-case analysis",
                  "points": 1
                }
              ]
            },
            "practical_application": {
              "points": 2,
              "criteria": [
                {
                  "description": "Applies concepts to wearable context",
                  "points": 1
                },
                {
                  "description": "Considers assembly implications",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**DFM for Injection Molded Wearable Housing:**\n\n**Wall Thickness:**\n- Target: 1.5-2.0mm uniform throughout\n- Variations cause differential cooling → warpage\n- Ribs: 60% of wall thickness to prevent sink marks\n- Boss design: OD = 2× screw diameter, wall = 0.6× nominal wall\n\n**Draft Angles:**\n- External surfaces: 1-2° minimum\n- Textured surfaces: +1° per 0.025mm texture depth\n- Internal features: 0.5-1° minimum\n- Snap-fit features: 3-5° for easy release\n\n**Parting Line and Gating:**\n```\n┌─────────────────────────────┐\n│      TOP HALF               │\n├─────────────────────────────┤ ← Parting line at midpoint\n│      BOTTOM HALF            │     (hidden by bezel)\n└─────────────────────────────┘\n\nGate: Edge gate on non-cosmetic surface\nAlternative: Submarine gate for auto-degating\n```\n\n**Snap-Fit Battery Door:**\n- Cantilever snap with calculated deflection\n- Strain: ε = (3×deflection×thickness) / (2×length²) < 2% for ABS\n- Include finger grip feature\n- Lead-in chamfer for easy engagement\n\n**Tolerance Stack-Up Analysis:**\n\n*Critical Interface: Display to Window*\n```\nStack-up (worst case):\n┌──────────────────────────────────────┐\n│ Feature                    Tolerance │\n├──────────────────────────────────────┤\n│ Housing pocket depth       ±0.15mm   │\n│ Display module thickness   ±0.10mm   │\n│ Adhesive thickness         ±0.05mm   │\n│ Window lens thickness      ±0.10mm   │\n│ Bezel height               ±0.10mm   │\n├──────────────────────────────────────┤\n│ Total (worst case)         ±0.50mm   │\n│ Total (RSS statistical)    ±0.25mm   │\n└──────────────────────────────────────┘\n```\n\n**Thermal Expansion:**\n- ABS: CTE = 90×10⁻⁶ /°C\n- ΔT = 60°C (0°C to 60°C operating)\n- 50mm part grows: 50 × 90×10⁻⁶ × 60 = 0.27mm\n- Must account for in clearance fits\n\n**Design Recommendations:**\n- Use datums for critical dimensions\n- Gate flush with surface (no witness mark on cosmetic)\n- Consider two-shot molding for soft-touch grip\n- Family mold for top/bottom to ensure matching",
          "follow_up": "The snap-fit battery door is breaking in drop tests. How would you redesign it to improve durability?",
          "follow_up_answer": "Snap-fit improvement: 1) Reduce strain by increasing cantilever length or reducing deflection, 2) Add radius at stress concentration (root of snap), 3) Use material with higher elongation at break (PC+ABS blend), 4) Add living hinge to distribute load, 5) Consider captured hinge + latch design, 6) Add bumper features to protect snap from direct impact, 7) FEA to verify stress under load.",
          "hints": [
            "Consider the injection molding process constraints",
            "Think about how parts fit together with manufacturing variation"
          ]
        },
        {
          "id": "me_2",
          "title": "Material Selection",
          "category": "materials",
          "difficulty": [
            "medium"
          ],
          "question_text": "You need to select materials for a wearable medical device that will be worn continuously for up to 14 days. The device contacts skin, is exposed to sweat, and must withstand accidental drops. What materials would you consider for the housing and skin-contact components?",
          "rubric": {
            "material_knowledge": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes appropriate housing materials",
                  "points": 1
                },
                {
                  "description": "Discusses skin-contact material requirements",
                  "points": 1
                },
                {
                  "description": "Considers biocompatibility standards",
                  "points": 1
                },
                {
                  "description": "Addresses chemical resistance",
                  "points": 1
                }
              ]
            },
            "selection_criteria": {
              "points": 3,
              "criteria": [
                {
                  "description": "Balances mechanical and aesthetic requirements",
                  "points": 1
                },
                {
                  "description": "Considers manufacturing constraints",
                  "points": 1
                },
                {
                  "description": "Addresses cost implications",
                  "points": 1
                }
              ]
            },
            "regulatory_awareness": {
              "points": 3,
              "criteria": [
                {
                  "description": "Mentions ISO 10993 biocompatibility",
                  "points": 1
                },
                {
                  "description": "Discusses material documentation",
                  "points": 1
                },
                {
                  "description": "Considers supply chain implications",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Material Selection for Medical Wearable:**\n\n**Housing (Main Body):**\n\n*Primary Choice: PC+ABS Blend*\n- Impact strength: 550 J/m (excellent drop resistance)\n- Chemical resistance: Good against sweat, lotions\n- Processability: Easy injection molding\n- Surface finish: Class A cosmetic possible\n- Cost: Medium ($3-5/kg)\n- Examples: Bayblend T65 XF, Cycoloy C1200\n\n*Alternative: Glass-filled Nylon (PA6-GF30)*\n- Higher stiffness for thinner walls\n- Better chemical resistance\n- Higher cost, more abrasive to tooling\n\n**Skin-Contact Components:**\n\n*Primary Choice: Medical-Grade Silicone (LSR)*\n- Biocompatibility: ISO 10993-5, -10 compliant\n- Hypoallergenic: Very low allergen potential\n- Sweat resistance: Excellent\n- Comfort: Soft durometer (30-50 Shore A)\n- Wear time: Proven for 14+ day continuous contact\n- Examples: NuSil MED-4950, Dow Silastic\n\n*Alternative: Medical TPE*\n- Lower cost than silicone\n- Easier overmolding with housing\n- Good for shorter wear times\n- Examples: Kraiburg TPE/TF, Santoprene\n\n**Biocompatibility Requirements:**\n\n| Test | Standard | Requirement |\n|------|----------|-------------|\n| Cytotoxicity | ISO 10993-5 | Grade 0-1 |\n| Sensitization | ISO 10993-10 | Non-sensitizing |\n| Irritation | ISO 10993-10 | Non-irritating |\n| Extractables | ISO 10993-18 | Chemical characterization |\n\n**Material Documentation:**\n- Material certificates of analysis (CoA)\n- Biocompatibility test reports\n- Change notification agreements with suppliers\n- Alternate supplier qualification\n\n**Stack-Up:**\n```\nOuter housing: PC+ABS (1.5mm)\nInner structure: Same PC+ABS\nSkin contact: LSR overmolded (1mm)\nElectrode interface: Medical PSA (3M 2477P)\nStrap: LSR or medical TPE\n```\n\n**Cost Considerations:**\n- Silicone: $15-30/kg (2-3× plastic cost)\n- Two-shot molding adds tooling cost\n- Biocompat testing: $5-15K per material\n- Worth it for medical device compliance",
          "follow_up": "A user reports skin irritation after 5 days of wear. How would you investigate and address this?",
          "follow_up_answer": "Skin irritation investigation: 1) Collect affected device for analysis, 2) Check manufacturing lot records, 3) Test for extractables (residual processing aids, mold release), 4) Verify biocompatibility test lot matches production, 5) Review cleaning process, 6) Consider mechanical irritation (edges, pressure points), 7) Patch testing with dermatologist, 8) May need to add ventilation features or change adhesive, 9) Document in complaint handling system per ISO 13485.",
          "hints": [
            "Consider long-term skin contact requirements",
            "Think about biocompatibility testing needs"
          ]
        },
        {
          "id": "me_3",
          "title": "Thermal Management",
          "category": "thermal",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "The electronics in your wearable device generate 500mW of heat continuously. The device is enclosed with no active cooling, worn against skin (33°C ambient). Maximum component temperature is 85°C. How would you design the thermal management system?",
          "rubric": {
            "thermal_analysis": {
              "points": 4,
              "criteria": [
                {
                  "description": "Calculates thermal resistance budget",
                  "points": 1
                },
                {
                  "description": "Identifies heat transfer paths",
                  "points": 1
                },
                {
                  "description": "Considers skin temperature limit",
                  "points": 1
                },
                {
                  "description": "Discusses hot spot management",
                  "points": 1
                }
              ]
            },
            "design_solutions": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes heat spreading solutions",
                  "points": 1
                },
                {
                  "description": "Discusses thermal interface materials",
                  "points": 1
                },
                {
                  "description": "Considers enclosure design",
                  "points": 1
                },
                {
                  "description": "Addresses component placement",
                  "points": 1
                }
              ]
            },
            "practical_implementation": {
              "points": 2,
              "criteria": [
                {
                  "description": "Considers manufacturing constraints",
                  "points": 1
                },
                {
                  "description": "Addresses testing/validation",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Thermal Management Design:**\n\n**Thermal Budget Analysis:**\n```\nHeat source: 500mW (MCU + BLE + LED driver)\nMax junction temp: 85°C\nSkin temp: 33°C (worn) or 25°C (ambient)\nΔT available: 85 - 33 = 52°C\n\nRequired thermal resistance: Rth = ΔT / P = 52 / 0.5 = 104°C/W\n```\n\n**Thermal Resistance Stack:**\n```\nComponent → Package → TIM → Spreader → Housing → Air/Skin\n\n  Component      Rth (°C/W)  Contribution\n  ─────────────────────────────────────────\n  Die to case      10        Package thermal\n  TIM (0.2mm)      2         Thermal paste\n  Spreader         5         Copper/aluminum\n  Housing          20        Plastic (poor conductor)\n  Housing→Air      50        Natural convection\n  ─────────────────────────────────────────\n  Total            87°C/W    < 104 budget ✓\n```\n\n**Design Solutions:**\n\n1. **Heat Spreading:**\n```\n┌─────────────────────────────────┐\n│  Housing (top)                  │\n├─────────────────────────────────┤\n│  Copper spreader (0.5mm)        │ ← Distributes heat\n│  ┌───────┐                      │\n│  │ MCU   │ TIM                  │\n│  └───────┘                      │\n│  PCB                            │\n├─────────────────────────────────┤\n│  Housing (bottom) → Skin        │\n└─────────────────────────────────┘\n\nSpreader: 40×40×0.5mm copper\nSpreading resistance: ~2°C/W\nMoves heat away from hot spot\n```\n\n2. **Thermal Interface Materials:**\n- Thermal pad (gap filler): k = 3-5 W/m·K\n- Thermal paste (thin bond line): k = 1-3 W/m·K\n- Phase change material: Good for varying gaps\n\n3. **Housing Design:**\n- Thin walls over hot components (reduce Rth)\n- Metal insert in plastic housing at hot spot\n- Ventilation slots if aesthetically acceptable\n- Thermal paint/coating (increases emissivity)\n\n4. **Component Placement:**\n- Spread heat sources across PCB\n- Place hot components away from skin side\n- Keep battery away from heat sources\n\n**Skin Temperature Limit:**\n```\nIEC 60601-1: Max skin contact temp = 43°C (prolonged)\n\nCurrent design:\nT_skin = T_ambient + P × Rth_to_skin\n       = 33 + 0.5 × (housing→skin Rth)\n       \nNeed: housing→skin Rth < (43-33)/0.5 = 20°C/W\n\nSolution: Thermal barrier between electronics and skin\n- Air gap or insulating foam on skin side\n- Route heat to top/sides instead\n```\n\n**Validation:**\n- Thermal simulation (ANSYS Icepak)\n- IR camera testing on prototype\n- Thermocouple measurements at key points\n- Worst-case testing (max workload, 40°C ambient)",
          "follow_up": "The device fails thermal testing when the user covers it with a blanket during sleep. How would you address this?",
          "follow_up_answer": "Covered/insulated scenario: 1) Add temperature sensor for thermal monitoring, 2) Firmware thermal throttling (reduce processing, LED brightness), 3) Increase thermal mass (heat sink) to handle transient events, 4) Add user warning if temp exceeds threshold, 5) Duty cycle heavy operations, 6) Design requirement: device must operate safely even when covered, 7) May need to derate specifications for covered use case.",
          "hints": [
            "Calculate the thermal budget first",
            "Consider where the heat goes when worn against skin"
          ]
        },
        {
          "id": "me_4",
          "title": "Structural Analysis",
          "category": "structural",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "Your wearable device must survive a 1.5m drop onto concrete per IEC 60601-1. Describe your approach to designing for drop resistance, including FEA analysis and physical testing strategy.",
          "rubric": {
            "design_approach": {
              "points": 4,
              "criteria": [
                {
                  "description": "Discusses impact energy absorption",
                  "points": 1
                },
                {
                  "description": "Proposes protective features",
                  "points": 1
                },
                {
                  "description": "Considers internal component protection",
                  "points": 1
                },
                {
                  "description": "Addresses weak point identification",
                  "points": 1
                }
              ]
            },
            "analysis_methods": {
              "points": 3,
              "criteria": [
                {
                  "description": "Describes FEA simulation approach",
                  "points": 1
                },
                {
                  "description": "Discusses material models for impact",
                  "points": 1
                },
                {
                  "description": "Considers mesh and solver settings",
                  "points": 1
                }
              ]
            },
            "testing_strategy": {
              "points": 3,
              "criteria": [
                {
                  "description": "Proposes physical test protocol",
                  "points": 1
                },
                {
                  "description": "Discusses failure criteria",
                  "points": 1
                },
                {
                  "description": "Addresses sample size and orientation",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Drop Test Design Strategy:**\n\n**Impact Analysis:**\n```\nDrop height: 1.5m\nImpact velocity: v = √(2gh) = √(2×9.81×1.5) = 5.4 m/s\nDevice mass: 50g = 0.05kg\nKinetic energy: KE = ½mv² = ½×0.05×5.4² = 0.73 J\n\nPeak acceleration (depends on impact duration):\n- Hard surface (1ms): a = v/t = 5400 m/s² = 550g\n- Soft surface (5ms): a = 110g\n```\n\n**Design Features for Drop Resistance:**\n\n1. **Corner/Edge Bumpers:**\n```\n┌─────────────────┐\n│  ╭───────────╮  │\n│  │           │  │\n│  │  Device   │  │ ← TPE overmold bumpers\n│  │           │  │    on corners/edges\n│  ╰───────────╯  │\n└─────────────────┘\n\n- Material: TPE, 60 Shore A\n- Thickness: 2-3mm\n- Extends 1mm beyond housing\n```\n\n2. **Internal Protection:**\n- PCB mounted with rubber grommets (vibration isolation)\n- Display bonded with compliant adhesive\n- Battery secured with foam compression pad\n- Critical components (crystal) oriented perpendicular to likely impact\n\n3. **Housing Reinforcement:**\n- Ribs at stress concentration points\n- Increased wall thickness at corners\n- Radius all internal corners (min 0.5mm)\n\n**FEA Simulation:**\n\n*Setup:*\n- Software: ANSYS Explicit Dynamics or LS-DYNA\n- Mesh: Hex-dominant, 0.5mm element size in impact zone\n- Material: Nonlinear plastic (Johnson-Cook for high strain rate)\n- Contact: Frictional, µ=0.3\n- Floor: Rigid or concrete material model\n\n*Impact Orientations:*\n1. Face down (display impact)\n2. Edge impact (long edge)\n3. Corner impact (worst case for housing)\n4. Back impact (battery side)\n\n*Results to Extract:*\n- Maximum stress vs yield strength\n- Plastic strain (permanent deformation)\n- Component accelerations\n- Deflection of critical features\n\n**Physical Testing Protocol:**\n\n```\nTest Standard: IEC 60601-1, clause 15.3.3\n\nProcedure:\n1. Sample size: 3 devices minimum\n2. Condition: Room temp, 50% RH\n3. Drop surface: Steel plate (20mm thick)\n4. Heights: 1.0m, 1.25m, 1.5m progression\n5. Orientations: 6 faces + 8 edges + 8 corners = 22 drops\n6. Inspection after each drop\n\nPass Criteria:\n- No safety hazard created\n- Device remains functional\n- No exposed sharp edges\n- No battery exposure\n- Display may crack (not safety critical)\n\nDocumentation:\n- Photo before/after each drop\n- Functional test results\n- Damage classification (cosmetic/functional/safety)\n```\n\n**Iterative Refinement:**\n- FEA predicts problem areas\n- First prototype validates FEA correlation\n- Adjust material, geometry based on physical results\n- Re-simulate and re-test until pass",
          "follow_up": "Your drop test prototype cracked at a boss near the corner. How would you redesign to fix this?",
          "follow_up_answer": "Boss crack fix: 1) Add gussets connecting boss to wall (distribute stress), 2) Increase radius at boss base (reduce stress concentration), 3) Reduce boss height if possible (less moment arm), 4) Change boss location away from impact zone, 5) Add ribs radiating from boss to walls, 6) Consider metal insert instead of plastic boss, 7) FEA to verify stress reduction, 8) Minimum 3 samples to validate fix.",
          "hints": [
            "Consider where impact energy goes",
            "Think about both simulation and physical testing"
          ]
        },
        {
          "id": "me_5",
          "title": "IP Rating and Sealing",
          "category": "sealing",
          "difficulty": [
            "medium",
            "hard"
          ],
          "question_text": "Your wearable device needs IP67 rating (dust tight, water immersion to 1m for 30 min). Describe your sealing strategy for the main housing, buttons, and charging port.",
          "rubric": {
            "sealing_design": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes housing seal design",
                  "points": 1
                },
                {
                  "description": "Addresses button sealing",
                  "points": 1
                },
                {
                  "description": "Discusses port sealing strategy",
                  "points": 1
                },
                {
                  "description": "Considers membrane/vent requirements",
                  "points": 1
                }
              ]
            },
            "material_selection": {
              "points": 3,
              "criteria": [
                {
                  "description": "Chooses appropriate gasket materials",
                  "points": 1
                },
                {
                  "description": "Discusses compression and tolerance",
                  "points": 1
                },
                {
                  "description": "Addresses long-term sealing reliability",
                  "points": 1
                }
              ]
            },
            "testing_validation": {
              "points": 3,
              "criteria": [
                {
                  "description": "Describes IP test procedure",
                  "points": 1
                },
                {
                  "description": "Discusses test sample requirements",
                  "points": 1
                },
                {
                  "description": "Addresses production verification",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**IP67 Sealing Design:**\n\n**Main Housing Seal:**\n```\n┌─────────────────────────────────┐\n│  Top Housing                    │\n│  ┌─────────────────────────┐    │\n│  │     O-ring groove        │←── Compression: 20-25%\n│  └─────────────────────────┘    │\n├─────────────────────────────────┤\n│  │     O-ring (silicone)    │    │\n├─────────────────────────────────┤\n│  Bottom Housing                 │\n└─────────────────────────────────┘\n\nO-ring: 70 Shore A silicone\nCross-section: 1.5mm\nGroove depth: 1.2mm (20% compression)\nGroove width: 2.0mm\n```\n\n**Gasket Design Alternative:**\n- Molded-in-place (MIP) gasket\n- Applied to one half during molding\n- Eliminates assembly step\n- Requires careful process control\n\n**Button Sealing:**\n```\n┌──────────────┐\n│   Button cap │ (rigid plastic)\n├──────────────┤\n│   Silicone   │ ← Integrated elastomer\n│   membrane   │    sealed to housing\n├──────────────┤\n│   Housing    │\n└──────────────┘\n\nOptions:\n1. Overmolded silicone button (best)\n2. Separate silicone boot with snap-in button\n3. Capacitive touch (no physical button)\n```\n\n**Charging Port:**\n\n*Option 1: Magnetic Pogo Pins*\n```\n┌─────────────┐\n│  Housing    │\n│  ┌───────┐  │\n│  │ Pogo  │  │ ← Pins sealed with\n│  │ pins  │  │    o-ring or molded boot\n│  └───────┘  │\n└─────────────┘\n\n- No opening in housing\n- Magnetic alignment\n- IP67 maintained during use\n```\n\n*Option 2: USB with Cover*\n```\n- Captive rubber plug\n- User must close for water exposure\n- Warning label required\n- Not recommended for IP67 requirement\n```\n\n*Option 3: Wireless Charging*\n- Best for IP67\n- Higher cost, lower efficiency\n- No user-accessible ports\n\n**Pressure Equalization:**\n```\n┌──────────────────┐\n│  GORE vent       │ ← Waterproof, breathable\n│  membrane        │    membrane (e.g., GORE-TEX)\n└──────────────────┘\n\n- Equalizes pressure during altitude change\n- Prevents seal stress during temperature cycling\n- Acoustic membranes for microphone (IP67 rated)\n```\n\n**IP67 Test Protocol (IEC 60529):**\n\n*IP6X (Dust tight):*\n- Test duration: 8 hours\n- Negative pressure: -2kPa to -8kPa inside\n- Talcum powder environment\n- Pass: No dust ingress\n\n*IPX7 (Immersion):*\n- Depth: 1m (top of device at 1m depth)\n- Duration: 30 minutes\n- Water temp: 15-35°C\n- Pass: No water ingress affecting operation\n\n**Production Verification:**\n- 100% leak test on production line\n- Pressurize to 10-20kPa, monitor decay\n- Sample IP test per lot (AQL)\n- Track seal compression at assembly",
          "follow_up": "Users in humid climates report fogging inside the display. How would you address this?",
          "follow_up_answer": "Internal fogging: 1) Add desiccant pack inside housing, 2) GORE vent allows moisture equalization, 3) Improve seal on display-to-housing interface, 4) Consider anti-fog coating on display inner surface, 5) Assembly in humidity-controlled environment, 6) Nitrogen fill before sealing (displace humid air), 7) Design for serviceable desiccant replacement if long product life.",
          "hints": [
            "Consider all potential water entry points",
            "Think about the trade-offs between sealing methods"
          ]
        },
        {
          "id": "me_6",
          "title": "Assembly Design",
          "category": "assembly",
          "difficulty": [
            "medium"
          ],
          "question_text": "Design the assembly sequence and fastening strategy for a wearable device with: main PCB, display module, battery, flex cables, and top/bottom housing. Consider both manual assembly and potential automation.",
          "rubric": {
            "assembly_sequence": {
              "points": 4,
              "criteria": [
                {
                  "description": "Proposes logical assembly order",
                  "points": 1
                },
                {
                  "description": "Minimizes handling/flipping",
                  "points": 1
                },
                {
                  "description": "Considers cable routing",
                  "points": 1
                },
                {
                  "description": "Addresses verification points",
                  "points": 1
                }
              ]
            },
            "fastening_strategy": {
              "points": 3,
              "criteria": [
                {
                  "description": "Selects appropriate fasteners",
                  "points": 1
                },
                {
                  "description": "Discusses screw vs snap-fit trade-offs",
                  "points": 1
                },
                {
                  "description": "Considers serviceability",
                  "points": 1
                }
              ]
            },
            "automation_considerations": {
              "points": 3,
              "criteria": [
                {
                  "description": "Identifies automation candidates",
                  "points": 1
                },
                {
                  "description": "Discusses pick-and-place friendly design",
                  "points": 1
                },
                {
                  "description": "Addresses fixture requirements",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Assembly Design for Wearable Device:**\n\n**Assembly Sequence:**\n```\nStep  Operation                    Time   Notes\n────────────────────────────────────────────────────\n1.    Bottom housing in fixture    5s     Locating features\n2.    Insert battery               10s    Tape pre-applied\n3.    Place main PCB               10s    Alignment pins\n4.    Connect battery flex         15s    ZIF connector\n5.    Route antenna flex           10s    Pre-formed shape\n6.    Secure PCB with screws (2)   20s    M1.4 × 3mm\n7.    Apply display adhesive       10s    Pre-cut gasket\n8.    Place display module         15s    Alignment to PCB\n9.    Connect display flex         15s    ZIF connector\n10.   Functional test (in-process) 30s    Display, touch, BLE\n11.   Apply housing seal           10s    O-ring or adhesive\n12.   Mate top housing             15s    Snap-fit + screw\n13.   Final torque screw           10s    M1.6 × 4mm\n14.   Final test                   45s    Full functional\n15.   Label and pack               20s    Serial number\n────────────────────────────────────────────────────\nTotal                              ~4 min\n```\n\n**Design for Assembly (DFA) Principles:**\n\n1. **Z-Axis Assembly:**\n   - All components insert from top\n   - No flipping of housing during assembly\n   - Reduces handling time and errors\n\n2. **Self-Locating Features:**\n```\n┌───────────────────┐\n│  ┌───┐    ┌───┐   │\n│  │   │    │   │   │ ← Locating bosses\n│  └───┘    └───┘   │    for PCB\n│                   │\n│    ○        ○     │ ← Alignment pins\n└───────────────────┘    for display\n```\n\n3. **Connector Selection:**\n   - ZIF (Zero Insertion Force) for flex cables\n   - Pre-tinned pads for battery (spot weld)\n   - Pogo pins for test interface\n\n**Fastening Strategy:**\n\n| Location | Method | Rationale |\n|----------|--------|----------|\n| PCB to housing | 2× M1.4 screws | Secure mounting, ESD ground |\n| Battery | Adhesive tape | Vibration damping, easy removal |\n| Display | Adhesive gasket | Sealing, optical bond |\n| Top to bottom | 1× screw + snaps | Security + tamper evidence |\n\n**Screw vs Snap-Fit:**\n- Screws: Stronger, serviceable, tamper-evident options\n- Snaps: Faster assembly, no fastener cost, may fatigue\n- Hybrid: Snaps for alignment, one screw for security\n\n**Automation Candidates:**\n\n```\nManual                  Automated\n─────────────────────────────────────\nFlex cable connection   Screw driving\nAdhesive application    Dispensing\nFunctional test         Test execution\n                        Labeling\n```\n\n*Automated Screw Driving:*\n- Consistent torque (0.1 ± 0.02 N·m)\n- Faster than manual\n- Requires fixture investment\n\n*Adhesive Dispensing:*\n- Consistent bead size and placement\n- Vision verification\n- Higher yield than manual\n\n**Fixture Design:**\n```\n┌─────────────────────────────┐\n│  Nest for bottom housing    │\n│  ┌─────────────────────┐    │\n│  │                     │    │\n│  │  Vacuum hold-down   │    │\n│  │                     │    │\n│  └─────────────────────┘    │\n│  Poke-yoke features         │ ← Prevent wrong orientation\n└─────────────────────────────┘\n```",
          "follow_up": "Production reports high reject rate due to display flex cable damage. How would you investigate and fix this?",
          "follow_up_answer": "Flex cable damage investigation: 1) Review failure mode (crease, tear, connector damage), 2) Observe assembly process at station, 3) Check cable routing clearance in design, 4) Verify cable minimum bend radius is met, 5) Add strain relief features, 6) Redesign cable with stiffener in stress area, 7) Modify fixture to guide cable routing, 8) Operator training on handling, 9) Consider different connector type (FPC vs wire).",
          "hints": [
            "Think about the order of operations",
            "Consider what can be automated"
          ]
        },
        {
          "id": "me_7",
          "title": "Design Iteration and Documentation",
          "category": "process",
          "difficulty": [
            "medium"
          ],
          "question_text": "You've completed the first prototype of a wearable device and need to transition to production. Describe your design documentation package and the process for managing design changes.",
          "rubric": {
            "documentation_package": {
              "points": 4,
              "criteria": [
                {
                  "description": "Lists required drawing types",
                  "points": 1
                },
                {
                  "description": "Discusses GD&T application",
                  "points": 1
                },
                {
                  "description": "Addresses BOM structure",
                  "points": 1
                },
                {
                  "description": "Considers specification documents",
                  "points": 1
                }
              ]
            },
            "change_management": {
              "points": 4,
              "criteria": [
                {
                  "description": "Describes ECO/ECN process",
                  "points": 1
                },
                {
                  "description": "Discusses impact assessment",
                  "points": 1
                },
                {
                  "description": "Addresses revision control",
                  "points": 1
                },
                {
                  "description": "Considers regulatory implications",
                  "points": 1
                }
              ]
            },
            "transition_process": {
              "points": 2,
              "criteria": [
                {
                  "description": "Discusses design freeze criteria",
                  "points": 1
                },
                {
                  "description": "Addresses production readiness",
                  "points": 1
                }
              ]
            }
          },
          "sample_strong_answer": "**Design Documentation Package:**\n\n**Drawing Types Required:**\n\n1. **Assembly Drawings:**\n   - Exploded view with item balloons\n   - Assembly sequence notes\n   - Torque specifications\n   - Critical dimensions (overall, interface)\n\n2. **Part Drawings:**\n   - Full GD&T per ASME Y14.5\n   - Material and finish callouts\n   - Critical-to-function (CTF) dimensions flagged\n   - Reference to 3D model\n\n3. **Manufacturing Drawings:**\n   - Injection mold reference (draft, gates)\n   - Fixture reference points\n   - Inspection points\n\n**GD&T Application Example:**\n```\n┌──────────────────────────────────────┐\n│  A                                   │\n│  ⊕│⊙ 0.1│A│B│C│                     │\n│   ↓                                  │\n│  ╔══════════════╗                    │\n│  ║  Display     ║ ← Position tolerance\n│  ║  Window      ║    to datums\n│  ╚══════════════╝                    │\n│                                      │\n│  ⊡ 0.05 A                           │ ← Flatness for seal\n└──────────────────────────────────────┘\n\nDatum A: Bottom surface (primary)\nDatum B: Long edge (secondary)\nDatum C: Short edge (tertiary)\n```\n\n**BOM Structure:**\n```\nLevel  Part Number    Description        Qty  Source\n───────────────────────────────────────────────────────\n0      ASM-1001       Wearable Device    1    Assembly\n1      ├─ PRT-2001    Top Housing        1    Injection\n1      ├─ PRT-2002    Bottom Housing     1    Injection\n1      ├─ PRT-2003    Display Module     1    Purchased\n1      ├─ PRT-2004    Battery            1    Purchased\n1      ├─ PCBA-3001   Main PCB Assy      1    Assembly\n2      │  ├─ PCB-4001 Bare Board         1    Fabricated\n2      │  └─ [BOM]    Components         n    Purchased\n1      └─ HDW-5001    Screw M1.4×3       2    Purchased\n```\n\n**Specification Documents:**\n- Product Requirements Document (PRD)\n- Mechanical Requirements Spec\n- Environmental Test Spec (temp, humidity, drop, IP)\n- Cosmetic Standards (color, texture, defect criteria)\n\n**Change Management Process:**\n\n```\n┌──────────────┐\n│  Identify    │ Issue found (quality, cost, supplier)\n│  Change Need │\n└──────┬───────┘\n       ↓\n┌──────────────┐\n│  ECR         │ Engineering Change Request\n│  (Request)   │ - Problem description\n│              │ - Proposed solution\n│              │ - Requester info\n└──────┬───────┘\n       ↓\n┌──────────────┐\n│  Impact      │ Review by cross-functional team:\n│  Assessment  │ - Cost impact\n│              │ - Tooling changes\n│              │ - Inventory impact\n│              │ - Regulatory impact\n│              │ - Testing required\n└──────┬───────┘\n       ↓\n┌──────────────┐\n│  ECO         │ Engineering Change Order\n│  (Approved)  │ - Revision letter change\n│              │ - Implementation date\n│              │ - Disposition of old inventory\n└──────┬───────┘\n       ↓\n┌──────────────┐\n│  ECN         │ Engineering Change Notice\n│  (Release)   │ - Notify all stakeholders\n│              │ - Update documentation\n│              │ - Close loop\n└──────────────┘\n```\n\n**Medical Device Considerations:**\n- Design History File (DHF) per FDA 21 CFR 820.30\n- Traceability: requirement → design → V&V\n- Change control impacts 510(k) status\n\n**Production Readiness:**\n- Design freeze criteria defined\n- First Article Inspection (FAI) complete\n- Process validation (IQ/OQ/PQ)\n- Manufacturing procedures released",
          "follow_up": "A supplier wants to change the resin grade for cost savings. How would you evaluate and approve this?",
          "follow_up_answer": "Resin change evaluation: 1) Compare datasheets (mechanical, thermal, chemical), 2) Verify biocompatibility equivalence (may need new testing), 3) Mold trial to check processability, 4) Environmental testing (temp cycling, humidity), 5) Drop test validation, 6) Cosmetic comparison, 7) Update material specification, 8) For medical device: assess regulatory impact (may need 510(k) letter-to-file), 9) Document in ECO with all test evidence.",
          "hints": [
            "Think about what documentation is needed for manufacturing",
            "Consider the regulatory aspects of changes"
          ]
        }
      ]
    }
  },
  "general_ml_questions": {
    "description": "General AI/ML fundamental questions applicable to all candidates with ML background",
    "questions": [
      {
        "id": "gml_1",
        "title": "Bias-Variance Tradeoff",
        "category": "ml_fundamentals",
        "difficulty": ["easy", "medium"],
        "question_text": "Explain the bias-variance tradeoff in machine learning. How do you identify whether your model suffers from high bias or high variance, and what strategies would you use to address each?",
        "rubric": {
          "conceptual_understanding": {
            "points": 4,
            "criteria": [
              {"description": "Correctly defines bias (underfitting) and variance (overfitting)", "points": 2},
              {"description": "Explains the tradeoff between them", "points": 1},
              {"description": "Discusses total error decomposition", "points": 1}
            ]
          },
          "diagnosis": {
            "points": 3,
            "criteria": [
              {"description": "Identifies high bias: poor training AND validation performance", "points": 1},
              {"description": "Identifies high variance: good training, poor validation", "points": 1},
              {"description": "Mentions learning curves as diagnostic tool", "points": 1}
            ]
          },
          "solutions": {
            "points": 3,
            "criteria": [
              {"description": "For high bias: more features, complex models, longer training", "points": 1},
              {"description": "For high variance: regularization, more data, simpler models", "points": 1},
              {"description": "Mentions cross-validation or ensemble methods", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Bias-Variance Tradeoff:**\n\nTotal Error = Bias² + Variance + Irreducible Noise\n\n**Bias:** Error from overly simplistic assumptions. High bias means the model can't capture the true pattern (underfitting).\n\n**Variance:** Error from sensitivity to small fluctuations in training data. High variance means the model fits noise (overfitting).\n\n**Diagnosis:**\n- High Bias: Both training and validation errors are high. Learning curve shows training error plateaus at high value.\n- High Variance: Training error is low, but validation error is much higher. Gap between training and validation curves.\n\n**Solutions:**\n\n*For High Bias:*\n- Add more features / feature engineering\n- Use more complex model architecture\n- Reduce regularization\n- Train longer / more epochs\n\n*For High Variance:*\n- Get more training data\n- Apply regularization (L1/L2, dropout)\n- Feature selection / dimensionality reduction\n- Use simpler model\n- Ensemble methods (bagging reduces variance)\n- Cross-validation for hyperparameter tuning",
        "follow_up": "How does ensemble learning (bagging vs boosting) relate to bias-variance?",
        "follow_up_answer": "Bagging (e.g., Random Forest) reduces variance by averaging independent models trained on bootstrapped samples. Boosting (e.g., XGBoost) reduces bias by sequentially training models to correct predecessor errors. Bagging works well with high-variance base learners; boosting works well with high-bias base learners."
      },
      {
        "id": "gml_2",
        "title": "Gradient Descent Variants",
        "category": "ml_fundamentals",
        "difficulty": ["easy", "medium"],
        "question_text": "Compare and contrast Batch Gradient Descent, Stochastic Gradient Descent (SGD), and Mini-batch Gradient Descent. When would you use each, and what are the tradeoffs?",
        "rubric": {
          "understanding": {
            "points": 4,
            "criteria": [
              {"description": "Correctly describes batch GD (uses all data)", "points": 1},
              {"description": "Correctly describes SGD (one sample at a time)", "points": 1},
              {"description": "Correctly describes mini-batch (subset of data)", "points": 1},
              {"description": "Understands the gradient estimation tradeoff", "points": 1}
            ]
          },
          "tradeoffs": {
            "points": 4,
            "criteria": [
              {"description": "Discusses convergence stability vs speed", "points": 1},
              {"description": "Mentions memory requirements", "points": 1},
              {"description": "Discusses noise in gradient estimates", "points": 1},
              {"description": "Mentions hardware utilization (GPU batching)", "points": 1}
            ]
          },
          "practical": {
            "points": 2,
            "criteria": [
              {"description": "Recommends typical batch sizes (32-256)", "points": 1},
              {"description": "Mentions learning rate considerations", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Comparison:**\n\n| Aspect | Batch GD | SGD | Mini-batch |\n|--------|----------|-----|------------|\n| Samples/update | All N | 1 | k (32-256) |\n| Gradient noise | None | High | Medium |\n| Convergence | Smooth | Noisy | Balanced |\n| Memory | O(N) | O(1) | O(k) |\n| Updates/epoch | 1 | N | N/k |\n| GPU utilization | Good | Poor | Best |\n\n**Batch GD:**\n- Exact gradient, smooth convergence\n- Can't fit large datasets in memory\n- Slow updates, may get stuck in local minima\n\n**SGD:**\n- Very noisy gradient estimate\n- Can escape local minima (noise helps exploration)\n- Poor hardware utilization\n- Requires careful learning rate scheduling\n\n**Mini-batch GD (most common):**\n- Good balance of noise and stability\n- Efficient GPU utilization (parallel computation)\n- Typical batch size: 32-256\n- Noise can help generalization",
        "follow_up": "What is the role of momentum in SGD, and how does Adam optimizer improve on vanilla SGD?",
        "follow_up_answer": "Momentum accumulates past gradients to dampen oscillations and accelerate in consistent directions (like a ball rolling downhill). Adam combines momentum (first moment) with RMSprop (second moment / adaptive learning rates), scaling learning rates per parameter based on gradient history. Adam typically converges faster and requires less tuning than vanilla SGD+momentum."
      },
      {
        "id": "gml_3",
        "title": "Regularization Techniques",
        "category": "ml_fundamentals",
        "difficulty": ["easy", "medium"],
        "question_text": "Explain L1 (Lasso) and L2 (Ridge) regularization. What are the mathematical differences, and when would you prefer one over the other? Also explain dropout for neural networks.",
        "rubric": {
          "mathematical": {
            "points": 4,
            "criteria": [
              {"description": "Correctly states L1 penalty: sum of |w|", "points": 1},
              {"description": "Correctly states L2 penalty: sum of w squared", "points": 1},
              {"description": "Explains L1 promotes sparsity (zero weights)", "points": 1},
              {"description": "Explains L2 shrinks weights but not to zero", "points": 1}
            ]
          },
          "practical": {
            "points": 3,
            "criteria": [
              {"description": "L1 for feature selection", "points": 1},
              {"description": "L2 when all features may be relevant", "points": 1},
              {"description": "Elastic Net as combination", "points": 1}
            ]
          },
          "dropout": {
            "points": 3,
            "criteria": [
              {"description": "Randomly zeros neurons during training", "points": 1},
              {"description": "Acts as ensemble of sub-networks", "points": 1},
              {"description": "Prevents co-adaptation of neurons", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**L1 (Lasso) Regularization:**\n- Penalty: lambda times sum of absolute weights\n- Adds diamond constraint in weight space\n- Promotes sparsity: many weights become exactly 0\n- Useful for feature selection\n\n**L2 (Ridge) Regularization:**\n- Penalty: lambda times sum of squared weights\n- Adds circular constraint in weight space\n- Shrinks weights toward zero but not exactly zero\n- All features retained (with reduced magnitude)\n\n**When to use:**\n- L1: Suspect many irrelevant features, want automatic feature selection\n- L2: All features likely relevant, just want to prevent overfitting\n- Elastic Net (L1+L2): Groups of correlated features\n\n**Dropout:**\n- Randomly set p% of neurons to 0 during training\n- Typical p: 0.2-0.5\n- At inference, scale weights by (1-p) or use inverted dropout\n- Effect: trains ensemble of 2^n sub-networks\n- Prevents co-adaptation: neurons can't rely on specific others",
        "follow_up": "What is batch normalization, and how does it interact with dropout?",
        "follow_up_answer": "Batch norm normalizes layer inputs to zero mean and unit variance, using learned scale/shift parameters. It reduces internal covariate shift and allows higher learning rates. Batch norm + dropout can conflict because dropout changes the statistics batch norm expects. In practice: use one or the other, or apply dropout after batch norm, or use layer norm instead."
      },
      {
        "id": "gml_4",
        "title": "Imbalanced Datasets",
        "category": "ml_fundamentals",
        "difficulty": ["medium"],
        "question_text": "You're building a fraud detection model where only 1% of transactions are fraudulent. What problems arise from this class imbalance, and what techniques would you use to address them? What metrics would you use to evaluate the model?",
        "rubric": {
          "problem_understanding": {
            "points": 3,
            "criteria": [
              {"description": "Identifies accuracy paradox (99% accuracy by predicting all negative)", "points": 1},
              {"description": "Explains model bias toward majority class", "points": 1},
              {"description": "Notes rare class signal is weak", "points": 1}
            ]
          },
          "techniques": {
            "points": 4,
            "criteria": [
              {"description": "Resampling: SMOTE, oversampling, undersampling", "points": 1},
              {"description": "Class weights in loss function", "points": 1},
              {"description": "Threshold tuning", "points": 1},
              {"description": "Anomaly detection approach", "points": 1}
            ]
          },
          "metrics": {
            "points": 3,
            "criteria": [
              {"description": "Precision, Recall, F1 for positive class", "points": 1},
              {"description": "AUC-ROC and AUC-PR curves", "points": 1},
              {"description": "Confusion matrix analysis", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Problems with 1% Positive Class:**\n\n1. Accuracy Paradox: A model predicting 'not fraud' always achieves 99% accuracy\n2. Class Dominance: Gradient updates dominated by majority class\n3. Poor Minority Recall: Model learns to ignore rare positive cases\n\n**Techniques:**\n\n*Data-Level:*\n- Oversampling: SMOTE (Synthetic Minority Over-sampling)\n- Undersampling: Random or informed (NearMiss)\n- Hybrid: SMOTE + Tomek links\n\n*Algorithm-Level:*\n- Class weights in loss function\n- Cost-sensitive learning\n- Focal Loss\n\n*Threshold Tuning:*\n- Don't use default 0.5 threshold\n- Tune based on business cost of FP vs FN\n\n**Evaluation Metrics:**\n- Precision: fraud predictions that are correct\n- Recall: frauds we actually caught\n- F1: Harmonic mean of precision/recall\n- AUC-PR: Area under precision-recall curve (better for imbalance)\n- AUC-ROC: Area under ROC curve\n\n**For fraud: Prioritize recall** (catch more fraud) but monitor precision (false alarms cost).",
        "follow_up": "How would you set the classification threshold in production?",
        "follow_up_answer": "1) Define business cost: C_FP (investigating false alarm) vs C_FN (missed fraud). 2) Calculate expected cost at each threshold. 3) Choose threshold minimizing expected cost. 4) Use precision-recall curve to find threshold meeting minimum recall requirement. 5) A/B test in production."
      },
      {
        "id": "gml_5",
        "title": "Cross-Validation Strategies",
        "category": "ml_fundamentals",
        "difficulty": ["easy", "medium"],
        "question_text": "Explain k-fold cross-validation and its purpose. When would you use stratified k-fold, time-series cross-validation, or group k-fold instead?",
        "rubric": {
          "basic_cv": {
            "points": 3,
            "criteria": [
              {"description": "Correctly explains k-fold process", "points": 1},
              {"description": "Understands purpose: robust performance estimate", "points": 1},
              {"description": "Mentions typical k values (5, 10)", "points": 1}
            ]
          },
          "variants": {
            "points": 4,
            "criteria": [
              {"description": "Stratified: preserves class distribution", "points": 1},
              {"description": "Time-series: respects temporal order, no future data leakage", "points": 1},
              {"description": "Group: keeps related samples together", "points": 1},
              {"description": "Leave-one-out: when data is very limited", "points": 1}
            ]
          },
          "practical": {
            "points": 3,
            "criteria": [
              {"description": "Identifies when each variant is needed", "points": 1},
              {"description": "Discusses computational tradeoffs", "points": 1},
              {"description": "Mentions nested CV for hyperparameter tuning", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**K-Fold Cross-Validation:**\n\n1. Split data into k equal parts\n2. Train on k-1 folds, validate on remaining fold\n3. Repeat k times, average performance\n4. Benefits: Every sample used for validation exactly once\n\n**Variants:**\n\n*Stratified K-Fold:*\n- Preserves class distribution in each fold\n- Use for: Classification with imbalanced classes\n\n*Time-Series CV:*\n- Training set always before validation set\n- Use for: Forecasting, time-dependent data\n- Prevents future data leaking into training\n\n*Group K-Fold:*\n- Keeps all samples from same group together\n- Use for: Patient data, user data, repeated measurements\n- Prevents data leakage when samples are correlated\n\n*Leave-One-Out (LOO):*\n- k = N (each sample is one fold)\n- Use for: Very small datasets\n- Computationally expensive, high variance estimate\n\n**Nested CV:** For unbiased performance estimate with hyperparameter tuning:\n- Outer loop: model evaluation\n- Inner loop: hyperparameter selection",
        "follow_up": "What is data leakage and how does it relate to cross-validation?",
        "follow_up_answer": "Data leakage occurs when information from outside the training data is used to create the model. In CV context: 1) Feature engineering before split (e.g., scaling with full data), 2) Not respecting temporal order, 3) Not grouping related samples. Fix: do all preprocessing inside the CV loop, use appropriate CV variant."
      },
      {
        "id": "gml_6",
        "title": "Neural Network Architecture",
        "category": "deep_learning",
        "difficulty": ["medium", "hard"],
        "question_text": "Compare CNNs, RNNs/LSTMs, and Transformers. What types of data and tasks is each architecture best suited for, and what are their key limitations?",
        "rubric": {
          "cnn_understanding": {
            "points": 3,
            "criteria": [
              {"description": "Explains convolutional layers and spatial hierarchy", "points": 1},
              {"description": "Mentions translation invariance / weight sharing", "points": 1},
              {"description": "Suitable for: images, spectrograms, grids", "points": 1}
            ]
          },
          "rnn_understanding": {
            "points": 3,
            "criteria": [
              {"description": "Explains sequential processing and hidden state", "points": 1},
              {"description": "Discusses vanishing gradient and LSTM solution", "points": 1},
              {"description": "Suitable for: sequences, time series, text", "points": 1}
            ]
          },
          "transformer_understanding": {
            "points": 4,
            "criteria": [
              {"description": "Explains self-attention mechanism", "points": 1},
              {"description": "Mentions parallel processing advantage", "points": 1},
              {"description": "Discusses positional encodings", "points": 1},
              {"description": "Notes computational complexity O(n^2)", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**CNNs:**\n*Best for:* Images, spectrograms, 1D/2D signals with local patterns\n*Key properties:* Local receptive fields, weight sharing, pooling\n*Limitations:* Fixed receptive field, poor at long-range dependencies\n\n**RNNs / LSTMs:**\n*Best for:* Sequential data, time series, text\n*Key properties:* Hidden state captures history, LSTM gates control information flow\n*Limitations:* Sequential processing (no parallelization), struggles with very long sequences\n\n**Transformers:**\n*Best for:* Text (BERT, GPT), increasingly images (ViT)\n*Key properties:* Self-attention for every position, fully parallel, captures long-range dependencies\n*Limitations:* O(n^2) attention complexity, no built-in inductive bias, memory intensive\n\n**When to use:**\n- Images: CNN (efficient) or ViT (if lots of data)\n- Short sequences: LSTM or Transformer\n- Long sequences: Transformers with efficient attention\n- Real-time inference: CNN or small LSTM",
        "follow_up": "What is attention mechanism and how does multi-head attention improve on single attention?",
        "follow_up_answer": "Attention computes weighted sum of values based on query-key similarity: Attention(Q,K,V) = softmax(QK^T/sqrt(d))V. Multi-head attention runs h parallel attention operations with different learned projections, then concatenates results. Benefits: learns different relationships (syntax, semantics), increases model capacity, more stable training."
      },
      {
        "id": "gml_7",
        "title": "Model Deployment Considerations",
        "category": "mlops",
        "difficulty": ["medium", "hard"],
        "question_text": "You've trained a model that performs well on test data. What considerations and challenges arise when deploying this model to production? Discuss monitoring, versioning, and handling model drift.",
        "rubric": {
          "deployment_challenges": {
            "points": 3,
            "criteria": [
              {"description": "Discusses latency/throughput requirements", "points": 1},
              {"description": "Mentions model serialization and serving", "points": 1},
              {"description": "Addresses infrastructure (GPU, scaling)", "points": 1}
            ]
          },
          "monitoring": {
            "points": 4,
            "criteria": [
              {"description": "Explains concept drift vs data drift", "points": 1},
              {"description": "Discusses input validation and feature monitoring", "points": 1},
              {"description": "Mentions prediction distribution monitoring", "points": 1},
              {"description": "Describes alerting strategy", "points": 1}
            ]
          },
          "versioning": {
            "points": 3,
            "criteria": [
              {"description": "Model versioning and rollback", "points": 1},
              {"description": "A/B testing or shadow deployment", "points": 1},
              {"description": "Reproducibility: data + code + model", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Deployment Challenges:**\n- Latency requirements (real-time vs batch)\n- Model optimization: quantization, pruning, distillation\n- Hardware: CPU vs GPU, edge vs cloud\n- Model serving (TorchServe, TF Serving, Triton)\n\n**Monitoring:**\n\n*Data Drift:* Input distribution changes\n- Monitor feature statistics\n- Compare to training distribution\n\n*Concept Drift:* Relationship between X and Y changes\n- Monitor prediction distribution\n- Track actual outcomes when available\n\n*Model Health:*\n- Prediction latency, error rates\n- GPU/memory utilization\n\n**Versioning:**\n- Version: data, code, model artifacts\n- Shadow deployment: new model runs in parallel\n- Canary: gradual traffic shift\n- A/B test: randomized comparison\n- Keep previous versions for rollback",
        "follow_up": "How would you handle a model that needs retraining on new data periodically?",
        "follow_up_answer": "1) Automated retraining pipeline triggered by schedule or drift detection. 2) Validate new model against holdout set before promotion. 3) Champion-challenger: new model must beat current. 4) Track lineage: which data trained which model. 5) Consider online learning if data arrives continuously."
      },
      {
        "id": "gml_8",
        "title": "Feature Engineering",
        "category": "ml_fundamentals",
        "difficulty": ["easy", "medium"],
        "question_text": "What is feature engineering and why is it important? Describe common feature engineering techniques for numerical, categorical, and time-series data.",
        "rubric": {
          "importance": {
            "points": 2,
            "criteria": [
              {"description": "Explains feature engineering importance for model performance", "points": 1},
              {"description": "Mentions domain knowledge integration", "points": 1}
            ]
          },
          "numerical": {
            "points": 3,
            "criteria": [
              {"description": "Normalization/standardization", "points": 1},
              {"description": "Binning, log transform, polynomial features", "points": 1},
              {"description": "Handling outliers", "points": 1}
            ]
          },
          "categorical": {
            "points": 3,
            "criteria": [
              {"description": "One-hot encoding", "points": 1},
              {"description": "Target encoding / frequency encoding", "points": 1},
              {"description": "Embedding for high cardinality", "points": 1}
            ]
          },
          "time_series": {
            "points": 2,
            "criteria": [
              {"description": "Lag features, rolling statistics", "points": 1},
              {"description": "Date/time decomposition (hour, day, month)", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Feature Engineering:** Transforming raw data into features that better represent underlying patterns for ML models.\n\n**Importance:**\n- Often larger impact than model choice\n- Incorporates domain knowledge\n- Enables simpler models to perform well\n\n**Numerical Features:**\n- Standardization (z-score): Equal scale for gradient methods\n- Min-max normalization: Bounded [0,1] range\n- Log transform: Reduce skewness\n- Binning: Capture non-linear effects\n- Outlier clipping\n\n**Categorical Features:**\n- One-hot encoding: Low cardinality\n- Label encoding: Ordinal categories\n- Target encoding: High cardinality, regression\n- Frequency encoding: When frequency is informative\n- Embeddings: Very high cardinality (neural nets)\n\n**Time-Series Features:**\n- Lag features: x(t-1), x(t-2), ...\n- Rolling statistics: mean, std, min, max over window\n- Date features: hour, day_of_week, month, is_weekend\n- Trend features: moving average, exponential smoothing",
        "follow_up": "What is feature selection and how does it differ from feature engineering?",
        "follow_up_answer": "Feature engineering creates new features; feature selection chooses which features to keep. Selection methods: 1) Filter (correlation, mutual information), 2) Wrapper (forward/backward selection), 3) Embedded (L1 regularization, tree importance). Use selection to reduce overfitting, improve interpretability, and speed up training."
      }
    ]
  },
  "general_engineering_questions": {
    "description": "General software engineering concepts applicable to all technical candidates",
    "questions": [
      {
        "id": "ge_1",
        "title": "Big O Notation",
        "category": "algorithms",
        "difficulty": ["easy", "medium"],
        "question_text": "Explain Big O notation and time complexity. What are the common complexity classes and when would you expect to see each? Analyze the time complexity of finding duplicates in an array.",
        "rubric": {
          "definition": {
            "points": 3,
            "criteria": [
              {"description": "Explains Big O as describing algorithm growth rate", "points": 1},
              {"description": "Mentions it describes worst-case or average-case behavior", "points": 1},
              {"description": "Explains that constants are ignored", "points": 1}
            ]
          },
          "complexity_classes": {
            "points": 4,
            "criteria": [
              {"description": "O(1) constant - array access, hash lookup", "points": 1},
              {"description": "O(log n) logarithmic - binary search", "points": 1},
              {"description": "O(n) linear - simple loop", "points": 1},
              {"description": "O(n log n), O(n²) - sorting, nested loops", "points": 1}
            ]
          },
          "analysis": {
            "points": 3,
            "criteria": [
              {"description": "Naive approach: O(n²) with nested loops", "points": 1},
              {"description": "Optimized: O(n) with hash set", "points": 1},
              {"description": "Trade-off: time vs space", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Big O Notation:**\nDescribes how algorithm runtime or space grows as input size (n) increases. Focuses on the dominant term, ignoring constants. Typically describes worst-case.\n\n**Common Classes:**\n- O(1): Constant - array index access, hash table lookup\n- O(log n): Logarithmic - binary search, balanced tree operations\n- O(n): Linear - single loop through data\n- O(n log n): Linearithmic - efficient sorting (merge, quick)\n- O(n²): Quadratic - nested loops\n- O(2^n): Exponential - brute force subset problems\n\n**Duplicate Finding:**\n```python\n# Naive O(n²)\nfor i in range(n):\n    for j in range(i+1, n):\n        if arr[i] == arr[j]: return True\n\n# Optimized O(n) with O(n) space\nseen = set()\nfor x in arr:\n    if x in seen: return True\n    seen.add(x)\n```\nTrade-off: Hash set uses O(n) extra space but reduces time from O(n²) to O(n).",
        "follow_up": "What is amortized analysis and when is it useful?",
        "follow_up_answer": "Amortized analysis averages time over a sequence of operations. Example: dynamic array append is O(1) amortized - usually O(1) but occasionally O(n) when resizing. Useful when rare expensive operations are 'paid for' by many cheap ones."
      },
      {
        "id": "ge_2",
        "title": "Design Patterns",
        "category": "software_design",
        "difficulty": ["medium", "hard"],
        "question_text": "Explain the Singleton, Factory, and Observer design patterns. When would you use each, and what are potential drawbacks?",
        "rubric": {
          "singleton": {
            "points": 3,
            "criteria": [
              {"description": "Single instance across application", "points": 1},
              {"description": "Use cases: config, logging, connection pools", "points": 1},
              {"description": "Drawbacks: testing difficulty, hidden dependencies", "points": 1}
            ]
          },
          "factory": {
            "points": 4,
            "criteria": [
              {"description": "Creates objects without exposing creation logic", "points": 1},
              {"description": "Returns objects implementing common interface", "points": 1},
              {"description": "Use case: multiple implementations of same interface", "points": 1},
              {"description": "Drawback: can lead to many small classes", "points": 1}
            ]
          },
          "observer": {
            "points": 3,
            "criteria": [
              {"description": "One-to-many dependency between objects", "points": 1},
              {"description": "Automatic notification on state changes", "points": 1},
              {"description": "Use cases: event systems, MVC", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Singleton:**\nEnsures only one instance of a class exists.\n```python\nclass Config:\n    _instance = None\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\n```\nUse: Database connections, logging, configuration.\nDrawbacks: Global state, hard to test, hidden dependencies.\n\n**Factory:**\nEncapsulates object creation, returns objects via interface.\n```python\ndef create_db(db_type):\n    if db_type == 'postgres': return PostgresDB()\n    if db_type == 'mysql': return MySQLDB()\n```\nUse: When creation logic is complex or varies by type.\nDrawbacks: Many classes, may over-engineer simple cases.\n\n**Observer:**\nSubject notifies observers of state changes.\n```python\nclass EventEmitter:\n    def __init__(self): self.listeners = []\n    def subscribe(self, fn): self.listeners.append(fn)\n    def emit(self, data):\n        for fn in self.listeners: fn(data)\n```\nUse: Event systems, reactive programming, MVC.\nDrawbacks: Memory leaks (dangling references), debugging complexity.",
        "follow_up": "How does dependency injection relate to these patterns?",
        "follow_up_answer": "DI inverts object creation control - instead of classes creating dependencies, they receive them. Addresses Singleton's testability issues by injecting mock instances. Factory patterns often used to implement DI containers."
      },
      {
        "id": "ge_3",
        "title": "Database Indexing",
        "category": "databases",
        "difficulty": ["easy", "medium", "hard"],
        "question_text": "Explain database indexes and how they improve query performance. What is the difference between clustered and non-clustered indexes? When might an index hurt performance?",
        "rubric": {
          "basics": {
            "points": 3,
            "criteria": [
              {"description": "Indexes are data structures for faster lookups", "points": 1},
              {"description": "B-tree structure commonly used", "points": 1},
              {"description": "Trade-off: faster reads vs slower writes", "points": 1}
            ]
          },
          "clustered_vs_nonclustered": {
            "points": 4,
            "criteria": [
              {"description": "Clustered: rows physically ordered by index key", "points": 1},
              {"description": "Only one clustered index per table", "points": 1},
              {"description": "Non-clustered: separate structure pointing to rows", "points": 1},
              {"description": "Can have multiple non-clustered indexes", "points": 1}
            ]
          },
          "when_hurts": {
            "points": 3,
            "criteria": [
              {"description": "Heavy write workloads - index maintenance overhead", "points": 1},
              {"description": "Low cardinality columns - not selective enough", "points": 1},
              {"description": "Small tables - full scan may be faster", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Database Indexes:**\nData structures (usually B-trees) that allow O(log n) lookups instead of O(n) full scans.\n\n**How They Work:**\n- Store sorted references to rows\n- Enable binary search-like access\n- Created on columns frequently used in WHERE, JOIN, ORDER BY\n\n**Clustered Index:**\n- Determines physical order of data on disk\n- One per table (usually primary key)\n- Range queries very efficient\n- Insert in middle requires data movement\n\n**Non-Clustered Index:**\n- Separate structure pointing to data rows\n- Multiple per table allowed\n- Contains index key + row pointer\n- Extra hop to get actual data\n\n**When Indexes Hurt:**\n1. Write-heavy workloads: Every INSERT/UPDATE/DELETE must update indexes\n2. Low cardinality: Index on boolean column not selective\n3. Small tables: Index overhead exceeds benefit\n4. Wide composite indexes: Storage and maintenance cost\n5. Queries don't use them: Unused indexes waste space",
        "follow_up": "What is a covering index and how does it improve performance?",
        "follow_up_answer": "A covering index contains all columns needed by a query in the index itself. The database can satisfy the query from the index alone without accessing the table data (avoiding 'bookmark lookups'). Example: CREATE INDEX idx_user_email ON users(email) INCLUDE (name, created_at)."
      },
      {
        "id": "ge_4",
        "title": "REST API Design",
        "category": "api_design",
        "difficulty": ["easy", "medium"],
        "question_text": "What are the principles of RESTful API design? Explain HTTP methods, status codes, and best practices for designing endpoints. How would you design an API for a blog with posts and comments?",
        "rubric": {
          "principles": {
            "points": 3,
            "criteria": [
              {"description": "Stateless - each request contains all info needed", "points": 1},
              {"description": "Resource-based URLs using nouns", "points": 1},
              {"description": "Uniform interface - consistent structure", "points": 1}
            ]
          },
          "http_methods": {
            "points": 3,
            "criteria": [
              {"description": "GET for read, POST for create", "points": 1},
              {"description": "PUT/PATCH for update, DELETE for remove", "points": 1},
              {"description": "Idempotency of GET, PUT, DELETE", "points": 1}
            ]
          },
          "status_codes": {
            "points": 2,
            "criteria": [
              {"description": "2xx success, 4xx client error, 5xx server error", "points": 1},
              {"description": "Specific codes: 201 Created, 404 Not Found, 401/403", "points": 1}
            ]
          },
          "design": {
            "points": 2,
            "criteria": [
              {"description": "Nested resources: /posts/{id}/comments", "points": 1},
              {"description": "Pagination, filtering considerations", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**REST Principles:**\n- Stateless: No session state on server\n- Resource-based: URLs identify resources (nouns not verbs)\n- Uniform interface: Consistent HTTP semantics\n- Client-server separation\n\n**HTTP Methods:**\n- GET: Read (idempotent, safe)\n- POST: Create (not idempotent)\n- PUT: Full update (idempotent)\n- PATCH: Partial update\n- DELETE: Remove (idempotent)\n\n**Status Codes:**\n- 200 OK, 201 Created, 204 No Content\n- 400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found\n- 500 Internal Server Error\n\n**Blog API Design:**\n```\nGET    /posts           - List posts (paginated)\nGET    /posts/{id}      - Get single post\nPOST   /posts           - Create post\nPUT    /posts/{id}      - Update post\nDELETE /posts/{id}      - Delete post\n\nGET    /posts/{id}/comments    - List comments on post\nPOST   /posts/{id}/comments    - Add comment to post\nGET    /comments/{id}          - Get single comment\nDELETE /comments/{id}          - Delete comment\n\n# Query params: /posts?page=2&limit=20&author=john\n```",
        "follow_up": "How do GraphQL and REST differ, and when would you choose each?",
        "follow_up_answer": "REST: Multiple endpoints, fixed response shapes, HTTP caching, simpler. GraphQL: Single endpoint, client specifies data shape, avoids over/under-fetching, good for complex relationships and mobile (bandwidth). Choose REST for simple CRUD and caching; GraphQL for complex data needs and multiple clients."
      },
      {
        "id": "ge_5",
        "title": "Testing Strategies",
        "category": "testing",
        "difficulty": ["easy", "medium"],
        "question_text": "Explain the testing pyramid and different types of tests (unit, integration, e2e). What makes a good unit test? How do you decide what to test and what to mock?",
        "rubric": {
          "pyramid": {
            "points": 3,
            "criteria": [
              {"description": "Many unit tests at base (fast, cheap)", "points": 1},
              {"description": "Fewer integration tests in middle", "points": 1},
              {"description": "Few e2e tests at top (slow, expensive)", "points": 1}
            ]
          },
          "test_types": {
            "points": 3,
            "criteria": [
              {"description": "Unit: single function/class in isolation", "points": 1},
              {"description": "Integration: multiple components together", "points": 1},
              {"description": "E2E: full system from user perspective", "points": 1}
            ]
          },
          "good_unit_test": {
            "points": 2,
            "criteria": [
              {"description": "Fast, isolated, deterministic", "points": 1},
              {"description": "Clear arrange-act-assert structure", "points": 1}
            ]
          },
          "mocking": {
            "points": 2,
            "criteria": [
              {"description": "Mock external dependencies (DB, APIs)", "points": 1},
              {"description": "Don't over-mock - test real behavior where possible", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Testing Pyramid:**\n```\n      /\\  E2E (few)\n     /  \\ - Full user flows\n    /____\\ - Slow, brittle\n   /      \\ Integration (some)\n  /        \\ - Component interactions\n /          \\ - API, DB tests\n/______________\\ Unit (many)\n                 - Fast, isolated\n                 - Single function/class\n```\n\n**Test Types:**\n- Unit: Test function in isolation, mock dependencies\n- Integration: Test modules working together, real DB/APIs\n- E2E: Full user journey through actual UI\n\n**Good Unit Test Characteristics:**\n- Fast (<100ms)\n- Isolated (no shared state)\n- Deterministic (same result every run)\n- Clear: Arrange → Act → Assert\n- Tests one behavior\n\n**What to Mock:**\n- External services (APIs, email)\n- Databases (for unit tests)\n- Time, randomness\n\n**What NOT to Mock:**\n- The thing you're testing\n- Simple value objects\n- Your own code in integration tests",
        "follow_up": "What is TDD and what are its benefits?",
        "follow_up_answer": "Test-Driven Development: Write test first (red), implement to pass (green), refactor (clean). Benefits: Better design (testable code), documentation via tests, confidence in refactoring, fewer bugs. Drawback: Slower initial development, learning curve."
      },
      {
        "id": "ge_6",
        "title": "Git Workflow",
        "category": "version_control",
        "difficulty": ["easy"],
        "question_text": "Explain common Git workflows including branching strategies. What is a merge vs rebase? How do you handle merge conflicts? What is a pull request?",
        "rubric": {
          "branching": {
            "points": 3,
            "criteria": [
              {"description": "Feature branches from main/develop", "points": 1},
              {"description": "GitFlow, trunk-based, or GitHub Flow", "points": 1},
              {"description": "Describes branch naming conventions", "points": 1}
            ]
          },
          "merge_rebase": {
            "points": 4,
            "criteria": [
              {"description": "Merge: creates merge commit, preserves history", "points": 1},
              {"description": "Rebase: replays commits on new base, linear history", "points": 1},
              {"description": "When to use each", "points": 1},
              {"description": "Don't rebase public/shared branches", "points": 1}
            ]
          },
          "conflicts": {
            "points": 2,
            "criteria": [
              {"description": "Identifies conflicting changes in files", "points": 1},
              {"description": "Manual resolution and testing", "points": 1}
            ]
          },
          "pr": {
            "points": 1,
            "criteria": [
              {"description": "Code review mechanism before merging", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Common Workflows:**\n- GitHub Flow: main + feature branches, deploy from main\n- GitFlow: main, develop, feature, release, hotfix branches\n- Trunk-based: Short-lived branches, frequent merges\n\n**Merge vs Rebase:**\n```\nMerge:              Rebase:\n  A---B---C main      A---B---C main\n       \\                        \\\n        D---E feature            D'---E' feature\n\nMerge creates M      Rebase replays commits\n```\n- Merge: Preserves history, creates merge commit\n- Rebase: Linear history, rewrites commits\n- Use rebase for local cleanup, merge for shared branches\n- Never rebase pushed commits (rewrites history)\n\n**Merge Conflicts:**\n1. Git marks conflicts with <<<<<<, =======, >>>>>>>\n2. Manually edit to resolve\n3. git add resolved files\n4. git commit (for merge) or git rebase --continue\n\n**Pull Request:**\n- Mechanism for code review before merge\n- Shows diff, allows comments, runs CI\n- Requires approvals before merging",
        "follow_up": "What is git cherry-pick and when would you use it?",
        "follow_up_answer": "Cherry-pick applies a specific commit from one branch to another. Use cases: backporting a bugfix to release branch, picking specific features without merging entire branch, recovering work from abandoned branches."
      },
      {
        "id": "ge_7",
        "title": "Data Structures: Hash Tables",
        "category": "data_structures",
        "difficulty": ["medium"],
        "question_text": "Explain how hash tables work, including collision handling. What is the time complexity for common operations? When would you choose a hash table over other data structures?",
        "rubric": {
          "how_works": {
            "points": 4,
            "criteria": [
              {"description": "Hash function maps keys to array indices", "points": 1},
              {"description": "Load factor and resizing", "points": 1},
              {"description": "Collision handling: chaining (linked lists)", "points": 1},
              {"description": "Collision handling: open addressing", "points": 1}
            ]
          },
          "complexity": {
            "points": 3,
            "criteria": [
              {"description": "Average O(1) for insert, lookup, delete", "points": 1},
              {"description": "Worst case O(n) with many collisions", "points": 1},
              {"description": "Amortized O(1) with proper resizing", "points": 1}
            ]
          },
          "when_use": {
            "points": 3,
            "criteria": [
              {"description": "When fast lookup by key is needed", "points": 1},
              {"description": "Not when order matters", "points": 1},
              {"description": "Examples: caching, counting, deduplication", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**How Hash Tables Work:**\n1. Hash function converts key to integer\n2. Integer % array_size = bucket index\n3. Store value at that bucket\n\n**Collision Handling:**\n- Chaining: Each bucket is linked list\n- Open Addressing: Probe for next empty slot\n  - Linear probing: check i+1, i+2, ...\n  - Quadratic: check i+1, i+4, i+9, ...\n  - Double hashing: use second hash function\n\n**Load Factor:**\n- α = n/capacity\n- When α > threshold (e.g., 0.7), resize and rehash\n- Prevents excessive collisions\n\n**Time Complexity:**\n- Average: O(1) insert, lookup, delete\n- Worst: O(n) if hash function causes clustering\n- Space: O(n)\n\n**When to Use:**\n✓ Fast key-value lookups (caching, memoization)\n✓ Counting frequencies\n✓ Deduplication\n✗ Need ordered data (use balanced tree)\n✗ Range queries (use tree or skip list)\n✗ Memory constrained (overhead of empty slots)",
        "follow_up": "What makes a good hash function?",
        "follow_up_answer": "Good hash function: 1) Deterministic (same input → same output), 2) Uniform distribution (minimize collisions), 3) Fast to compute. For strings: djb2, murmur3, FNV-1a. For security (passwords): bcrypt, argon2 (slow by design)."
      },
      {
        "id": "ge_8",
        "title": "Debugging Strategies",
        "category": "debugging",
        "difficulty": ["easy", "medium"],
        "question_text": "Describe your approach to debugging a complex bug. What tools and techniques do you use? How do you debug issues that only occur in production?",
        "rubric": {
          "approach": {
            "points": 3,
            "criteria": [
              {"description": "Reproduce the bug first", "points": 1},
              {"description": "Binary search / bisect to isolate", "points": 1},
              {"description": "Form hypothesis, test systematically", "points": 1}
            ]
          },
          "tools": {
            "points": 3,
            "criteria": [
              {"description": "Debugger (breakpoints, stepping)", "points": 1},
              {"description": "Logging, print statements", "points": 1},
              {"description": "Profilers, monitoring tools", "points": 1}
            ]
          },
          "production": {
            "points": 4,
            "criteria": [
              {"description": "Logging and metrics collection", "points": 1},
              {"description": "Error tracking (Sentry, etc.)", "points": 1},
              {"description": "Reproduce in staging with prod data", "points": 1},
              {"description": "Feature flags for safe rollback", "points": 1}
            ]
          }
        },
        "sample_strong_answer": "**Debugging Approach:**\n1. **Reproduce**: Get consistent repro steps\n2. **Isolate**: Narrow down where bug occurs\n   - Binary search: comment out half the code\n   - Git bisect: find breaking commit\n3. **Hypothesize**: Form theory based on symptoms\n4. **Test**: Verify hypothesis with minimal change\n5. **Fix & Verify**: Ensure fix doesn't break else\n\n**Tools:**\n- Debugger: Breakpoints, step through, inspect vars\n- Logging: Strategic log statements\n- Print debugging: Quick and dirty\n- Profilers: For performance bugs\n- Memory analyzers: For leaks\n- Network tools: curl, Postman, browser devtools\n\n**Production Debugging:**\n- Structured logging (JSON, log levels)\n- Error tracking (Sentry, Rollbar) - stack traces\n- APM tools (Datadog, New Relic) - traces, metrics\n- Replicate: Pull prod data to staging\n- Feature flags: Disable suspected code\n- Canary deploys: Roll back quickly\n- Post-mortems: Document root cause",
        "follow_up": "What is rubber duck debugging?",
        "follow_up_answer": "Explaining your code line-by-line to an inanimate object (or colleague). Forces you to articulate assumptions and often reveals the bug during explanation. Based on the book 'The Pragmatic Programmer'. Surprisingly effective for catching logical errors."
      }
    ]
  }
}